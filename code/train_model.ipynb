{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, shuffle=True):\n",
    "\n",
    "  sequences = df.astype(np.float32).to_numpy().tolist()\n",
    "\n",
    "  dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences]\n",
    "\n",
    "  if shuffle: \n",
    "    random.Random(6).shuffle(dataset)\n",
    "\n",
    "  n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
    "\n",
    "  return dataset, seq_len, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_170</th>\n",
       "      <th>t_171</th>\n",
       "      <th>t_172</th>\n",
       "      <th>t_173</th>\n",
       "      <th>t_174</th>\n",
       "      <th>t_175</th>\n",
       "      <th>t_176</th>\n",
       "      <th>t_177</th>\n",
       "      <th>t_178</th>\n",
       "      <th>t_179</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.580723</td>\n",
       "      <td>0.579518</td>\n",
       "      <td>0.581928</td>\n",
       "      <td>0.580723</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.581928</td>\n",
       "      <td>0.581928</td>\n",
       "      <td>0.580723</td>\n",
       "      <td>0.580723</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.555422</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.549398</td>\n",
       "      <td>0.550602</td>\n",
       "      <td>0.550602</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.550602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.574699</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.568675</td>\n",
       "      <td>0.569880</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.574699</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557831</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.556627</td>\n",
       "      <td>0.556627</td>\n",
       "      <td>0.555422</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.554217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.563855</td>\n",
       "      <td>0.561446</td>\n",
       "      <td>0.568675</td>\n",
       "      <td>0.567470</td>\n",
       "      <td>0.568675</td>\n",
       "      <td>0.567470</td>\n",
       "      <td>0.567470</td>\n",
       "      <td>0.565060</td>\n",
       "      <td>0.567470</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.565060</td>\n",
       "      <td>0.563855</td>\n",
       "      <td>0.561446</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.563855</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.561446</td>\n",
       "      <td>0.562651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.569880</td>\n",
       "      <td>0.568675</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.574699</td>\n",
       "      <td>0.574699</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.573494</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556627</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.545783</td>\n",
       "      <td>0.546988</td>\n",
       "      <td>0.545783</td>\n",
       "      <td>0.549398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.575904</td>\n",
       "      <td>0.575904</td>\n",
       "      <td>0.575904</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.569880</td>\n",
       "      <td>0.572289</td>\n",
       "      <td>0.574699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557831</td>\n",
       "      <td>0.559036</td>\n",
       "      <td>0.559036</td>\n",
       "      <td>0.556627</td>\n",
       "      <td>0.555422</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.551807</td>\n",
       "      <td>0.555422</td>\n",
       "      <td>0.555422</td>\n",
       "      <td>0.553012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_0       t_1       t_2       t_3       t_4       t_5       t_6  \\\n",
       "0  0.580723  0.579518  0.581928  0.580723  0.584337  0.581928  0.581928   \n",
       "1  0.574699  0.571084  0.571084  0.568675  0.569880  0.572289  0.574699   \n",
       "2  0.563855  0.561446  0.568675  0.567470  0.568675  0.567470  0.567470   \n",
       "3  0.571084  0.571084  0.569880  0.568675  0.571084  0.574699  0.574699   \n",
       "4  0.572289  0.572289  0.572289  0.575904  0.575904  0.575904  0.571084   \n",
       "\n",
       "        t_7       t_8       t_9  ...     t_170     t_171     t_172     t_173  \\\n",
       "0  0.580723  0.580723  0.584337  ...  0.553012  0.555422  0.554217  0.551807   \n",
       "1  0.572289  0.572289  0.572289  ...  0.557831  0.554217  0.556627  0.556627   \n",
       "2  0.565060  0.567470  0.571084  ...  0.566265  0.565060  0.563855  0.561446   \n",
       "3  0.573494  0.573494  0.572289  ...  0.556627  0.551807  0.551807  0.551807   \n",
       "4  0.569880  0.572289  0.574699  ...  0.557831  0.559036  0.559036  0.556627   \n",
       "\n",
       "      t_174     t_175     t_176     t_177     t_178     t_179  \n",
       "0  0.553012  0.549398  0.550602  0.550602  0.553012  0.550602  \n",
       "1  0.555422  0.554217  0.553012  0.553012  0.551807  0.554217  \n",
       "2  0.560241  0.560241  0.563855  0.566265  0.561446  0.562651  \n",
       "3  0.553012  0.551807  0.545783  0.546988  0.545783  0.549398  \n",
       "4  0.555422  0.551807  0.551807  0.555422  0.555422  0.553012  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/mit_bih.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_0      0.580723\n",
       "t_1      0.579518\n",
       "t_2      0.581928\n",
       "t_3      0.580723\n",
       "t_4      0.584337\n",
       "           ...   \n",
       "t_175    0.549398\n",
       "t_176    0.550602\n",
       "t_177    0.550602\n",
       "t_178    0.553012\n",
       "t_179    0.550602\n",
       "Name: 0, Length: 180, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faad436ebd0>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfGklEQVR4nO3deXxU1fk/8M+dfbLvK9nYwhoCgYSwiRoFRUWtCsqiaFERKpXaVtqvWKs/qVqRqljQgqC2ilqrtiouUVR2AVHWIHtYEpJA9sySmfv7Y3JvZpLJMiHJ3Mx83q/XvEhu7tw5d2aYee5znnOOIIqiCCIiIiIFU3m7AURERERtYcBCREREiseAhYiIiBSPAQsREREpHgMWIiIiUjwGLERERKR4DFiIiIhI8RiwEBERkeJpvN2AzmC323H27FkEBwdDEARvN4eIiIjaQRRFVFVVISEhASpV6zkUnwhYzp49i6SkJG83g4iIiDqgsLAQvXr1anUfnwhYgoODAThOOCQkxMutISIiovaorKxEUlKS/D3eGp8IWKRuoJCQEAYsREREPUx7yjlYdEtERESKx4CFiIiIFI8BCxERESkeAxYiIiJSPAYsREREpHgMWIiIiEjxGLAQERGR4jFgISIiIsVjwEJERESKx4CFiIiIFI8BCxERESkeAxYiIiJSPAYsRKR49TY7/vHdMew/W+HtphCRlzBgISLF++7nUjz58UE8/tEBbzeFiLyEAQsRKd6Z8joAwMkLNV5uCRF5CwMWIlK80mozAOB8lRmWeruXW0NE3sCAhYgUTwpYRBEorjR5uTVE5A0MWIhI8UqrLPLP5yoYsBD5IwYsRKR4UoYFAM421LMQkX9hwEJEiucSsFQwYCHyRwxYiEjxyqqduoTK2SVE5I8YsBCRopmsNlSZ6+Xf2SVE5J8YsBCRojl3BwHAWRbdEvklBixEpGilTt1BAHCONSxEfokBCxEpWmmVI8PSOyoQAFBea0Wtpb61uxCRD2LAQkSKJnUJpUYFIkivAQCcZeEtkd9hwEJEilZW4+gSigrSISHMAIDdQkT+iAELESlaSUOXUGSQHvGhRgAcKUTkjxiwEJGiSV1CUUF6OcPCLiEi/6PxdgOIiFrTGLDoUGt2ZFjYJUTkfxiwEJGiScOao4P0sNpEAMywEPkjBixEpGhShiUySA8Ijm1cT4jI/zBgISLFstrsKK+1AnB0Cek1jrK7c+UmiKIIQRC82Twi6kYsuiUixbrQMKRZrRIQHqBDXKij6LbOapMDGSLyDwxYiEixpCHNEYE6qFQCDFo1ooJ0ANgtRORvGLAQkWI5D2mWxIY4siznK81u70NEvokBCxEpljRCSMqqAEBgw/T8tRabV9pERN7BgIWIFMtdhiVApwbgqGMhIv/RoYBlxYoVSE1NhcFgQE5ODnbs2NHivhMnToQgCM1uU6ZMkfe56667mv198uTJHWkaEfmQMqdJ4yRGbUPAwhWbifyKx8Oa169fj0WLFmHlypXIycnB8uXLMWnSJBQUFCAmJqbZ/u+//z4sFov8e1lZGYYNG4Zbb73VZb/Jkyfjtddek3/X6/UgIv/W2CXU+HkgByzMsBD5FY8zLMuWLcPcuXMxZ84cDBo0CCtXrkRAQADWrFnjdv+IiAjExcXJty+++AIBAQHNAha9Xu+yX3h4eMfOiIh8hsukcQ0MUpeQxe6VNhGRd3gUsFgsFuzatQt5eXmNB1CpkJeXh61bt7brGKtXr8b06dMRGBjosn3jxo2IiYlBeno65s2bh7KyMk+aRkQ+yF3RbUBDhqXWyi4hIn/iUZdQaWkpbDYbYmNjXbbHxsbi0KFDbd5/x44d2LdvH1avXu2yffLkybj55puRlpaGo0eP4g9/+AOuueYabN26FWq1utlxzGYzzObGIY2VlZWenAYR9RA1ZkdQEmzQytuMDRkWE0cJEfmVbp2af/Xq1Rg6dCiys7Ndtk+fPl3+eejQocjIyECfPn2wceNGXHnllc2Os3TpUjz++ONd3l4i8i6pTkWqWwEAA2tYiPySR11CUVFRUKvVKC4udtleXFyMuLi4Vu9bU1ODt99+G/fcc0+bj9O7d29ERUXhyJEjbv++ePFiVFRUyLfCwsL2nwQR9Rh1DVkUKasCOBfdsoaFyJ94FLDodDpkZWUhPz9f3ma325Gfn4/c3NxW7/vuu+/CbDZj5syZbT7O6dOnUVZWhvj4eLd/1+v1CAkJcbkRkW8RRdFthkWeh4XDmon8isejhBYtWoRXX30V69atw8GDBzFv3jzU1NRgzpw5AIDZs2dj8eLFze63evVq3HjjjYiMjHTZXl1djd/+9rfYtm0bTpw4gfz8fEydOhV9+/bFpEmTOnhaRNTTWW0ibHYRQJMMCyeOI/JLHtewTJs2DSUlJViyZAmKioqQmZmJDRs2yIW4p06dgkrlGgcVFBRg06ZN+Pzzz5sdT61W46effsK6detQXl6OhIQEXH311XjiiSc4FwuRH3MOSNzWsLDolsivdKjodsGCBViwYIHbv23cuLHZtvT0dIii6HZ/o9GIzz77rCPNICIfJgUkapUArVqQt0tdQlxLiMi/cC0hIlIk5/oVQWgMWKRsi4ldQkR+hQELESmSuxFCAIc1E/krBixEpEh1DTPZOtevAE5Ft+wSIvIrDFiISJGktYKaBiwBHCVE5JcYsBCRIsk1LE26hKQAxmoTYbVx8jgif8GAhYgUyd2kcUBjDQvAwlsif8KAhYgUSZrJtmmGRa9RQRo0xG4hIv/BgIWIFEkeJdQkwyIIAgI4eRyR32HAQkSKJC1u2DTD4ryNGRYi/8GAhYgUqaUaFoDT8xP5IwYsRKRILdWwAM4rNjNgIfIXDFiISJGkDIvBTYbFyNluifwOAxYiUiRp4rgANxkWTs9P5H8YsBCRIplaqWHh9PxE/ocBCxEpUq3F/VpCAKfnJ/JHDFiISJFampof4CghIn/EgIWIFEmeh4VFt0QEBixEpFAc1kxEzhiwEJEicVgzETljwEJEitTqsGZmWIj8DgMWIlKkVoc1M8NC5HcYsBCR4oii2DismTUsRAQGLESkQBabHXbR8bO7GhbOdEvkfxiwEJHimBrqVwD3NSzsEiLyPwxYiEhxpEBEoxKgVTf/mOLU/ET+hwELESlOa9PyA5yan8gfMWAhIsVpbVp+gFPzE/kjBixEpDimNgIW1rAQ+R8GLESkONKkcS13CWka9mPAQuQvGLAQkeJINSzuhjQDjYFMvV2E1WZ3uw8R+RYGLESkOFJXj7shzQBg0Kma7UtEvo0BCxEpTmvT8gOATq2CSmjYl91CRH6BAQsRKY5Um2JoIcMiCIJcx1LLgIXILzBgISLFqW0jwwJwen4if8OAhYgUR+rmaamGBQCMDXUsDFiI/AMDFiJSnLp2ZFgCtBzaTORPGLAQkeJIdSktDWsGGutbGLAQ+QcGLESkOG0NawYAo5ZdQkT+hAELESlOW1PzA5yen8jfMGAhIsWpa0eXEKfnJ/IvDFiISHGkGhYOayYiCQMWIlIcU3tqWKRhzcywEPkFBixEpDjtGdbMGhYi/8KAhYgURwpCWpqaHwCMrGEh8isMWIhIceraM9MtMyxEfoUBCxEpTl07im45DwuRf2HAQkSKIopi+6bmZ5cQkV9hwEJEimKx2WEXHT+3VsPCqfmJ/EuHApYVK1YgNTUVBoMBOTk52LFjR4v7Tpw4EYIgNLtNmTJF3kcURSxZsgTx8fEwGo3Iy8vDzz//3JGmEVEP5xyAcJQQEUk8DljWr1+PRYsW4bHHHsPu3bsxbNgwTJo0CefPn3e7//vvv49z587Jt3379kGtVuPWW2+V93nmmWfwwgsvYOXKldi+fTsCAwMxadIkmEymjp8ZEfVIUgCiVQvQqlv+iJIDFmZYiPyCxwHLsmXLMHfuXMyZMweDBg3CypUrERAQgDVr1rjdPyIiAnFxcfLtiy++QEBAgBywiKKI5cuX4//+7/8wdepUZGRk4PXXX8fZs2fxwQcfXNLJEVHP055p+YHGieNM9QxYiPyBRwGLxWLBrl27kJeX13gAlQp5eXnYunVru46xevVqTJ8+HYGBgQCA48ePo6ioyOWYoaGhyMnJafGYZrMZlZWVLjci8g3tmZYfcJqanxkWIr/gUcBSWloKm82G2NhYl+2xsbEoKipq8/47duzAvn378Mtf/lLeJt3Pk2MuXboUoaGh8i0pKcmT0yAiBWvPtPwAa1iI/E23jhJavXo1hg4diuzs7Es6zuLFi1FRUSHfCgsLO6mFRORt8iy37cywmK32Lm8TEXmfRwFLVFQU1Go1iouLXbYXFxcjLi6u1fvW1NTg7bffxj333OOyXbqfJ8fU6/UICQlxuRGRb5AnjWtnhsVis6PexqCFyNd5FLDodDpkZWUhPz9f3ma325Gfn4/c3NxW7/vuu+/CbDZj5syZLtvT0tIQFxfncszKykps3769zWMSke9pz6RxgGtAY6pnwELk6zSe3mHRokW48847MXLkSGRnZ2P58uWoqanBnDlzAACzZ89GYmIili5d6nK/1atX48Ybb0RkZKTLdkEQ8Otf/xpPPvkk+vXrh7S0NDz66KNISEjAjTfe2PEzI6IeydTOgEWvabzeqrPYEKT3+OOMiHoQj/+HT5s2DSUlJViyZAmKioqQmZmJDRs2yEWzp06dgkrlmrgpKCjApk2b8Pnnn7s95u9+9zvU1NTg3nvvRXl5OcaNG4cNGzbAYDB04JSIqCczNdSktFXDIggCjFo16qw2OcghIt8liKIoersRl6qyshKhoaGoqKhgPQtRD7fqm6NY+ukh3DwiEctuy2x13+F//hwXa6344qEJ6Bcb3D0NJKJO48n3N9cSIiJFae8oIYBDm4n8CQMWIlIUuUtI03bAwgUQifwHAxYiUhS56FbX9scTMyxE/oMBCxEpirlhbaB2ZVgaAhYTJ48j8nkMWIhIUdo7SghozLBwlBCR72PAQkSK0rhac9sfTwZ2CRH5DQYsRKQopoYuIX17MiwsuiXyGwxYiEhR2jvTrWMfx0eYFOQQke9iwEJEiuJJDYtcdMsMC5HPY8BCRIpisra/hoXDmon8BwMWIlIUkwcz3bLolsh/MGAhIkXxZKZbqeiW87AQ+T4GLESkKFIBbbuGNWsc+zDDQuT7GLAQkaJ40iUkZ1hYdEvk8xiwEJFiiKLYoVFCzLAQ+T4GLESkGOb6xloUT0YJcWp+It/HgIWIFMM58PCkS6iORbdEPo8BCxEphtQdpFYJ0Krbv5YQMyxEvo8BCxEphifT8jvvx7WEiHwfAxYiUgxPhjQ79mPRLZG/YMBCRIohZUr07Zg0DnCeOI4BC5GvY8BCRIrROKS5fR9NUpeQud4Ou13ssnYRkfcxYCEixWjsEmpfhsU5sJHuS0S+iQELESmG2cOiW+f1hlh4S+TbGLAQkWJ4MsstAKhUAvQN6wmZ6jkXC5EvY8BCRIpRZ/VslBDgNHkcMyxEPo0BCxEphjTaR9/ODAvQ2C3EkUJEvo0BCxEphtwl1M5hzYDz9PwMWIh8GQMWIlIMeaZbXfs/mjg9P5F/YMBCRIohD2v2JMPSUO/CGhYi38aAhYgUw2TxbB4WgF1CRP6CAQsRKYanM90CLLol8hcMWIhIMTyd6RYADBzWTOQXGLAQkWKYrB3oEpKKbjlxHJFPY8BCRIrh6Uy3QGPAwgwLkW9jwEJEitGRmW6lfVnDQuTbGLAQkWJIix96NqyZo4SI/AEDFiJSjI50CUlFt8ywEPk2BixEpBjSKCFPZrptzLCw6JbIlzFgISLFkBc/7EiXEItuiXwaAxYiUoy6Dsx0y7WEiPwDAxYiUgxpLhXPRgkxYCHyBwxYiEgR7HYRlvoOzMPCtYSI/AIDFiJSBLPTTLXGjkwcx4CFyKcxYCEiRXDu0vGshqVh4jgW3RL5NAYsRKQIUoZEqxagVgntvh/XEiLyDwxYiEgRTB2Y5RZozMZwWDORb+tQwLJixQqkpqbCYDAgJycHO3bsaHX/8vJyzJ8/H/Hx8dDr9ejfvz8++eQT+e9/+tOfIAiCy23AgAEdaRoR9VDSLLd6D7qDANeiW1EUO71dRKQMGk/vsH79eixatAgrV65ETk4Oli9fjkmTJqGgoAAxMTHN9rdYLLjqqqsQExOD9957D4mJiTh58iTCwsJc9hs8eDC+/PLLxoZpPG4aEfVgHZnlFnAt0DXX2z2qfyGinsPjqGDZsmWYO3cu5syZAwBYuXIlPv74Y6xZswaPPPJIs/3XrFmDCxcuYMuWLdBqtQCA1NTU5g3RaBAXF+dpc4jIR1xqlxDg6BZiwELkmzy6lLFYLNi1axfy8vIaD6BSIS8vD1u3bnV7n48++gi5ubmYP38+YmNjMWTIEDz11FOw2Vz7m3/++WckJCSgd+/emDFjBk6dOtWB0yGinkoOWDwMONQqATp1w0ihetaxEPkqjzIspaWlsNlsiI2NddkeGxuLQ4cOub3PsWPH8NVXX2HGjBn45JNPcOTIETzwwAOwWq147LHHAAA5OTlYu3Yt0tPTce7cOTz++OMYP3489u3bh+Dg4GbHNJvNMJvN8u+VlZWenAYRKVDjSs2el9YZtCpYbHYW3hL5sC4vFLHb7YiJicErr7wCtVqNrKwsnDlzBs8++6wcsFxzzTXy/hkZGcjJyUFKSgreeecd3HPPPc2OuXTpUjz++ONd3XQi6kYdzbAAjsLbSlM9J48j8mEeXcpERUVBrVajuLjYZXtxcXGL9Sfx8fHo378/1OrGD6GBAweiqKgIFovF7X3CwsLQv39/HDlyxO3fFy9ejIqKCvlWWFjoyWkQkQI1Zlg8D1i4nhCR7/MoYNHpdMjKykJ+fr68zW63Iz8/H7m5uW7vM3bsWBw5cgR2e+OkTocPH0Z8fDx0Op3b+1RXV+Po0aOIj493+3e9Xo+QkBCXGxH1bJeUYZEDFk4eR+SrPO4sXrRoEV599VWsW7cOBw8exLx581BTUyOPGpo9ezYWL14s7z9v3jxcuHABCxcuxOHDh/Hxxx/jqaeewvz58+V9Hn74YXzzzTc4ceIEtmzZgptuuglqtRq33357J5wiEfUEdfIooY7UsHDyOCJf53ENy7Rp01BSUoIlS5agqKgImZmZ2LBhg1yIe+rUKahUjR84SUlJ+Oyzz/DQQw8hIyMDiYmJWLhwIX7/+9/L+5w+fRq33347ysrKEB0djXHjxmHbtm2Ijo7uhFMkop7A3AkZllp2CRH5rA4V3S5YsAALFixw+7eNGzc225abm4tt27a1eLy33367I80gIh8irQXUkVFCAdJst5b6Tm0TESkH1xIiIkWQaliMHRwlBAC17BIi8lkMWIhIEaSAxdO1hIDGDAsDFiLfxYCFiBThUoY1B+gcvdssuiXyXQxYiEgR5FFCHahhYZcQke9jwEJEitDRxQ8BIEAa1mxl0S2Rr2LAQkSKYG7oEpKyJZ5ghoXI9zFgISJFkFZa7tiwZkcNCwMWIt/FgIWIFOGSuoR0nOmWyNcxYCEiRai7hGHNjV1CrGEh8lUMWIhIEeosjhqWgA7UsHAeFiLfx4CFiBRBmla/IzPdyl1CXEuIyGcxYCEirxNFUQ42OpJhMWpZdEvk6xiwEJHXmevtsIuOnw2X0CXEolsi38WAhYi8zjnQCLiktYTqIYpip7WLiJSDAQsReZ3UHaRTq6BRd3xqfrvoyNYQke9hwEJEXifVnnRk0jjAtVCX3UJEvokBCxF5nUkuuNV06P4atQq6hsxMLUcKEfkkBixE5HVShqUjI4QkRrnwlpPHEfkiBixE5HXSDLWGDhTcSjh5HJFvY8BCRF5nuoQ5WCRcsZnItzFgISKvk4IM4yUELJyLhci3MWAhIq+TA5ZL6RLibLdEPo0BCxF5Xed2CbHolsgXMWAhIq/r1C4hDmsm8kkMWIjI66QgQ1rEsCNYdEvk2xiwEJHX1ckZlo5/JHFYM5FvY8BCRF4n1Z10dKZb5/ty4jgi38SAhYi8rs7qWLDwUkYJSfdlhoXINzFgISKvk7IinIeFiFrCgIWIvK4z1hJiDQuRb2PAQkRe1zhK6FLmYWmYOI7Dmol8EgMWIvK6uk6dmp9Ft0S+iAELEXldHRc/JKI2MGAhIq+TggzDJa0lxKJbIl/GgIWIvK5OLrq99HlYmGEh8k0MWIjIq0RR7OQuIdawEPkiBixE5FUWmx02uwjgEruEuPghkU9jwEJEXuVcc9IZ87BYbSKsNvslt4uIlIUBCxF5lZQR0aoFaNUd/0hyHhLNOhYi38OAhYi8qjNGCAGATq2CWiUA4EghIl/EgIWIvKquE6blBwBBEOShzSy8JfI9DFiIyKs6Y1p+CSePI/JdDFiIyKtq5Wn5Oz4Hi4QjhYh8FwMWIvKqzuoSApwWQGSGhcjnMGAhIq+qszrqTTqjS4gLIBL5LgYsRORVtZ2wUrMkgDUsRD6LAQsReVWndglpGbAQ+SoGLETkVVLA0rldQgxYiHxNhwKWFStWIDU1FQaDATk5OdixY0er+5eXl2P+/PmIj4+HXq9H//798cknn1zSMYnIN8jDmll0S0St8DhgWb9+PRYtWoTHHnsMu3fvxrBhwzBp0iScP3/e7f4WiwVXXXUVTpw4gffeew8FBQV49dVXkZiY2OFjEpHvqO2CDEutlUW3RL7G44Bl2bJlmDt3LubMmYNBgwZh5cqVCAgIwJo1a9zuv2bNGly4cAEffPABxo4di9TUVFx22WUYNmxYh49JRL6jM2tY2CVE5Ls8ClgsFgt27dqFvLy8xgOoVMjLy8PWrVvd3uejjz5Cbm4u5s+fj9jYWAwZMgRPPfUUbDZbh49pNptRWVnpciOinqmxS+jSJ47jTLdEvsujgKW0tBQ2mw2xsbEu22NjY1FUVOT2PseOHcN7770Hm82GTz75BI8++iiee+45PPnkkx0+5tKlSxEaGirfkpKSPDkNIlKQTu0S0jLDQuSrunyUkN1uR0xMDF555RVkZWVh2rRp+OMf/4iVK1d2+JiLFy9GRUWFfCssLOzEFhNRd5ImjuucLiGp6JY1LES+xqMcbFRUFNRqNYqLi122FxcXIy4uzu194uPjodVqoVY3fhgNHDgQRUVFsFgsHTqmXq+HXq/3pOlEpFB1nThxHLuEiHyXRxkWnU6HrKws5Ofny9vsdjvy8/ORm5vr9j5jx47FkSNHYLfb5W2HDx9GfHw8dDpdh45JRL6jK0YJcfFDIt/jcZfQokWL8Oqrr2LdunU4ePAg5s2bh5qaGsyZMwcAMHv2bCxevFjef968ebhw4QIWLlyIw4cP4+OPP8ZTTz2F+fPnt/uYROS7TNbOXPyQGRYiX+VxWf60adNQUlKCJUuWoKioCJmZmdiwYYNcNHvq1CmoVI1xUFJSEj777DM89NBDyMjIQGJiIhYuXIjf//737T4mEfkuKbgwdEKGJUjv+EirNrGGhcjXCKIoit5uxKWqrKxEaGgoKioqEBIS4u3mEJEHhj72GarM9fjqN5ehd3TQJR3reGkNLv/rRgTq1Nj/58md1EIi6iqefH9zLSEi8qo6uUvo0udhCTY4jlFjscFm7/HXYkTkhAELEXmNpd6O+obAojOKbqWABWC3EJGvYcBCRF7jPMFbZwxr1mvU0GscH2uVJuslH4+IlIMBCxF5jdQdpFEJ0Gk65+Mo2KAFAFQxw0LkUxiwEJHXSDPSdkZ3kCSkoVuoihkWIp/CgIWIvKZx4cPOC1iC5YCFGRYiX8KAhYi8pjOn5ZfIXUJmZliIfAkDFiLyms6cll/CDAuRb2LAQkReU9eJ0/JLGLAQ+SYGLETkNV3ZJcRhzUS+hQELEXlNY5fQpc9yK2GGhcg3MWAhIq+pMTuCiiB9FxTdMmAh8ikMWIjIa6obApZAfVdkWNglRORLGLAQkddIE8d1ZsASwi4hIp/EgIWIvKba7KhhCeyElZoljV1CzLAQ+RIGLETkNTVylxCHNRNR6xiwEJHXNBbddkWGhQELkS9hwEJEXtOVRbfV5nrY7GKnHZeIvIsBCxF5TY2lKzIsjceSAiIi6vkYsBCR19SaO39qfr1GDZ3G8dHGwlsi38GAhYi8piu6hAAObSbyRQxYiMhruqLoFmDhLZEvYsBCRF5ht4uoaVhLqLMzLJztlsj3MGAhIq+otdrknzs/w8IuISJfw4CFiLxC6g5SCYBB27kfRcF6znZL5GsYsBCRV8iz3Oo0EAShU48tZVgqmWEh8hkMWIjIK2rMXVO/ArDolsgXMWAhIq+o7oJ1hCQhRinDwi4hIl/BgIWIvKKrhjQDzLAQ+SIGLETkFdK0/F3TJcRhzUS+hgELEXlFV9awcKZbIt/DgIWIvKJxlFDn17A0dgkxw0LkKxiwEJFXdNU6QgAnjiPyRQxYiMgrWHRLRJ5gwEJEXtEdRbfV5nrY7GKnH5+Iuh8DFiLyiuounTiu8ZhS1xMR9WwMWIjIK2rlLqHOL7rVa9TQaRwfbyy8JfINDFiIyCukzEeArvMzLACHNhP5GgYsROQVUg1LVxTdAiy8JfI1DFiIyCu6cuI4gLPdEvkaBixE5BVdufghAIQ0ZFi4ACKRb2DAQkReUduF87AAQEywHgBQVGHukuMTUfdiwEJE3c5uF1Fj6douoYQwIwDgTHltlxyfiLoXAxYi6na1Vpv8c2AXjRJKDG8IWC7Wdcnxiah7MWAhom4nTcuvEgCDtms+hhLlDAsDFiJfwICFiLqd88KHgiB0yWM4Z1hEkdPzE/V0DFiIqNt15cKHEinDUmOxobKOc7EQ9XQMWIio23X1HCwAYNCqERWkAwCcZuEtUY/XoYBlxYoVSE1NhcFgQE5ODnbs2NHivmvXroUgCC43g8Hgss9dd93VbJ/Jkyd3pGlE1APUmLtupWZn8kghFt4S9Xgef1qsX78eixYtwsqVK5GTk4Ply5dj0qRJKCgoQExMjNv7hISEoKCgQP7dXZ/15MmT8dprr8m/6/V6T5tGRD2ENC1/oK5rJo2TJIYZ8dPpChbeEvkAjzMsy5Ytw9y5czFnzhwMGjQIK1euREBAANasWdPifQRBQFxcnHyLjY1tto9er3fZJzw83NOmEVEPUd1NGZZEZliIfIZHAYvFYsGuXbuQl5fXeACVCnl5edi6dWuL96uurkZKSgqSkpIwdepU7N+/v9k+GzduRExMDNLT0zFv3jyUlZW1eDyz2YzKykqXGxH1HN1RdAs0jhQ6W8GAhain8yhgKS0thc1ma5YhiY2NRVFRkdv7pKenY82aNfjwww/x5ptvwm63Y8yYMTh9+rS8z+TJk/H6668jPz8fTz/9NL755htcc801sNlsbo+5dOlShIaGyrekpCRPToOIvKxaLrrt+i4hgBkWIl/QtZc3AHJzc5Gbmyv/PmbMGAwcOBCrVq3CE088AQCYPn26/PehQ4ciIyMDffr0wcaNG3HllVc2O+bixYuxaNEi+ffKykoGLUQ9SG13F92yhoWox/MowxIVFQW1Wo3i4mKX7cXFxYiLi2vXMbRaLYYPH44jR460uE/v3r0RFRXV4j56vR4hISEuNyLqOaSi26AumpZf0quhS6i02gKT1X3Gloh6Bo8CFp1Oh6ysLOTn58vb7HY78vPzXbIorbHZbNi7dy/i4+Nb3Of06dMoKytrdR8i6rmkLqGALs6whBq18kgkZlmIejaPRwktWrQIr776KtatW4eDBw9i3rx5qKmpwZw5cwAAs2fPxuLFi+X9//znP+Pzzz/HsWPHsHv3bsycORMnT57EL3/5SwCOgtzf/va32LZtG06cOIH8/HxMnToVffv2xaRJkzrpNIlISRqLbru2hkUQBC6CSOQjPL68mTZtGkpKSrBkyRIUFRUhMzMTGzZskAtxT506BZWqMQ66ePEi5s6di6KiIoSHhyMrKwtbtmzBoEGDAABqtRo//fQT1q1bh/LyciQkJODqq6/GE088wblYiHxUdw1rBhyFt4eLq3GWGRaiHq1DnxYLFizAggUL3P5t48aNLr8///zzeP7551s8ltFoxGeffdaRZhBRD1Vr6caAJZyFt0S+gGsJEVG3k9YS6up5WABOz0/kKxiwEFG3k7uEuniUENA4F8tpZliIejQGLETUrURRREWtFQAQGqDt8sfrFR4AADhWUgO7XezyxyOirsGAhYi6Va3FBovNDgAI74aAZXBCCIL0GpRWm7H71MUufzwi6hoMWKjHstTbcaqs1tvNIA9drLUAAHQaFYzarh3WDAAGrRpXD3aMYvzvj2e7/PGIqGswYKEeyVJvx8x/bMeEZ7/GZ/vdr2NFylTe0B0UZtRCEIRueczrhyUAAD7eew71DdkdIupZGLBQj/T4f/djx4kLAIA/fbRfHiZLyicFLOEBum57zHF9oxAWoEVptQXbj1/otsclos7DgIV6nLd2nMI/t5+CIAARgTqcqzDhxa9aXpuKlEXqEgrrhvoViVatwjVDHEt9sFuIqGdiwEI9ytGSaiz5cB8A4OGr0/H0LzIAAP/47hiOnK/2ZtOoncrrGrqEujFgAYDrhzkClk/3FcFSz24hop6GAQv1KG9sPQmrTcT4flF4YGIf5A2MwRUDYmC1iXjy4wPebh61Q3mNI8PSnV1CAJCTFonoYD0q6qzYdKSkWx+biC4dAxbqMUxWG97ffRoAcM+4NAiCAEEQsOS6QRAEYGNBCY6X1ni5ldSWi1LRbTcHLGqVgKsGOUYLbT5S1q2PTUSXjgEL9Rgf/3QOlaZ69Ao3YkK/aHl7alQgJvZ3/P7WjlPeah61U3ld99ewSHLSIgAAO1h4S9TjMGChHkMKRm7PToZK5Toc9o6cFADAe7tOw1xv6/a2Ufs1jhLq/oAluyFg2X+2AlUma7c/PhF1HAMW6hEOF1dh58mL0KgE3JrVq9nfL0+PRlyIARdqLPhsf7EXWkjt1ThKqHu7hAAgPtSIpAgj7CKw+1R5tz8+EXUcAxbqEf613ZFdyRsYi5gQQ7O/a9QqTBuV1LDvyW5tG3mmwmniOG/ITo0EAOw4zjoWop6EAQsp3sUaC97ZWQgAuCMnucX9po1KgkoAth270GyIs90u4rufS7Bo/R4s+XAfDp6r7NI2U8ukDEt4YPdnWAAgOy0cAPD9ca4rRNSTdP3a7kSX6LUtJ1BrsWFwQgjG94tqcb+EMCOuGBCLLw8W45kNh/DK7JEAgC1HSvHHD/a5jCB6fetJjEwJx6zcFFwzJB46DWP37mC3i6jw0jwskuw0R4ZlT2E5TFYbDN2wnhERXTp+SpOiVZmsWLv5OABg/uV921x75neT06FWCfj8QDG+LjiPI+ercO8bu3C8tAbBeg1m56ZgSkY8NCoBO09exMK392DMX/LxNkcXdYsqUz3souPnMKN3MiypkQGICtLDYrPjx8Jyr7SBiDzHDAsp2hvbTqLSVI++MUGYPDiuzf37xwbj7rGpePW74/jTR/uhFgRUm+uRnRaB1+4ahUC94y1fXGnC2zsK8a8dJ1FcacYf/rMX4/tHIzHM2NWn5Nek7qBAndprWS1BEJCTFoGP957D9ycuIKd3pFfaQUSeYYaFFKvOYsPq7xzZlQcm9mk2lLklC/P6IzZEj5NltThWWoOEUANenjFCDlYAIDbEgIV5/bDp91cgOy0CdhFY/31hl5wHNfLmCCFn0vBmLoRI1DabXcTSTw9ixdfeXbONAQsp1sd7z6GsxoKkCCNuGJbQ7vsF6TX4vymDAAB6jQqrZo1EVJDe7b5atQqzRjvmcFn//SnU21peY+aLA8XIP8gh05fCW+sINZXTuzFgKakye7UtREpWba7HfW/sxKpvjuGvnxfgyPkqr7WFXUKkWNKqurdlJUGj9iy2vi4jHnZRRK9wI4b2Cm1130mD4xAZqENxpRlfF5TI07c7+3TvOcz7524IAvD5ryegX2ywR+0hh/Ja76wj1FR6bDAyk8Kwp7Ac/9h0DIuvGejV9hApUeGFWvxy3U4UFFdBp1Hh2Vsy0DfGe599zLCQIl2osWDTkVIAwHUeZFckgiBgamYislIi2txXp1HhlobJ6P61/SSsNju+LjiP734ugc0uoqCoCr9590cAgCgCL2886nF7yOFijTIyLIIg4FdX9AUAvLn1pBxIEZHD9ycuYOqKzSgorkJ0sB7r7x2NqZmJXm0TMyykSJ/uOwebXcTQxFCkRQV2+eNNz07Gqm+PYePhEox7+isUVzq6CaQi3FqLDQPjQ3DwXCU++vEsfp3XDymRXd8uX6OULiEAuGJAjPyavrb5BB68sh+2HytDTIjeq1eRRN727s5C/OE/e2G1iRiSGIJXZ49EfKj3ByQww0KKJHUHXT8svlseLy0qEGP6REIUgeJKM6KCdAgL0OJMeR3OlNchMcyIf/4yB5f1j4bNLmLlN8yydIRSuoQAR5Zl/uV9AABrNh3HhGe+xh3/2I7rX9zMiQXJb3380zn89r2fYLWJuHZoHN65L1cRwQrADAspUHGlSR69MSXD8+6gjvrz1MF45dtjGNs3CtcMcdTA/O+nc9h6tAzzJvZGRKAOC67oi28Ol+C9Xafxqyv6IYHDoD1yUZqWXwEBCwBcMyQevaMP41hJDarM9RAEoM5qw71v7MR/F4xTTDuJusOhoko83ND9fWduCh67fnC7R2d2BwYspBgny2pwrsKErw6dhygCI1PCu3VelL4xwXjmlmEu227J6iXXtwDAqNQI5KRFYPvxC5i0/FvcktULd41JZfdQO0kZFm+tI9SUWiXg2VuGYc2m45iYHo3x/aJx66otKLxQh1+99QNeu2uUxwXfRD1Rea0F976+C3VWG8b2jcSj1w1SVLACsEuIFOLtHacw8a8bMf2VbXjl22MAgOs7UGzbHf48dQhSIwNQZarHa5tPYPLy77D/bIW3m9UjlDdkWMIDlRGwAEBWSjhWzBiBW0cmIS7UgFdmjYRRq8Z3P5di7ZYT3m4eUZez2UX86q0fcOpCLXqFG/HS7SMUGagrr0Xkd3advIBHP9wHUQRSIgPQNyYI4/tF4cbh3q1Ib0l6XDC++s1ErJ0zCsOSwlBnteG+N3bhYg1HmrRFKRPHtWZgfAgevc4xj8+qb4/BZLV5uUVEnaOs2oyjJdU4WlKNogqTvP2Zzw7hu59LYdCq8MqskV5bmLQt7BIiryquNOH+N3fLBV4r7hjR5npBSqBSCZiYHoPMpDBMXbEZJ8tqseCt3Vg3J1uRVyZKUSHVsCikS6glt47shRVfH8GZ8jq8s7MQs3NTvd0kog6x20V8+3MJ3th6El8VOLrbJdmpEchKDceqbxxZ7WdvGYZBCSFeamnb+MlKXlFcacLyLw/j+hc3oaTKjAFxwXj2lmE9IlhxFhagwyuzRiJAp8bmI2VY9M6PvCJvgdVmR5W5HoAyRgm1RqtW4b7LegMAVn1zDNZWZkAmUqLyWgte/fYYLn9uI+567XvkN9QGhhg0CDVqoRKAHScu4O8N80rdd1lvxXbDS5hhoW6370wFbl25FXUNX+wJoQasmpXlstZPT5IeF4xlt2Vi/r9246Mfz+LUhVq8MisLMSEGbzdNUaT6FUEAQhSeYQGA20Ym4YV8R5blPz+cwW0jk7zdJKJ2+bGwHDP/sV2+QAg2aHBrVhJmjE5Gn+ggAEBRhQn/2nEK//nhNIb1CsPvJg3wZpPbpWd+Q1CPtm7LCdRZbRgQF4x5E/vgmiHxXlu5t7NMHhKHN+7Oxrx/7saewnLcuGIz/jN/LGIZtMgq6hz1KyEGLdQKG33gjkGrxtzxaVj66SGs3HgUt2b16nEZQPI/56tMuO+NXagy16NfTBDuHpeGqZkJCNC5ft3HhRqw6Kr+WHRVfy+11HM9+1uCehxzvQ0b9hcBAB6/YTCmZib2+GBFMqZvFD6cPxZpUYE4W2HCvDd3wVzP7iGJNAdLuAJmuW2vGaNTYNSqcay0Bgc4mRwpnKXejvn/3I2iShP6RAfi/QfG4Pbs5GbBSk/lG98U1GN8e7gUVaZ6xIboMSq17XV+eprUqEC8dtcohBg02H2qHH/6aL+3m6QY0igqJY8QaipIr8HYvlEAgPyD573cGqLWPbPhEL4/cRHBeg1enT0SwYaec3HQHgxYqFOcq6jDc58X4IaXNiH/YHGL+0lT7l+XkaC4SYk6S2pUIF64fTgEAXhrRyF+++6POFTEq3NpHaGelGEBgLyBMQDQ6vuayNvOltdh3dYTAIDnp2Wid0Otii9hwEKXpM5iw2/f/RHjnv4aL351BD+drsCjH+yDpb75qIpaSz2+OOD40Fd6Nfqlmpgeg99PdhSxvbvrNCYv/w6z1+xAdUMRnD+S5n2ICtJ7uSWeuWKAI2D58XQFzlea2tibyDte+fYYrDYRo3tHIG9QrLeb0yUYsFCHnauow62rtuDdXadhszv+o0QF6XG2woQPfjjTbP+vDp1HndWGpAgjhvUK9UKLu9f9l/XBW3NH49qhcVCrBHx7uASL1u+B3S62fWcfdLy0BgCQFt2zljGICTHI79f8Q+wWIuUprTbj7e9PAQAWXN7Py63pOr5RiUPd7odTF3HvG7tQUmVGRKAOL88YgdG9I/Hqt8fw/z45iJc3HsHNIxJdJlH74IeGFZgzEvxmtEVun0jk9onE7lMXMX3VNnx+oBgvfX0ED17pux8qLTlWUg0A6B3VswIWALhyYCx+PF2B/IPFuD072dvNIT9VUmXGMxsO4VxDtjIsQItfjOiFbcfLYLLaMSwpDGP7Rnq5lV2HGRby2Id7zmDaK9tQUmVGemwwPpw/FqN7O/6T3JGTjLAALU6U1eLjvedc7vNlQw3ADZm+3R3kzojkcDx54xAAwLIvDuOVb4+iymRt132/OVyCJ/93AGXV5q5sYpcSRRHHShwZlp7Yt35lQx3LpiOlnBiQvMJcb8N9b+zEu7tOY9ORUmw6Uor//XQOc9Z+L89Uu+Dyvj59MciApR1E0T9T+E3Z7SL++lkBFr69B5Z6O/IGxuDfD4xBUkSAvE+gXoO7x6YBAF7I/xlHS6qx/2wFfv/vnwAAD0zsgwFxyp36uSvdNioJs0anAACe+uQQRj+Vjyf+d6DVWVTPltfh/jd24R+bjmPqis0oKKrqruZ2qtJqC6rM9RAEINnp/dJTDIoPQUKoASarHRsLSrzdHPJDf/roAHafKkewQYNnbsnA36Zn4u6xaQgxODpKBsWH4MqGeitfJYg+8G1cWVmJ0NBQVFRUICSk874MRVHEyxuPoqjChCcaro7bq8pkxYd7zkIEMDMnucdHvTXmeix6Zw8+2+/Iktx3WW/8btIAtxOAVdRZMe7pr1BlchSYBurUqLHYMDE9GqvvHNUjJg3rKvU2O97acQprt5zA0YaMw525KXh8qvv317w3d+HTfUXy74E6NW7IdHSpRQbqMP/yvjBo1d3S9kux/VgZpr2yDUkRRnz3uyu83ZwO+dNH+7F2ywkE6TV48fbhuNzHvxzIu6pMVqzZdALFVSZU1Fnx8U/nIAjAmrtG4fL0xvdencWGLUdLMbRXKGKCe95ElZ58f7OGpRV7z1Tgr58XQBSBQQkh7eq7vlhjwbIvDuP93adRY3GkjmOC9Zg0OK7N+54qq8XnB4qQHBGAKwbEdPkiemfK6/DBD2dwWf9oDEl0XwQriiL2FJbjj//ZhwPnKqFTq/DUzUNxS1avFo8batTirbmjsfzLn/HVoWLUWGxIiQzA36YN9+tgBQA0ahVm5aZi5ugUfLjnLH69fg/WbT2JIYmhuLXJ1O/fHC7Bp/uKoFYJePOeHPwt/zC2HbuAt3YUyvvYRRG/bWVK7UNFlfjyQDEmD4lD35jgdrdTFEXsOnkR+YfOo74hAzSmT1SHv6TlgtuontcdJHkorz8OnKvEjuMXcM+677HkukG4qyGbSNSZbHYRv3rrh2bZvIevTncJVgDAqFPjyoG+OSqoKWZY2rDi6yN49rMCaNUC3r53NLJSWp7s7OfiKtyzbidOXagF4Fi/ocpUj4xeofhw/liXLIvJasO7OwtRWu2YTOvH0+X45nCJvJJmQqgBd49Lwz3j0pplZ0RRxDs7C3G2vPkQy4np0RieHN7meVWarLjxpc041vBFMjw5DLNzU3DNkHgYtGrUWWz4749n8fq2E9h3xjGHSFSQDqtmZbX6HDR1+mItvjp0HnkDY5EQZmz3/fzF8i8PY/mXP0OnUWHVzCxMTI+GIAgorjRh+ivbcLy0BnePTcOS6wfBarPj/d2nUVRhxoUaM9ZtPQmtWsBnv57gti7kk73nsOidPTBZpYAjEgsu74sxDROhteSTvefwQv7POOSm++nlGSNw7dB4j8/zqU8O4pVvj+GuMan40w2DPb6/Uljq7Xj0g31Yv9MRNH7x0AT0i21/IEjUHs9+dggrvj4Kg1aFueN7Q6NSoW9MEK4dGtfjs/VNefL9zYClDaIo4oF/7san+4oQHazHfxeMQ1yoa9rNZhfx6b5zWPzvvagy1yMpwoi/3JyB9LhgjHv6K5isdrx+dzYm9I8G4FipeO7rO/HT6Ypmj5eTFoGfz1fjQsOsoM/cktFs0bW3d5zCI+/vddveYL0G+Q9f1mpq0G4XMff1ncg/dB6hRi1qLfWw2hxvg4hAHcb3i8LGghJUNEz0pdOocF1GPH5zdToSGXR0KrtdxH1v7pLnp+kXE4S0qEDkHzoPm11EdLAe+b+5DCFNZqwURRF3r/0eXxeUYHy/KLx+d7b8QSaKIl7IP4LnvzwMAEiLCsTJshrYRcfCg49MHoB7J/R2+8G370wFrntxEwDAoFXh2iHxiA7W42hJNb48eB4BOjX+88BYpMd59iX9y3U78eXBYvx56mDMzk319GlSFFEUce8bjtfspuGJeH5aprebRD7Cbhfx3u7T+N17jpq/v03PxNTMRC+3qmsxYOlkNeZ63PTyZhwurkZimBGvzh6JQQkhuFBjwfrvC/HP7Sdx+mIdACA7LQIrZ2YhItAx/fif/3sAazYfR3ZaBNbfOxrbj1/Awrd/QHGlGWEBWkwZGg9BACICdLh5RC+kRgXCZLU1jCQ5hrSoQHy56DK5K+VijQVXPLcRF2utuHJADOLDGgOTLUfKcKy0BjcPT8SyVj5El31egBe+OgK9RoX37h+D2FA93vm+EP/cfkoeLgcASRFGzMxJwa0jk+Tzoc5Xba7H058ewr93n0atpXEESnZqBP4wZSAyk8Lc3u9kWQ2uev5bWOrteH7aMNw0vBfqLDY8/N6P+Pgnxwite8al4Q/XDkRRpQnLvziMd3edBgDcMCwB2WmOTNn4flFIiXQMNb7/jV3YsL8IeQNj8NytmQhtmJW23mbHna/twOYjZUiOCMBHC8Z6NMX+Fc9txLGSGrx5Tw7G9Ws9w9MT7D1dgetf2gSVAHz98ET5+SNyRxRF7Dh+ASIgj6gEgMPFVdhx/AIAoLzWgvd2ncaJMkeGfu74NPxxyiBvNLdbMWDpAoUXajF7zQ4cL61BgE6NienR+PLgeXlG11CjFjNHJ2Phlf1dFvMrqjBhwjNfw2Kzo19MEH4+75iLol9MEFbfOQrJke5HTNSY6zH26a9QXmvFC7cPxw0NM8P+4T978a/tp5AeG4z/PTgOWqc6lx8Ly3Hjy5shisD6e0cjp3fz8fgb9p3D/W/uBgAsu20Ybh7RWItSb7Mj/9B57DxxAWP6RGFC/2i/rznpTpUmK/6z+wzOV5lwXUYCBsa3/V5e9sVhvJD/MwBgWFIY6m127D9bCa1awJM3DsG0UY11V6Io4vWtJ/Hn/x2AzWnyOqNWjfcfGAONSsBVz38LAPj8oQno36Sr42KNBde/tAmnL9Z5lFmot9kx4NENqLeL2PzIFT6TpZu9Zge+PVyC27OTsPTmDG83hxTIXG/DW9tP4Y1tJ+Ui+3kT++C3V6fjnZ2F+L8P9qG+yUSSwXoNZoxOwcNX9+/yOkYl6PKAZcWKFXj22WdRVFSEYcOG4cUXX0R2drbbfdeuXYs5c+a4bNPr9TCZGq/kRVHEY489hldffRXl5eUYO3Ys/v73v6Nfv/ZNrtUdAQsAVNRaMf9fu7HpSKm8bWhiKGblpuCGYQktjtZY/P5evLXDMQuhTqPCzcMT8ccpA9tcmOqF/J+x7IvDSI8NxqcLx+PH0+W4+e9bWg1IpMdyF9D8XFyFG1dsRo3FJtdFUM9mstrw2If78Z8fzsDSUBwbHqDFyplZbt8fALDlaCne2lEIS70Nx0pq8PP5aiRFGDEgLgRfHCjGpMGxWDVrpNv77iksx01tBMVNHS+tweV/3QiDVoUDj0/2mTWkvj9xAbeu3AqtWsDG317uM4EYdY7zVSbc98Yu/HCqHIDjwqCuYQ4f54vXEclhiA7WQ60SMK5vNG4cnuAzqyu3R5cGLOvXr8fs2bOxcuVK5OTkYPny5Xj33XdRUFCAmJjmIwjWrl2LhQsXoqCgoPFBBQGxsY1VzU8//TSWLl2KdevWIS0tDY8++ij27t2LAwcOwGBoe5hWdwUsAGC12fHiV0dQUmXGtFFJLabrnZVVm/GXTw+hT0wQbvOge6Wi1oqxT3+FanM9ctIisOvkRdTbxVavbp27jBZc3hcPT0p3HKvOiqkvbcKJslrk9o7EG/dk+0X07i9Kq81Y/30hjpyvxkN5/VvM3DV1scaCG1ZsQuGFOnnbfxeMw9BWlk5oLcvnzleHinH32p0YEBeMDb+e0L4T6iFuW7UVO45fQJBeg1+MSMRdY9OQ1gNn8qXOtf9sBeau24mzFSaEGDR4eFI6bh7RC18eKMbv/v2TnJlfdFV//OoK357srS1dGrDk5ORg1KhReOmllwAAdrsdSUlJ+NWvfoVHHnmk2f5r167Fr3/9a5SXl7s9niiKSEhIwG9+8xs8/PDDAICKigrExsZi7dq1mD59eptt6s6Apbs9veEQ/r7xqPz76N4RWHHHCES2soDch3vOYOHbewAAK2eOQGZSOOa+vhN7z1QgMcyIjxaMbfX+5F8OnqvEzS9vQZ3Vhgn9o/H63e6zpRLnoPj/pgzEL8f3bnX/f3x3DE9+fBBThsZjxYwRndl0rysoqsK8f+6SZ/E1atV49/7cFqcJIN+3Yd85PLT+R9RZbegdHYjVd45yCWJ3n7qIVd8cxU3De2HykLanu/B1nnx/e3SJbbFYsGvXLuTl5TUeQKVCXl4etm7d2uL9qqurkZKSgqSkJEydOhX79++X/3b8+HEUFRW5HDM0NBQ5OTktHtNsNqOystLl5qvun9AHkwfH4Y6cZHzy4Hi8fW9um8HG1MxEebbZ37zzI254aRP2nqlARKAOr8zOYrBCLgbGh+DlGSMwpk8kHp0ysM39wwN18krUz39xGEcaUtstOSbPweJ7mYf0uGB8+dBlePOeHAxPDkOd1Yb73tglj/Ij/yGKIl766mfc/+Zu1FltGN8vCv95YGyz9/2I5HCsmjWSwUoHeBSwlJaWwmazuXTnAEBsbCyKiorc3ic9PR1r1qzBhx9+iDfffBN2ux1jxozB6dOO0QrS/Tw55tKlSxEaGirfkpKS3O7nC0IDtFg5KwtP3TQUgxLanz36w7UDkNs7EjUWG85XmdE/Nggfzh+LwQm88qPmLh8Qg3/NHd3uOUVuG5mEnLQI1FhsuPeNnahsZV0kedHDHrZKc3upVALG9YvC2ruykRIZgDPldVjwr93yhHvkOyz19mZLaVSarFi7+Tjyln2Dv37umErgrjGpeO2uUQg1tl6nSJ7p8sqe3Nxc5Obmyr+PGTMGAwcOxKpVq/DEE0906JiLFy/GokWL5N8rKyt9OmjpCI1ahRUzRmDBv3YjLsSAx6cObrPIl6i9VCoBK2aMwPUvbsKxkho89PYevDp7ZLOC2kqTVZ6AzhczLM5CA7R4ZdZI3PTyZmw5WoYpL2zCrNwU3Dg8EUF6ZRZRiqKIervYZh2SLzDX26DXdGwZi31nKvDG1pP48Mcz0KpU+EVWL1w5MAaf7ivCBz+ckacjCNSp8ccpg3BHDlf07goe/S+KioqCWq1GcXGxy/bi4mLExbUvvaXVajF8+HAcOXIEAOT7FRcXIz6+cQbN4uJiZGZmuj2GXq+HXs9ujbZEBOrwr7mjvd0M8lFRQXqsmpWFW1ZuRf6h8/jDf/biiRuHyF9+druIX7+9B+W1ViSGGds1TLunS48Lxt+mD8eDb/2AguIq/N8H+7D8y8P4+8wsjEpt/wzRXa3OYsNHP57B61tP4ufiaiybNgzXZbRvFXVRFDulSLSzjtOWKpMVD63/EV8XnMdVA2MxKzcFWSmO2cCPldTgjW0n8cEPZxBi1OCO7BTcNqoXwgN0qLeL+Hx/EV7fehJ7Csvl45lgx9otJ7B2ywl5W7+YIMxuCE55Ydh1OlR0m52djRdffBGAo+g2OTkZCxYscFt025TNZsPgwYNx7bXXYtmyZXLR7cMPP4zf/OY3ABwZk5iYGBbdEvUAH/xwBg+9swei6CgK//uMLIQH6vDc5wV40WmCwtZGHvmailor3tt9Guu2nMCpC7XQqgU8ddPQZutFecOFGgtuaJhPR2LQqvDveWPa7DIuqzZj1uodUKmAF6YPd7skRFtEUcS6LSfw9IYCZPRyTAsxaXBcl2R5TpXV4p5138tDiDtKqxZwzZB4zMpNQZ3Fhje2nZTnq5qVm4KctAi/HulzKbp8WPOdd96JVatWITs7G8uXL8c777yDQ4cOITY2FrNnz0ZiYiKWLl0KAPjzn/+M0aNHo2/fvigvL8ezzz6LDz74ALt27cKgQY55QJ5++mn85S9/cRnW/NNPPylyWDMRNZd/sBgPvvUDaiw2CAKgEgR5crqmExT6k1pLPX7zzo/yittPTB2MWa0sTfDxT+fwlw0HMXt0KuZOaHn01dGSatzx6jZ5LbKkcCNeumNEu0YnrfzmKP7y6SFEBekxd3waNh8tw7eHS9Ar3Ij/LhiH8BamXbDa7Ji9ege2HisDAIQYNHj21mEovFCL9d8XIjxQh9fvzm42H9Ub205i+ReHMSIlHLNGp+DTfUXyvFSSlMgAvH3vaMSHup/LpspkxSP/3ovP9heh6RdWmFGLm4YnYsboFJdux23HyjDvzV24WGtFTLAej98wGFuOlrksTKtRCZg0JA4zcpJRUmXG61tPYtfJi/IxEkINmDE6BbeNTEJ0MLP6XaHLJ4576aWX5InjMjMz8cILLyAnJwcAMHHiRKSmpmLt2rUAgIceegjvv/8+ioqKEB4ejqysLDz55JMYPny4fDxp4rhXXnkF5eXlGDduHF5++WX079+/00+YiLpGQVEV7ntjpzy1uCAAD0zs0+pq0v7AbhfxzGcFWPnNUYQHaLH5kSuaTQwmiiL+lv8zln/pmLVYrRLwyYPj3a7ZJIoiZvxjO7YcLXPZ3p4pC+x2EZc/txEny2rxzC8ycNuoJFTUWnHDik04WVaLAXHBuO+y3rh2aHyzeo/H/7sfr20+gUCdGn1jgvCjm7XQfp3XD7/Oc3xuW212/Pm/B/DGtpPN9hMEYFFef1htdvxrxymUVlswLCkM6+8d3SzgOVVWi1++/j0OF7edJZnQPxqzRqegpMqMJR86ZpHN6BWKV2aNlNeAq7fZYWqYB0WrFpqdZ53FBlvD12KgTs3MSRfj1PxE5BU2u4iyajMAQK9Ry2sR+bt6mx1XLvsGJ8tqXeausdlFfHP4PP7x3XE5AEkMM+JMeZ28/ljTL8z//ngWv3rrB+g1jm6cEIMWd77mWDakrUkhNx8pxYx/bEewXoPtf7xSDpwKiqpwy8otqDLVA3DUv00blYQ7spNRZ7Xhtc3H8dYOxwrVKxtWFf/Df/bi/d1nMDA+BKNSw/H61pPQaVT48qHLEGrU4oF/7cLmI2UQBOBXl/dFpake/951GiIci/pdOdAxMvRUWS1uWLEJ5bVW3JrVC8/ckgFBEHC+yoS3dxRizebjKK+1IjZEjxdvH4HUJpMi7jvrKIjd6LTaveS6jHg8e8swGHUdK7alrseAhYhIYaRV1mOC9fj2d5djT2E5fvvej/Isw9L6T2P7RiFv2TcwWe1YPi0TNw5vXK232lyPK5/biOJKMx7K64+FeY7lS5yX3bhyQAweuLwvRiSHNQt25v9zNz7eew6zRqfgiRuHuPytpMqM9d+fclkEVRDgEgQsvLIfHrqqMfNdVm2WZ+6euXo7Nh8pQ05aBM5XmeV115ZPy8TVgx2DK0xWG+rtYrNRU5t+LsXsNdthFx3dQ2pBwKkLtfI6Oxm9QvHq7JGIDWm5ROBUWS3+uf0k1u8sRHmtFQ/l9ceDV/r3LLI9AQMWIiKFsdTbcdmzX+NchQlXD4rFV4fOo94uItSoxW0je2Hm6BR51ecVXx/Bs58VICpIh8euH4xJg+NwvLQGz31egM8PFCM5IgCfPzTBpftkw74izPvnLjnAyEwKw6uzR8q1FyVVZuQuzUe9XcQnD45vcV4naRHUN7aexKYjpVCrBFw1MBazc1Mwpm/LK20fOV+Na/72Law2RwMSw4z4x50j2z06TJoR2VlWQ93LtUPjXRaVbY3JakN5rVXuAiJlY8BCRKRAazcfx5/+e0D+/bqMeDxzS0azmhZzvQ1TXtgkzyIcrNegyuzorhEEYM1do3B5evO12/adqcBrm0/gvz+dhaXe7pJJeXnjETyzoQCZSWH4YP7YdrW3qMIEjVpAVDtnx5ZGhmWlhGPlzCyPC1WPl9agpMrRpRgZpEOfDoxCop6FAQsRkQKZrDZc8deNOFthanPhuws1FqzbcgJv7TiF81VmOdNx97g0ZKe1PqfLliOluOMf26HTqLDpd5dDo1bh8r9uREWdFc/ektFlw6tFUcTh4mr0jg70i8no6NIxYCEiUqjzVSZUmerbnT2w2uzYU1iOpPCAdndziKKIX/x9C3afKsfc8WmoNtfjrR2FGBAXjP/9ahxXaifF6LLFD4mI6NLEBBs86urQqlUYlRrhUU2GIAj41RWOgtzXt57E2987Rvg8ceMQBivUY/GdS0TkgyamR2NwQgjM9XaIIvCLEb0UtTwAkacYsBAR+SBBELDg8r4AgGCDBo9c498T+FHPp8wlRImI6JJNHhKHZbcNQ5/oIE4tTz0eAxYiIh8lCILfruNEvoddQkRERKR4DFiIiIhI8RiwEBERkeIxYCEiIiLFY8BCREREiseAhYiIiBSPAQsREREpHgMWIiIiUjwGLERERKR4DFiIiIhI8RiwEBERkeIxYCEiIiLFY8BCREREiucTqzWLoggAqKys9HJLiIiIqL2k723pe7w1PhGwVFVVAQCSkpK83BIiIiLyVFVVFUJDQ1vdxye6hBISElBYWIjy8nJUVFR06q2wsBAAUFhYKP984MABv9qmhDYoaZsS2sBzV8Y2JbRBSduU0Aaee9efZ2d+x5aXl6OwsBAJCQloi09kWFQqFXr16tWljxESEiL/HBwc7FfblNAGJW1TQht47srYpoQ2KGmbEtrAc+/683T+vTO0lVmR+ESGhYiIiHwbAxYiIiJSPJ/oEupKer0ejz32GPR6PQDgscceQ0hIiF9tU0IblLRNCW3guStjmxLaoKRtSmgDz717ztMbBLE9Y4mIiIiIvIhdQkRERKR4DFiIiIhI8RiwEBERkeIxYCEiIiLF8/tRQhMnTkRmZib27NmDzMxMLF++vNVt27dvR1FREQoLC2Gz2QAAarUaer0eJpMJdrsdgGMyO6PRiJqamg61S6VSITIyEmVlZfI26dhtEQShXesyXAqtVgur1dqlj3Ep3LVPr9fDbDa7bEtLS8OJEye6/PnylLvXsDteVyVQq9Xy/y0i6rk0Gg0CAwNhMpmQnp6Op59+GpMnT+7w8Zhh8YAoivjpp59gs9lw6623IiEhQf4SycvLw5/+9CekpqYCAIxGo/zlOHToUABAVlYWtFotJkyYgK1bt2LIkCHQaDRQqRwvQ1hYGARBgEqlgt1uh16vR79+/aDRaCAIAgRBQHR0tDwroFqthlqtRu/evREREQEAiIyMlL/UDAaD3BYASElJwZQpU+R9JfHx8dBqtS7bIiMj5Z91Oh0AIDo6Wt4mBQNarRYajSPuDQgIkI/nfH/AMZOhWq2WfzcajfL9BEFAnz59EB8fL+9jMBiQmJgo7y8IAtRqNUJCQpCUlCSfOwD069fPZT/puWwqIiICQUFBLtsCAwPl50t6HZzPuSnp+E3PDYDcnri4OLnt8fHxzfY3Go0ICAiAWq1GVlYWJkyYIN9PIr0e0kyTzsGKwWBAcHAwNBqNPMNzr1695LZJ5xgdHQ21Wo3MzEw89NBD8ntTEhcXJ783AMeHS3h4uHwuer0eISEh8msptQVwvNYGg0F+nnQ6HdRqtfwecOb8vDr/LLXX+X0RHBzcLFiR2qjVapu9T52fK3ePIQkPD5cfx93fpfaoVCro9XoEBga6tLGl/Vt6n7SH1A7n9jg/Fy09pjPn/3/S390dQxAEaLVaqFQq+flyfu0l7s5Ho9G4PAbQ+Nni3Ca1Wi0/b85/b6qlc3R+bOnzzt19pGOr1epmr7279jvfV/q7uzaoVCq37w3pnJy19r5o2o6mx5R+V6vVEAQBBoOhxfdkS48n3aeldkjbtVotIiMjoVar5c+FmJiYZvu5m2k2MjISw4YNk/9fJCQkuHzWRkZGQqfTISYmBitWrEBaWhoGDhwIrVaLp59+2qWdd9xxB+6//37cdNNN+OGHH1o911aJfuzOO+8UAfDGG2+88cab391CQkJEAOK3334rZmRkiADEsLAwEYC4YMEC8b777hMBiHq9XgQgDhkyRL5Pr169RADiV199Je7Zs0cEIN50003iFVdcIT7//POiIAjic889Jy5atEgcO3asKIqiePPNN4szZszo8He2X2dY/va3vyE3Nxd33nknRo4ciRkzZuDAgQMYOXIkZs+e3Wzb0KFDYTQaXbalp6cDAK655hocOHAAmZmZ0Ov1LhG887oLUVFRCAkJQXR0NMLCwuTIOioqCnPnzoVGo4FOp5Mj9KlTp8qZBemxH3jgAfl+BoPBZZtzBsH56kO6+omPj5evogHH1cPEiRPlTI90hTxu3Dh5gqCwsDCkpqYiLy8PWq3W7ZWZIAiIjY2Vr/CcMxw6nQ7JyclQqVRQq9Xo06cPAEekL53HlClTkJGRgZiYGAiCIF/lNr3ykJ4X5ytJ53WkpCuGv/3tb/LPUkbpsssuk+8j/RsbG9vsXDpD0ytSTzi/bq1dyTXNFrkjPU/R0dEur5sgCPjmm29csnsA8L///U9ue2JiIrRaLXbt2tXsdXCXSQFcr6yl9490hebM+QpUOlZL7y2J85WpJDg42O06Yk2Pk56eLq/mLp1fS6+R88RY0mMajUb07t1b3i69LgaDweXnplrLNDhzl/2ZOHGiSxYmMDCwWQald+/eSE1NdXlfu8tCSX8XBEG+mm76OQUAAwcOxIgRIwA0vi5BQUHIyMhwaZu718n5it9dVkISHx8vZ+ucs7YRERHy/QMCAtrMYjTN2gJATk6O/LP0HrvsssvkbVFRUQAcn2/u2t9Z2mp7a9y9fu6O35793B03MzNTfh9VVlZCEASMHz8eJSUlABpf2z59+mD37t3Q6XSor69HcHAwQkNDUV1dDQA4ffo0BEHAnDlzkJOTA51OJ39evPjii9DpdLBYLPjkk09w7bXXysfetGmTR+120VnZip7qsssuExcuXCj/29q2IUOGiOHh4S7bRo0aJQIQp06dKoqiKEenAEStVivm5OSIL7zwgktUO3ToUHHr1q1iWFiYKAiCqFKp5L8JgiAOHjxY3nbzzTeLKpVKjI6OFgMDA8WFCxeKjz32mKhWq0UAotFoFBcuXChGRETIx1Cr1eL48ePF8ePHuzzumDFjxK+//lo0GAzNIm1BEOQ2AxCff/55eb/evXuLgiCIAQEB8v5ZWVkiADElJUW+r3RzPp+mN41G43a70Wj06MrA+Xzd3Xr37i23Q3rMiIgIMTk5udm+QUFBLr8HBwc3e17cPV8ZGRnyY+h0OhGAGBcXJ8bHxze7n/TctXQ8dzeVSuX2tZIes7Xn2fkWEhIiJiUltfjYERER8t+2bNki/2w0GsWhQ4fKV1TSeysxMVGcO3euyzHUanW7z815P0EQ2n0/tVotv+/aukmvh3SLjY1t9jobDAb5/d7S6+98i4qKavacO7dfeq2k/5vO25xv0mM6n/eECRNaPR+NRiPvr9Fo5HYMGDBATE9Pb9Yu6bPF+f7S8+Lcvqa3a665Rr6aDg0NFQGISUlJYt++fZudd3p6utvzcvc6O793m77+7Xk9W3oPxcTEtLlvUFCQfM7uzl06lrv3sLvPN3e3tLQ0+TVw91kmPa6791tHb1FRUc22hYeHN3u+W3uNpPfKpk2bmr1XnnnmGdFgMLicj1arFQ0Gg/w5qlKp5EzLjTfeKEZERIijR492eV6nT58u2mw28fPPPxeNRqOo0+k6/H3t1xmWzrZv3z5UV1fLWRJRFFFbWyvXsACOrMLx48cxevRoJCYmQhRFBAYGIjMzE1qtFqIoYv/+/S5XRG2RlvsWBAFhYWGw2+2w2+0utSpqtRonTpxAVFQULBaLfOyoqChoNBq5PkKqH4iJiYHJZAIAnDp1Cn369JHrVsLCwrB7925ER0cjJCQEoihCEARoNBoEBARgxowZ8hVWnz59XK5c6uvrMWDAAACOq2Ppb9ddd51c7yHVEYwaNQoqlcrlSrhXr14ICAhAeXm5y1Vl3759XZ6Ts2fPyrUX0pWFc9ZHel6HDx+Ouro6l/vu3btXbr9U9yEIAjIzM+VsmSiK2Ldvn1yrIhVEV1dXy48jZcb0ej0sFotLZgtwXEW6y1RI2+x2u9viU6lt0uur1Wrlq2Z3q6iq1WqcPn3apWA3LS1Nfl4vXLgg/23//v3yz3V1dQgKCpKfn7i4ONhsNiQnJ8vtcs4uDBo0qFlWzx3nWhSNRuOSXZCyG1L/vvN9bDYbdu3a5faYTUnvcUlxcbF8ZSgxmUztKhxXqVQIDQ1FaWlpuwvfJfX19QBcnwt3/6dLS0td/hYZGelSc1RfX++2LgsAKisrYbfb5avuuLg4VFZWuuzTu3dvGI1G1NfXuwwWaPo8f/311/JzUlFRAcDxmXDkyBF5H41GgyFDhqCgoMClfkX6f61SqZCUlCS/j5yzmHa7HSqVSn6fOtepSdko6XPAuY6lab2GKIoIDQ2VswLuMllShiU9PV3+nJGykiqVSv671E6phlA6RwC48cYb5c8Rd/Ux0vvYbDa71MdI7ZHqD202m8tjS6SfnY/t7nHcvWf69+/fbNugQYNc9rXb7YiPj5e3Wa1W6HQ6OYsNAOXl5bjuuuvk+0jv2b1798JkMsnfA//v//0/3HbbbTCZTPJnUFxcHJ588kkAjs/KadOmYdu2bUhMTMTXX3+N7OxsvP3229BqtViwYAHmzJlzaZmsDoc6PsKTDEt2drac5ZC2ZWZmigDEKVOmiDExMWJycnKzLIJzlBsZGSkCEE0mk3zl2q9fP3HhwoXikiVLxICAAFEQBPkK8Ze//KWo0WjE8PBwtxkWg8EgajQa8brrrms1GtdoNKLBYBBjYmLk9sTExIgzZ84UY2Ji5DZLGaKoqCj5MSIjI0W1Wi1fcbV0GzhwoBgcHCw++OCDLuc8ePBgua3Oz8ktt9ziEokLgiDm5OR02hVIZ9/Gjx8v3nDDDS7b+vbt2+yqTRCEdmUC4uLiXLJW0vPgnBmTfna+ypPeQ87PW0uPIb2XnJ97g8EgLly4UJwzZ06zDNEdd9whH0+v14tqtVqcOHFim+cSEhLi8t4CIA4aNEgEXK84pWMHBga6bWtLx1er1fLN+f9XZmamePPNNzfbPy0trdmxhw8f3uZ5SP9PnB9Xo9GIvXv3FhcuXNisrUFBQc3Oqb2ZL+db08xfWlqa+Nhjj7lkViIiIkSdTueSYUlMTGzXYyYnJ8uZI+n1EARBzM3Ndbm/RqORnzvnbHF7biqVSgwICBDVarXLlX52dractZH2i4uLk9vgnGXWarWtZoC6+9Y0S6XT6cSAgAD5fKR2N90mCEK7zsNdxln6vHTOhrjL+qlUqnY9xsCBA11+j4+PF9PT08WgoCC5/WPHjhVjY2NFoPHzJSoqShQEQT6vlh4rNzdXTEhIEKdPny4/1l//+lf5O3b16tWiwWAQ6+vrxd/97nfioEGDOvx97fcZFp1OB5vNJv/b2rbY2FjU1NSgqqpK3lZeXg4A+Oqrr3DnnXciPT0d0dHRiIuLk6+A//KXv8iPp9FooNfrYbVa5eNIIyOkvn5BEGC32xEUFCSPnJH6GqX2iA1ZDZPJhDFjxiArKwsBAQEYMmSIHEFPnDhRfly73Q6z2YwJEybIV4mJiYn497//jSlTprhcOapUKlx//fWw2WxQq9W4cOEC1Go1brrpJmg0GgwcOBDTp08H0Ji1kLI1RqMRhYWFzY4nnackIiJCPhfpambAgAFISEgAAIwdOxYAXEYOSVcJwcHBLtmjBx54wGV0FgA88sgj8hWEc3+7VCEv/fvggw/K7ZLa+dJLL8lZDulKKSQkBAaDAefPn3cZiVRaWuoyugsAMjIyoNFo5BFNWq0WvXr1anaVWFNT45I9Ehuu9Jz3ka5EpX+d/yZdiSYnJ+Pqq6+GM2k/URSh0+nkqyQASEhIgMlkwrvvviufu1arRUxMDD766CP5vhaLBVdeeSV27tyJlJQUaDQahIWFwWg0QqvVumQNtFot6uvrXV536T3tXM8QFBQEQRBQX18vP450bn379pXrJ5qeh/QY0n2l84+KikJhYaH8PKrVamg0GtTU1CAkJES+f//+/aHRaOQRXAEBAXItlUQ6hnNWz263Q61W48yZM26vcp0zCdLfnZ8D6X00ePDgZqOCpKtzlUoln5M70vNcVVUlX6VLj3HmzBkkJCRApVIhODgYAQEBSEtLc/l/CThei6qqKqhUKvn/XZ8+fbBz507o9Xr5fd6/f38cP34cKpXK5T0jXZGnpaXJ+zXNRvTt21fOoNTW1sr3lbI/ElEU5c8U6XPMeToIu90uf3Y5Zz2c63OaXqVLnwfjxo2Tn3NpH3f1KmPHjsXAgQMBNL4OERERzY6fnZ0t30en08FutyMgIECuhZHONyAgQH4N7Xa7/Fo4jwSV3pttjSyT/r847+c8skcyfvz4ZvVY7kYcXbx40eX3oqIi9O/fH9XV1XIN0ZYtW5plG0tLSzF27FiYzWa89NJL+N///idnmgFH/VFMTAy2bdsGQRAQHx+PoqIiAMBdd90lH8doNEIQBFitVvz73//G1KlTWz3/VnU41PERc+fOFUeNGiVOnz5dHD58uHj06FFx1qxZbreNHDlSDA0NFYODg8VVq1aJ2dnZcgQeEBAg3nPPPeKVV14phoWFiYGBgaIgCKJWqxUfffRREWjMMERERLhcrSUnJ4vh4eFiVlaWy5VSdHS0OH78eNFoNMpXIdHR0WJKSorLFUBoaKiYkZEhGgwG+Ypdr9fLV23O9SnOEb1KpRJ1Op1oNBpdonmdTudSAyBdpQcEBIixsbFy36ler5evJKTHSE5Odrmv1Gep0WhcHjs1NdXlXAMDA8XY2Fi5TS1ldKRjOB/rkUcekTMa0nOckpLS7Ipg4MCBYlJSkghAHDZsmAhAfPTRR5vVILi7skxNTRVTU1Ndnk+dTtcsK2A0GuX6GqluxPmKvaUaHjRcwTStPWh6/KZ/02q1otFodLmCbXreTR+zf//+YkhIiKjRaFyyH6GhoS6vSUxMjBgaGipqNBpxwIAB8mNK70fnx3TOFDm/v5q2x10mQGpfdHS0fEx3z0HTrExycrJ8Neh83KZ1I4IgyFf80v8JKZPZ0mvh7vHd7d+e2ivnDEJ7byqVSoyJiWn2mE1rc6SrbClTodFoWr3qbtqO8PBwt21r+rgqlUrer6XnTXrc9tRNtPacq9Vqt7VCrd1Hegzp/eB8c65dko4bFxcnZ6ecX0t3z2/Tx2q6T9P3uHST/u80fS46WrfT9Obus6Q9x9br9XI2pbXaOo1GI4aEhIgqlUp87bXXxKlTp8qZHo1GI44dO1bOiGm1WvHll1+W3ydvvfWW+P7774tLliwRk5OTxcsvv1y84oorxLS0NPHixYsd/r72+4CloKBAHD16tPxBKX3YDR8+3O02d2/W9t6kL3jnbc7BB2/tv7n7IOnIrbM+PHrKrbXAoj3PVdN9lZS+54033rr2FhMTIxerS0GyXq8XdTqdXHw7c+ZMsU+fPvKFqlqtFiMiIsRZs2aJZ86cuaTva0EU/WDqTCIiIurR/L6GhYiIiJTP79cScnb//ffjzTffhNlsbrUIjoiIyBdIA0EAYObMmVi5cqWXW9Qydgk5OX/+PCorK1FWVibP2XDx4kXU1tbCarVCq9XKFejt3VZUVITKykqYzWaIoojg4GBUV1dDFEV5TRhptMelPI43tzWdS8R5m9LayudIuds68zmqq6uDKIqoqqpCRUUF9Hq9vB6TwWCAVqtFREQEQkNDFX8uStnm7pyk/QA0u48vnbsvvhcCAgIQHh6OoKAgea6ZkJAQtyOSlIIBCxERESkea1iIiIhI8RiwEBERkeIxYCEiIiLFY8BCREREiseAhYiIiBSPAQsREREpHgMWIiIiUjwGLERERKR4/x8ijfmpLE3H3wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df.iloc[2341,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, n_epochs, lr):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "  criterion = torch.nn.L1Loss(reduction='sum').to(device)\n",
    "  history = dict(train=[], val=[])\n",
    "\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_loss = 10000.0\n",
    "  \n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    model = model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    for seq_true in train_dataset:\n",
    "\n",
    "      seq_true = seq_true.to(device)\n",
    "      seq_pred = model(seq_true)\n",
    "\n",
    "      loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "\n",
    "      train_losses.append(loss.item())\n",
    "\n",
    "    val_losses = []\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "      for seq_true in val_dataset:\n",
    "\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_pred = model(seq_true)\n",
    "\n",
    "        loss = criterion(seq_pred, seq_true)\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    # if val_loss < best_loss:\n",
    "    #   best_loss = val_loss\n",
    "    #   best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "  return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,seq_len, n_feat = create_dataset(df)\n",
    "dataset = dataset[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 1])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(1, 64, batch_first=True)\n",
       "    (rnn2): LSTM(64, 32, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn1): LSTM(32, 32, batch_first=True)\n",
       "    (rnn2): LSTM(32, 64, batch_first=True)\n",
       "    (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lstmae import RecurrentAutoencoder\n",
    "\n",
    "model = RecurrentAutoencoder(seq_len, n_feat, device, 32)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.1\n",
    "train_index = int(len(dataset)*train_ratio)\n",
    "test_index = int(len(dataset)*test_ratio) + train_index\n",
    "train = dataset[0:train_index]\n",
    "test = dataset[train_index:test_index]\n",
    "val = dataset[test_index::]\n",
    "assert len(train)+len(test)+len(val) == len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 4.967577934265137 val loss 5.008703231811523\n",
      "Epoch 2: train loss 5.008703231811523 val loss 4.9721832275390625\n",
      "Epoch 3: train loss 4.9721832275390625 val loss 4.973094940185547\n",
      "Epoch 4: train loss 4.973094940185547 val loss 4.987540245056152\n",
      "Epoch 5: train loss 4.987540245056152 val loss 4.988859176635742\n",
      "Epoch 6: train loss 4.988859176635742 val loss 4.9800872802734375\n",
      "Epoch 7: train loss 4.9800872802734375 val loss 4.969832897186279\n",
      "Epoch 8: train loss 4.969832897186279 val loss 4.969657897949219\n",
      "Epoch 9: train loss 4.969657897949219 val loss 4.975615501403809\n",
      "Epoch 10: train loss 4.975615501403809 val loss 4.980669975280762\n",
      "Epoch 11: train loss 4.980669975280762 val loss 4.97510290145874\n",
      "Epoch 12: train loss 4.97510290145874 val loss 4.97036600112915\n",
      "Epoch 13: train loss 4.97036600112915 val loss 4.967205047607422\n",
      "Epoch 14: train loss 4.967205047607422 val loss 4.970329284667969\n",
      "Epoch 15: train loss 4.970329284667969 val loss 4.972956657409668\n",
      "Epoch 16: train loss 4.972956657409668 val loss 4.973191261291504\n",
      "Epoch 17: train loss 4.973191261291504 val loss 4.971302032470703\n",
      "Epoch 18: train loss 4.971302032470703 val loss 4.9680023193359375\n",
      "Epoch 19: train loss 4.9680023193359375 val loss 4.967462539672852\n",
      "Epoch 20: train loss 4.967462539672852 val loss 4.969603538513184\n",
      "Epoch 21: train loss 4.969603538513184 val loss 4.970606327056885\n",
      "Epoch 22: train loss 4.970606327056885 val loss 4.970049858093262\n",
      "Epoch 23: train loss 4.970049858093262 val loss 4.968329429626465\n",
      "Epoch 24: train loss 4.968329429626465 val loss 4.966856002807617\n",
      "Epoch 25: train loss 4.966856002807617 val loss 4.968398094177246\n",
      "Epoch 26: train loss 4.968398094177246 val loss 4.969533443450928\n",
      "Epoch 27: train loss 4.969533443450928 val loss 4.969133377075195\n",
      "Epoch 28: train loss 4.969133377075195 val loss 4.967555999755859\n",
      "Epoch 29: train loss 4.967555999755859 val loss 4.967134952545166\n",
      "Epoch 30: train loss 4.967134952545166 val loss 4.968299865722656\n",
      "Epoch 31: train loss 4.968299865722656 val loss 4.968580722808838\n",
      "Epoch 32: train loss 4.968580722808838 val loss 4.967508316040039\n",
      "Epoch 33: train loss 4.967508316040039 val loss 4.966895580291748\n",
      "Epoch 34: train loss 4.966895580291748 val loss 4.967401504516602\n",
      "Epoch 35: train loss 4.967401504516602 val loss 4.9676971435546875\n",
      "Epoch 36: train loss 4.9676971435546875 val loss 4.967384338378906\n",
      "Epoch 37: train loss 4.967384338378906 val loss 4.966753959655762\n",
      "Epoch 38: train loss 4.966753959655762 val loss 4.967171669006348\n",
      "Epoch 39: train loss 4.967171669006348 val loss 4.967471599578857\n",
      "Epoch 40: train loss 4.967471599578857 val loss 4.966949939727783\n",
      "Epoch 41: train loss 4.966949939727783 val loss 4.966683387756348\n",
      "Epoch 42: train loss 4.966683387756348 val loss 4.966883659362793\n",
      "Epoch 43: train loss 4.966883659362793 val loss 4.9666595458984375\n",
      "Epoch 44: train loss 4.9666595458984375 val loss 4.96662712097168\n",
      "Epoch 45: train loss 4.96662712097168 val loss 4.966712951660156\n",
      "Epoch 46: train loss 4.966712951660156 val loss 4.966524600982666\n",
      "Epoch 47: train loss 4.966524600982666 val loss 4.966902732849121\n",
      "Epoch 48: train loss 4.966902732849121 val loss 4.966974258422852\n",
      "Epoch 49: train loss 4.966974258422852 val loss 4.966643333435059\n",
      "Epoch 50: train loss 4.966643333435059 val loss 4.966822624206543\n",
      "Epoch 51: train loss 4.966822624206543 val loss 4.9669718742370605\n",
      "Epoch 52: train loss 4.9669718742370605 val loss 4.966635227203369\n",
      "Epoch 53: train loss 4.966635227203369 val loss 4.966973304748535\n",
      "Epoch 54: train loss 4.966973304748535 val loss 4.967148303985596\n",
      "Epoch 55: train loss 4.967148303985596 val loss 4.9664506912231445\n",
      "Epoch 56: train loss 4.9664506912231445 val loss 4.967087268829346\n",
      "Epoch 57: train loss 4.967087268829346 val loss 4.967494010925293\n",
      "Epoch 58: train loss 4.967494010925293 val loss 4.966736316680908\n",
      "Epoch 59: train loss 4.966736316680908 val loss 4.966552734375\n",
      "Epoch 60: train loss 4.966552734375 val loss 4.966850280761719\n",
      "Epoch 61: train loss 4.966850280761719 val loss 4.966596603393555\n",
      "Epoch 62: train loss 4.966596603393555 val loss 4.966727256774902\n",
      "Epoch 63: train loss 4.966727256774902 val loss 4.966684341430664\n",
      "Epoch 64: train loss 4.966684341430664 val loss 4.96656608581543\n",
      "Epoch 65: train loss 4.96656608581543 val loss 4.966370105743408\n",
      "Epoch 66: train loss 4.966370105743408 val loss 4.966148376464844\n",
      "Epoch 67: train loss 4.966148376464844 val loss 4.966197490692139\n",
      "Epoch 68: train loss 4.966197490692139 val loss 4.96646785736084\n",
      "Epoch 69: train loss 4.96646785736084 val loss 4.966273784637451\n",
      "Epoch 70: train loss 4.966273784637451 val loss 4.966162204742432\n",
      "Epoch 71: train loss 4.966162204742432 val loss 4.966983318328857\n",
      "Epoch 72: train loss 4.966983318328857 val loss 4.9664106369018555\n",
      "Epoch 73: train loss 4.9664106369018555 val loss 4.966975212097168\n",
      "Epoch 74: train loss 4.966975212097168 val loss 4.967100620269775\n",
      "Epoch 75: train loss 4.967100620269775 val loss 4.966541290283203\n",
      "Epoch 76: train loss 4.966541290283203 val loss 4.966795444488525\n",
      "Epoch 77: train loss 4.966795444488525 val loss 4.966495037078857\n",
      "Epoch 78: train loss 4.966495037078857 val loss 4.966399192810059\n",
      "Epoch 79: train loss 4.966399192810059 val loss 4.966364860534668\n",
      "Epoch 80: train loss 4.966364860534668 val loss 4.96639347076416\n",
      "Epoch 81: train loss 4.96639347076416 val loss 4.966310501098633\n",
      "Epoch 82: train loss 4.966310501098633 val loss 4.9667253494262695\n",
      "Epoch 83: train loss 4.9667253494262695 val loss 4.966723442077637\n",
      "Epoch 84: train loss 4.966723442077637 val loss 4.966440200805664\n",
      "Epoch 85: train loss 4.966440200805664 val loss 4.966482162475586\n",
      "Epoch 86: train loss 4.966482162475586 val loss 4.966235637664795\n",
      "Epoch 87: train loss 4.966235637664795 val loss 4.966446876525879\n",
      "Epoch 88: train loss 4.966446876525879 val loss 4.966318130493164\n",
      "Epoch 89: train loss 4.966318130493164 val loss 4.966431140899658\n",
      "Epoch 90: train loss 4.966431140899658 val loss 4.966366291046143\n",
      "Epoch 91: train loss 4.966366291046143 val loss 4.966597557067871\n",
      "Epoch 92: train loss 4.966597557067871 val loss 4.966378211975098\n",
      "Epoch 93: train loss 4.966378211975098 val loss 4.966361999511719\n",
      "Epoch 94: train loss 4.966361999511719 val loss 4.966500282287598\n",
      "Epoch 95: train loss 4.966500282287598 val loss 4.966089248657227\n",
      "Epoch 96: train loss 4.966089248657227 val loss 4.966869354248047\n",
      "Epoch 97: train loss 4.966869354248047 val loss 4.966130256652832\n",
      "Epoch 98: train loss 4.966130256652832 val loss 4.966134071350098\n",
      "Epoch 99: train loss 4.966134071350098 val loss 4.966192722320557\n",
      "Epoch 100: train loss 4.966192722320557 val loss 4.966462135314941\n",
      "Epoch 101: train loss 4.966462135314941 val loss 4.966055870056152\n",
      "Epoch 102: train loss 4.966055870056152 val loss 4.96611213684082\n",
      "Epoch 103: train loss 4.96611213684082 val loss 4.966070175170898\n",
      "Epoch 104: train loss 4.966070175170898 val loss 4.966511249542236\n",
      "Epoch 105: train loss 4.966511249542236 val loss 4.966524124145508\n",
      "Epoch 106: train loss 4.966524124145508 val loss 4.966668128967285\n",
      "Epoch 107: train loss 4.966668128967285 val loss 4.966116428375244\n",
      "Epoch 108: train loss 4.966116428375244 val loss 4.9663519859313965\n",
      "Epoch 109: train loss 4.9663519859313965 val loss 4.966235160827637\n",
      "Epoch 110: train loss 4.966235160827637 val loss 4.9661664962768555\n",
      "Epoch 111: train loss 4.9661664962768555 val loss 4.966183662414551\n",
      "Epoch 112: train loss 4.966183662414551 val loss 4.966660022735596\n",
      "Epoch 113: train loss 4.966660022735596 val loss 4.966387748718262\n",
      "Epoch 114: train loss 4.966387748718262 val loss 4.966962814331055\n",
      "Epoch 115: train loss 4.966962814331055 val loss 4.966740131378174\n",
      "Epoch 116: train loss 4.966740131378174 val loss 4.966390609741211\n",
      "Epoch 117: train loss 4.966390609741211 val loss 4.966317176818848\n",
      "Epoch 118: train loss 4.966317176818848 val loss 4.966333389282227\n",
      "Epoch 119: train loss 4.966333389282227 val loss 4.966354846954346\n",
      "Epoch 120: train loss 4.966354846954346 val loss 4.966404914855957\n",
      "Epoch 121: train loss 4.966404914855957 val loss 4.966318607330322\n",
      "Epoch 122: train loss 4.966318607330322 val loss 4.96644401550293\n",
      "Epoch 123: train loss 4.96644401550293 val loss 4.96605920791626\n",
      "Epoch 124: train loss 4.96605920791626 val loss 4.966014862060547\n",
      "Epoch 125: train loss 4.966014862060547 val loss 4.966464042663574\n",
      "Epoch 126: train loss 4.966464042663574 val loss 4.966189384460449\n",
      "Epoch 127: train loss 4.966189384460449 val loss 4.966391086578369\n",
      "Epoch 128: train loss 4.966391086578369 val loss 4.966190338134766\n",
      "Epoch 129: train loss 4.966190338134766 val loss 4.966606140136719\n",
      "Epoch 130: train loss 4.966606140136719 val loss 4.9663472175598145\n",
      "Epoch 131: train loss 4.9663472175598145 val loss 4.966505527496338\n",
      "Epoch 132: train loss 4.966505527496338 val loss 4.966461658477783\n",
      "Epoch 133: train loss 4.966461658477783 val loss 4.966341495513916\n",
      "Epoch 134: train loss 4.966341495513916 val loss 4.966261863708496\n",
      "Epoch 135: train loss 4.966261863708496 val loss 4.965931415557861\n",
      "Epoch 136: train loss 4.965931415557861 val loss 4.9660139083862305\n",
      "Epoch 137: train loss 4.9660139083862305 val loss 4.966075897216797\n",
      "Epoch 138: train loss 4.966075897216797 val loss 4.966002941131592\n",
      "Epoch 139: train loss 4.966002941131592 val loss 4.966568470001221\n",
      "Epoch 140: train loss 4.966568470001221 val loss 4.966048717498779\n",
      "Epoch 141: train loss 4.966048717498779 val loss 4.966354846954346\n",
      "Epoch 142: train loss 4.966354846954346 val loss 4.966314315795898\n",
      "Epoch 143: train loss 4.966314315795898 val loss 4.966273307800293\n",
      "Epoch 144: train loss 4.966273307800293 val loss 4.966724395751953\n",
      "Epoch 145: train loss 4.966724395751953 val loss 4.966233253479004\n",
      "Epoch 146: train loss 4.966233253479004 val loss 4.966554641723633\n",
      "Epoch 147: train loss 4.966554641723633 val loss 4.965994834899902\n",
      "Epoch 148: train loss 4.965994834899902 val loss 4.9676995277404785\n",
      "Epoch 149: train loss 4.9676995277404785 val loss 4.967275142669678\n",
      "Epoch 150: train loss 4.967275142669678 val loss 4.966522216796875\n",
      "Epoch 151: train loss 4.966522216796875 val loss 4.967268466949463\n",
      "Epoch 152: train loss 4.967268466949463 val loss 4.966488361358643\n",
      "Epoch 153: train loss 4.966488361358643 val loss 4.966592788696289\n",
      "Epoch 154: train loss 4.966592788696289 val loss 4.966586112976074\n",
      "Epoch 155: train loss 4.966586112976074 val loss 4.966691970825195\n",
      "Epoch 156: train loss 4.966691970825195 val loss 4.967654705047607\n",
      "Epoch 157: train loss 4.967654705047607 val loss 4.966717720031738\n",
      "Epoch 158: train loss 4.966717720031738 val loss 4.966596603393555\n",
      "Epoch 159: train loss 4.966596603393555 val loss 4.966355800628662\n",
      "Epoch 160: train loss 4.966355800628662 val loss 4.966101169586182\n",
      "Epoch 161: train loss 4.966101169586182 val loss 4.966529846191406\n",
      "Epoch 162: train loss 4.966529846191406 val loss 4.966875076293945\n",
      "Epoch 163: train loss 4.966875076293945 val loss 4.966310501098633\n",
      "Epoch 164: train loss 4.966310501098633 val loss 4.967850685119629\n",
      "Epoch 165: train loss 4.967850685119629 val loss 4.96751070022583\n",
      "Epoch 166: train loss 4.96751070022583 val loss 4.96644401550293\n",
      "Epoch 167: train loss 4.96644401550293 val loss 4.966752529144287\n",
      "Epoch 168: train loss 4.966752529144287 val loss 4.9662346839904785\n",
      "Epoch 169: train loss 4.9662346839904785 val loss 4.966350555419922\n",
      "Epoch 170: train loss 4.966350555419922 val loss 4.966625690460205\n",
      "Epoch 171: train loss 4.966625690460205 val loss 4.96673059463501\n",
      "Epoch 172: train loss 4.96673059463501 val loss 4.966651916503906\n",
      "Epoch 173: train loss 4.966651916503906 val loss 4.966403484344482\n",
      "Epoch 174: train loss 4.966403484344482 val loss 4.9660258293151855\n",
      "Epoch 175: train loss 4.9660258293151855 val loss 4.965951919555664\n",
      "Epoch 176: train loss 4.965951919555664 val loss 4.966050148010254\n",
      "Epoch 177: train loss 4.966050148010254 val loss 4.966363430023193\n",
      "Epoch 178: train loss 4.966363430023193 val loss 4.966046333312988\n",
      "Epoch 179: train loss 4.966046333312988 val loss 4.966248512268066\n",
      "Epoch 180: train loss 4.966248512268066 val loss 4.966446876525879\n",
      "Epoch 181: train loss 4.966446876525879 val loss 4.966239929199219\n",
      "Epoch 182: train loss 4.966239929199219 val loss 4.966376781463623\n",
      "Epoch 183: train loss 4.966376781463623 val loss 4.967100143432617\n",
      "Epoch 184: train loss 4.967100143432617 val loss 4.966368198394775\n",
      "Epoch 185: train loss 4.966368198394775 val loss 4.9670281410217285\n",
      "Epoch 186: train loss 4.9670281410217285 val loss 4.9665632247924805\n",
      "Epoch 187: train loss 4.9665632247924805 val loss 4.9662675857543945\n",
      "Epoch 188: train loss 4.9662675857543945 val loss 4.967092514038086\n",
      "Epoch 189: train loss 4.967092514038086 val loss 4.966248035430908\n",
      "Epoch 190: train loss 4.966248035430908 val loss 4.967872619628906\n",
      "Epoch 191: train loss 4.967872619628906 val loss 4.967319965362549\n",
      "Epoch 192: train loss 4.967319965362549 val loss 4.966513633728027\n",
      "Epoch 193: train loss 4.966513633728027 val loss 4.967000961303711\n",
      "Epoch 194: train loss 4.967000961303711 val loss 4.96613883972168\n",
      "Epoch 195: train loss 4.96613883972168 val loss 4.966218948364258\n",
      "Epoch 196: train loss 4.966218948364258 val loss 4.966139793395996\n",
      "Epoch 197: train loss 4.966139793395996 val loss 4.966064929962158\n",
      "Epoch 198: train loss 4.966064929962158 val loss 4.965822219848633\n",
      "Epoch 199: train loss 4.965822219848633 val loss 4.965834617614746\n",
      "Epoch 200: train loss 4.965834617614746 val loss 4.966117858886719\n",
      "Epoch 201: train loss 4.966117858886719 val loss 4.966465950012207\n",
      "Epoch 202: train loss 4.966465950012207 val loss 4.965887069702148\n",
      "Epoch 203: train loss 4.965887069702148 val loss 4.967546463012695\n",
      "Epoch 204: train loss 4.967546463012695 val loss 4.966341972351074\n",
      "Epoch 205: train loss 4.966341972351074 val loss 4.967557907104492\n",
      "Epoch 206: train loss 4.967557907104492 val loss 4.967161178588867\n",
      "Epoch 207: train loss 4.967161178588867 val loss 4.966636657714844\n",
      "Epoch 208: train loss 4.966636657714844 val loss 4.966773509979248\n",
      "Epoch 209: train loss 4.966773509979248 val loss 4.966564178466797\n",
      "Epoch 210: train loss 4.966564178466797 val loss 4.967433452606201\n",
      "Epoch 211: train loss 4.967433452606201 val loss 4.966306686401367\n",
      "Epoch 212: train loss 4.966306686401367 val loss 4.967611789703369\n",
      "Epoch 213: train loss 4.967611789703369 val loss 4.967141151428223\n",
      "Epoch 214: train loss 4.967141151428223 val loss 4.966439723968506\n",
      "Epoch 215: train loss 4.966439723968506 val loss 4.967398166656494\n",
      "Epoch 216: train loss 4.967398166656494 val loss 4.966300964355469\n",
      "Epoch 217: train loss 4.966300964355469 val loss 4.966830730438232\n",
      "Epoch 218: train loss 4.966830730438232 val loss 4.966348648071289\n",
      "Epoch 219: train loss 4.966348648071289 val loss 4.966631889343262\n",
      "Epoch 220: train loss 4.966631889343262 val loss 4.966770648956299\n",
      "Epoch 221: train loss 4.966770648956299 val loss 4.9662184715271\n",
      "Epoch 222: train loss 4.9662184715271 val loss 4.967252254486084\n",
      "Epoch 223: train loss 4.967252254486084 val loss 4.966197967529297\n",
      "Epoch 224: train loss 4.966197967529297 val loss 4.966987609863281\n",
      "Epoch 225: train loss 4.966987609863281 val loss 4.967658519744873\n",
      "Epoch 226: train loss 4.967658519744873 val loss 4.966094970703125\n",
      "Epoch 227: train loss 4.966094970703125 val loss 4.968865871429443\n",
      "Epoch 228: train loss 4.968865871429443 val loss 4.9682135581970215\n",
      "Epoch 229: train loss 4.9682135581970215 val loss 4.966413974761963\n",
      "Epoch 230: train loss 4.966413974761963 val loss 4.968080043792725\n",
      "Epoch 231: train loss 4.968080043792725 val loss 4.966719627380371\n",
      "Epoch 232: train loss 4.966719627380371 val loss 4.966405868530273\n",
      "Epoch 233: train loss 4.966405868530273 val loss 4.966267108917236\n",
      "Epoch 234: train loss 4.966267108917236 val loss 4.966440200805664\n",
      "Epoch 235: train loss 4.966440200805664 val loss 4.967097282409668\n",
      "Epoch 236: train loss 4.967097282409668 val loss 4.966744899749756\n",
      "Epoch 237: train loss 4.966744899749756 val loss 4.96628475189209\n",
      "Epoch 238: train loss 4.96628475189209 val loss 4.967530727386475\n",
      "Epoch 239: train loss 4.967530727386475 val loss 4.966490268707275\n",
      "Epoch 240: train loss 4.966490268707275 val loss 4.966795921325684\n",
      "Epoch 241: train loss 4.966795921325684 val loss 4.967563629150391\n",
      "Epoch 242: train loss 4.967563629150391 val loss 4.9659504890441895\n",
      "Epoch 243: train loss 4.9659504890441895 val loss 4.9666266441345215\n",
      "Epoch 244: train loss 4.9666266441345215 val loss 4.9659528732299805\n",
      "Epoch 245: train loss 4.9659528732299805 val loss 4.966085910797119\n",
      "Epoch 246: train loss 4.966085910797119 val loss 4.965786933898926\n",
      "Epoch 247: train loss 4.965786933898926 val loss 4.965938568115234\n",
      "Epoch 248: train loss 4.965938568115234 val loss 4.965904235839844\n",
      "Epoch 249: train loss 4.965904235839844 val loss 4.965743541717529\n",
      "Epoch 250: train loss 4.965743541717529 val loss 4.96603536605835\n",
      "Epoch 251: train loss 4.96603536605835 val loss 4.966095924377441\n",
      "Epoch 252: train loss 4.966095924377441 val loss 4.966033935546875\n",
      "Epoch 253: train loss 4.966033935546875 val loss 4.965765953063965\n",
      "Epoch 254: train loss 4.965765953063965 val loss 4.9663405418396\n",
      "Epoch 255: train loss 4.9663405418396 val loss 4.966325759887695\n",
      "Epoch 256: train loss 4.966325759887695 val loss 4.966343879699707\n",
      "Epoch 257: train loss 4.966343879699707 val loss 4.96628999710083\n",
      "Epoch 258: train loss 4.96628999710083 val loss 4.966644763946533\n",
      "Epoch 259: train loss 4.966644763946533 val loss 4.96624755859375\n",
      "Epoch 260: train loss 4.96624755859375 val loss 4.96652889251709\n",
      "Epoch 261: train loss 4.96652889251709 val loss 4.966314315795898\n",
      "Epoch 262: train loss 4.966314315795898 val loss 4.96614933013916\n",
      "Epoch 263: train loss 4.96614933013916 val loss 4.966052055358887\n",
      "Epoch 264: train loss 4.966052055358887 val loss 4.965682029724121\n",
      "Epoch 265: train loss 4.965682029724121 val loss 4.965858459472656\n",
      "Epoch 266: train loss 4.965858459472656 val loss 4.966091156005859\n",
      "Epoch 267: train loss 4.966091156005859 val loss 4.966060638427734\n",
      "Epoch 268: train loss 4.966060638427734 val loss 4.966015338897705\n",
      "Epoch 269: train loss 4.966015338897705 val loss 4.966498374938965\n",
      "Epoch 270: train loss 4.966498374938965 val loss 4.966835975646973\n",
      "Epoch 271: train loss 4.966835975646973 val loss 4.965875148773193\n",
      "Epoch 272: train loss 4.965875148773193 val loss 4.967521667480469\n",
      "Epoch 273: train loss 4.967521667480469 val loss 4.9663801193237305\n",
      "Epoch 274: train loss 4.9663801193237305 val loss 4.9675822257995605\n",
      "Epoch 275: train loss 4.9675822257995605 val loss 4.967575550079346\n",
      "Epoch 276: train loss 4.967575550079346 val loss 4.966054439544678\n",
      "Epoch 277: train loss 4.966054439544678 val loss 4.967059135437012\n",
      "Epoch 278: train loss 4.967059135437012 val loss 4.966056823730469\n",
      "Epoch 279: train loss 4.966056823730469 val loss 4.966212272644043\n",
      "Epoch 280: train loss 4.966212272644043 val loss 4.9661993980407715\n",
      "Epoch 281: train loss 4.9661993980407715 val loss 4.966156005859375\n",
      "Epoch 282: train loss 4.966156005859375 val loss 4.966198444366455\n",
      "Epoch 283: train loss 4.966198444366455 val loss 4.96627140045166\n",
      "Epoch 284: train loss 4.96627140045166 val loss 4.966769218444824\n",
      "Epoch 285: train loss 4.966769218444824 val loss 4.965954780578613\n",
      "Epoch 286: train loss 4.965954780578613 val loss 4.966855049133301\n",
      "Epoch 287: train loss 4.966855049133301 val loss 4.966085433959961\n",
      "Epoch 288: train loss 4.966085433959961 val loss 4.967013359069824\n",
      "Epoch 289: train loss 4.967013359069824 val loss 4.966718673706055\n",
      "Epoch 290: train loss 4.966718673706055 val loss 4.965996265411377\n",
      "Epoch 291: train loss 4.965996265411377 val loss 4.966038227081299\n",
      "Epoch 292: train loss 4.966038227081299 val loss 4.966329097747803\n",
      "Epoch 293: train loss 4.966329097747803 val loss 4.966423034667969\n",
      "Epoch 294: train loss 4.966423034667969 val loss 4.966090202331543\n",
      "Epoch 295: train loss 4.966090202331543 val loss 4.967540740966797\n",
      "Epoch 296: train loss 4.967540740966797 val loss 4.966540813446045\n",
      "Epoch 297: train loss 4.966540813446045 val loss 4.966723442077637\n",
      "Epoch 298: train loss 4.966723442077637 val loss 4.966930389404297\n",
      "Epoch 299: train loss 4.966930389404297 val loss 4.966268539428711\n",
      "Epoch 300: train loss 4.966268539428711 val loss 4.967266082763672\n",
      "Epoch 301: train loss 4.967266082763672 val loss 4.96631383895874\n",
      "Epoch 302: train loss 4.96631383895874 val loss 4.966482639312744\n",
      "Epoch 303: train loss 4.966482639312744 val loss 4.96638822555542\n",
      "Epoch 304: train loss 4.96638822555542 val loss 4.96600866317749\n",
      "Epoch 305: train loss 4.96600866317749 val loss 4.966076374053955\n",
      "Epoch 306: train loss 4.966076374053955 val loss 4.965777397155762\n",
      "Epoch 307: train loss 4.965777397155762 val loss 4.9660844802856445\n",
      "Epoch 308: train loss 4.9660844802856445 val loss 4.965944290161133\n",
      "Epoch 309: train loss 4.965944290161133 val loss 4.9658355712890625\n",
      "Epoch 310: train loss 4.9658355712890625 val loss 4.965994834899902\n",
      "Epoch 311: train loss 4.965994834899902 val loss 4.965806007385254\n",
      "Epoch 312: train loss 4.965806007385254 val loss 4.966089248657227\n",
      "Epoch 313: train loss 4.966089248657227 val loss 4.966022491455078\n",
      "Epoch 314: train loss 4.966022491455078 val loss 4.966142654418945\n",
      "Epoch 315: train loss 4.966142654418945 val loss 4.9658355712890625\n",
      "Epoch 316: train loss 4.9658355712890625 val loss 4.966338634490967\n",
      "Epoch 317: train loss 4.966338634490967 val loss 4.9656171798706055\n",
      "Epoch 318: train loss 4.9656171798706055 val loss 4.965714454650879\n",
      "Epoch 319: train loss 4.965714454650879 val loss 4.9663004875183105\n",
      "Epoch 320: train loss 4.9663004875183105 val loss 4.966078758239746\n",
      "Epoch 321: train loss 4.966078758239746 val loss 4.966358661651611\n",
      "Epoch 322: train loss 4.966358661651611 val loss 4.965785503387451\n",
      "Epoch 323: train loss 4.965785503387451 val loss 4.965842247009277\n",
      "Epoch 324: train loss 4.965842247009277 val loss 4.9657392501831055\n",
      "Epoch 325: train loss 4.9657392501831055 val loss 4.965576648712158\n",
      "Epoch 326: train loss 4.965576648712158 val loss 4.96571683883667\n",
      "Epoch 327: train loss 4.96571683883667 val loss 4.966390132904053\n",
      "Epoch 328: train loss 4.966390132904053 val loss 4.965823650360107\n",
      "Epoch 329: train loss 4.965823650360107 val loss 4.965651035308838\n",
      "Epoch 330: train loss 4.965651035308838 val loss 4.966423511505127\n",
      "Epoch 331: train loss 4.966423511505127 val loss 4.965816974639893\n",
      "Epoch 332: train loss 4.965816974639893 val loss 4.966275215148926\n",
      "Epoch 333: train loss 4.966275215148926 val loss 4.966091156005859\n",
      "Epoch 334: train loss 4.966091156005859 val loss 4.966126441955566\n",
      "Epoch 335: train loss 4.966126441955566 val loss 4.966117858886719\n",
      "Epoch 336: train loss 4.966117858886719 val loss 4.9661478996276855\n",
      "Epoch 337: train loss 4.9661478996276855 val loss 4.966121196746826\n",
      "Epoch 338: train loss 4.966121196746826 val loss 4.965911865234375\n",
      "Epoch 339: train loss 4.965911865234375 val loss 4.965837478637695\n",
      "Epoch 340: train loss 4.965837478637695 val loss 4.966436386108398\n",
      "Epoch 341: train loss 4.966436386108398 val loss 4.965903282165527\n",
      "Epoch 342: train loss 4.965903282165527 val loss 4.965947151184082\n",
      "Epoch 343: train loss 4.965947151184082 val loss 4.965817451477051\n",
      "Epoch 344: train loss 4.965817451477051 val loss 4.9662184715271\n",
      "Epoch 345: train loss 4.9662184715271 val loss 4.966067314147949\n",
      "Epoch 346: train loss 4.966067314147949 val loss 4.966179847717285\n",
      "Epoch 347: train loss 4.966179847717285 val loss 4.96622371673584\n",
      "Epoch 348: train loss 4.96622371673584 val loss 4.966015815734863\n",
      "Epoch 349: train loss 4.966015815734863 val loss 4.966588497161865\n",
      "Epoch 350: train loss 4.966588497161865 val loss 4.966422080993652\n",
      "Epoch 351: train loss 4.966422080993652 val loss 4.9661760330200195\n",
      "Epoch 352: train loss 4.9661760330200195 val loss 4.966391563415527\n",
      "Epoch 353: train loss 4.966391563415527 val loss 4.965796947479248\n",
      "Epoch 354: train loss 4.965796947479248 val loss 4.966190338134766\n",
      "Epoch 355: train loss 4.966190338134766 val loss 4.966241836547852\n",
      "Epoch 356: train loss 4.966241836547852 val loss 4.9658660888671875\n",
      "Epoch 357: train loss 4.9658660888671875 val loss 4.965889930725098\n",
      "Epoch 358: train loss 4.965889930725098 val loss 4.9664306640625\n",
      "Epoch 359: train loss 4.9664306640625 val loss 4.966446876525879\n",
      "Epoch 360: train loss 4.966446876525879 val loss 4.965792655944824\n",
      "Epoch 361: train loss 4.965792655944824 val loss 4.966533660888672\n",
      "Epoch 362: train loss 4.966533660888672 val loss 4.965901851654053\n",
      "Epoch 363: train loss 4.965901851654053 val loss 4.966594696044922\n",
      "Epoch 364: train loss 4.966594696044922 val loss 4.966011047363281\n",
      "Epoch 365: train loss 4.966011047363281 val loss 4.96620512008667\n",
      "Epoch 366: train loss 4.96620512008667 val loss 4.965870380401611\n",
      "Epoch 367: train loss 4.965870380401611 val loss 4.96604061126709\n",
      "Epoch 368: train loss 4.96604061126709 val loss 4.965761184692383\n",
      "Epoch 369: train loss 4.965761184692383 val loss 4.966397762298584\n",
      "Epoch 370: train loss 4.966397762298584 val loss 4.9657487869262695\n",
      "Epoch 371: train loss 4.9657487869262695 val loss 4.965604305267334\n",
      "Epoch 372: train loss 4.965604305267334 val loss 4.966976165771484\n",
      "Epoch 373: train loss 4.966976165771484 val loss 4.9656219482421875\n",
      "Epoch 374: train loss 4.9656219482421875 val loss 4.966710090637207\n",
      "Epoch 375: train loss 4.966710090637207 val loss 4.965631484985352\n",
      "Epoch 376: train loss 4.965631484985352 val loss 4.967228889465332\n",
      "Epoch 377: train loss 4.967228889465332 val loss 4.965646743774414\n",
      "Epoch 378: train loss 4.965646743774414 val loss 4.968542098999023\n",
      "Epoch 379: train loss 4.968542098999023 val loss 4.967991828918457\n",
      "Epoch 380: train loss 4.967991828918457 val loss 4.9662275314331055\n",
      "Epoch 381: train loss 4.9662275314331055 val loss 4.967384338378906\n",
      "Epoch 382: train loss 4.967384338378906 val loss 4.966082572937012\n",
      "Epoch 383: train loss 4.966082572937012 val loss 4.966644763946533\n",
      "Epoch 384: train loss 4.966644763946533 val loss 4.966302871704102\n",
      "Epoch 385: train loss 4.966302871704102 val loss 4.965899467468262\n",
      "Epoch 386: train loss 4.965899467468262 val loss 4.966640472412109\n",
      "Epoch 387: train loss 4.966640472412109 val loss 4.966116905212402\n",
      "Epoch 388: train loss 4.966116905212402 val loss 4.966373443603516\n",
      "Epoch 389: train loss 4.966373443603516 val loss 4.966221809387207\n",
      "Epoch 390: train loss 4.966221809387207 val loss 4.966254234313965\n",
      "Epoch 391: train loss 4.966254234313965 val loss 4.965807914733887\n",
      "Epoch 392: train loss 4.965807914733887 val loss 4.965672969818115\n",
      "Epoch 393: train loss 4.965672969818115 val loss 4.965756416320801\n",
      "Epoch 394: train loss 4.965756416320801 val loss 4.965758323669434\n",
      "Epoch 395: train loss 4.965758323669434 val loss 4.965909957885742\n",
      "Epoch 396: train loss 4.965909957885742 val loss 4.966969966888428\n",
      "Epoch 397: train loss 4.966969966888428 val loss 4.966053009033203\n",
      "Epoch 398: train loss 4.966053009033203 val loss 4.968355655670166\n",
      "Epoch 399: train loss 4.968355655670166 val loss 4.96766996383667\n",
      "Epoch 400: train loss 4.96766996383667 val loss 4.966811656951904\n",
      "Epoch 401: train loss 4.966811656951904 val loss 4.967343330383301\n",
      "Epoch 402: train loss 4.967343330383301 val loss 4.966248512268066\n",
      "Epoch 403: train loss 4.966248512268066 val loss 4.9667582511901855\n",
      "Epoch 404: train loss 4.9667582511901855 val loss 4.966364860534668\n",
      "Epoch 405: train loss 4.966364860534668 val loss 4.966755390167236\n",
      "Epoch 406: train loss 4.966755390167236 val loss 4.966368675231934\n",
      "Epoch 407: train loss 4.966368675231934 val loss 4.966233253479004\n",
      "Epoch 408: train loss 4.966233253479004 val loss 4.966461658477783\n",
      "Epoch 409: train loss 4.966461658477783 val loss 4.966253280639648\n",
      "Epoch 410: train loss 4.966253280639648 val loss 4.967281341552734\n",
      "Epoch 411: train loss 4.967281341552734 val loss 4.966556072235107\n",
      "Epoch 412: train loss 4.966556072235107 val loss 4.966855525970459\n",
      "Epoch 413: train loss 4.966855525970459 val loss 4.967209815979004\n",
      "Epoch 414: train loss 4.967209815979004 val loss 4.966104984283447\n",
      "Epoch 415: train loss 4.966104984283447 val loss 4.966424942016602\n",
      "Epoch 416: train loss 4.966424942016602 val loss 4.965914726257324\n",
      "Epoch 417: train loss 4.965914726257324 val loss 4.966361999511719\n",
      "Epoch 418: train loss 4.966361999511719 val loss 4.966015815734863\n",
      "Epoch 419: train loss 4.966015815734863 val loss 4.966146469116211\n",
      "Epoch 420: train loss 4.966146469116211 val loss 4.966047763824463\n",
      "Epoch 421: train loss 4.966047763824463 val loss 4.965906143188477\n",
      "Epoch 422: train loss 4.965906143188477 val loss 4.9656524658203125\n",
      "Epoch 423: train loss 4.9656524658203125 val loss 4.965770244598389\n",
      "Epoch 424: train loss 4.965770244598389 val loss 4.96629524230957\n",
      "Epoch 425: train loss 4.96629524230957 val loss 4.9662017822265625\n",
      "Epoch 426: train loss 4.9662017822265625 val loss 4.966127395629883\n",
      "Epoch 427: train loss 4.966127395629883 val loss 4.965888977050781\n",
      "Epoch 428: train loss 4.965888977050781 val loss 4.96624231338501\n",
      "Epoch 429: train loss 4.96624231338501 val loss 4.965859413146973\n",
      "Epoch 430: train loss 4.965859413146973 val loss 4.966059684753418\n",
      "Epoch 431: train loss 4.966059684753418 val loss 4.965974807739258\n",
      "Epoch 432: train loss 4.965974807739258 val loss 4.966517925262451\n",
      "Epoch 433: train loss 4.966517925262451 val loss 4.966025352478027\n",
      "Epoch 434: train loss 4.966025352478027 val loss 4.966076850891113\n",
      "Epoch 435: train loss 4.966076850891113 val loss 4.96616792678833\n",
      "Epoch 436: train loss 4.96616792678833 val loss 4.966067314147949\n",
      "Epoch 437: train loss 4.966067314147949 val loss 4.965786933898926\n",
      "Epoch 438: train loss 4.965786933898926 val loss 4.965847015380859\n",
      "Epoch 439: train loss 4.965847015380859 val loss 4.965950012207031\n",
      "Epoch 440: train loss 4.965950012207031 val loss 4.966135025024414\n",
      "Epoch 441: train loss 4.966135025024414 val loss 4.966041564941406\n",
      "Epoch 442: train loss 4.966041564941406 val loss 4.966239929199219\n",
      "Epoch 443: train loss 4.966239929199219 val loss 4.965677261352539\n",
      "Epoch 444: train loss 4.965677261352539 val loss 4.966562271118164\n",
      "Epoch 445: train loss 4.966562271118164 val loss 4.9660162925720215\n",
      "Epoch 446: train loss 4.9660162925720215 val loss 4.966114044189453\n",
      "Epoch 447: train loss 4.966114044189453 val loss 4.96650505065918\n",
      "Epoch 448: train loss 4.96650505065918 val loss 4.9660820960998535\n",
      "Epoch 449: train loss 4.9660820960998535 val loss 4.96622371673584\n",
      "Epoch 450: train loss 4.96622371673584 val loss 4.9663496017456055\n",
      "Epoch 451: train loss 4.9663496017456055 val loss 4.9671711921691895\n",
      "Epoch 452: train loss 4.9671711921691895 val loss 4.965936660766602\n",
      "Epoch 453: train loss 4.965936660766602 val loss 4.968213081359863\n",
      "Epoch 454: train loss 4.968213081359863 val loss 4.966736793518066\n",
      "Epoch 455: train loss 4.966736793518066 val loss 4.966732978820801\n",
      "Epoch 456: train loss 4.966732978820801 val loss 4.967851638793945\n",
      "Epoch 457: train loss 4.967851638793945 val loss 4.96590518951416\n",
      "Epoch 458: train loss 4.96590518951416 val loss 4.968564987182617\n",
      "Epoch 459: train loss 4.968564987182617 val loss 4.967524528503418\n",
      "Epoch 460: train loss 4.967524528503418 val loss 4.966564655303955\n",
      "Epoch 461: train loss 4.966564655303955 val loss 4.969013690948486\n",
      "Epoch 462: train loss 4.969013690948486 val loss 4.966462135314941\n",
      "Epoch 463: train loss 4.966462135314941 val loss 4.967242240905762\n",
      "Epoch 464: train loss 4.967242240905762 val loss 4.967798709869385\n",
      "Epoch 465: train loss 4.967798709869385 val loss 4.96609354019165\n",
      "Epoch 466: train loss 4.96609354019165 val loss 4.967207908630371\n",
      "Epoch 467: train loss 4.967207908630371 val loss 4.966513633728027\n",
      "Epoch 468: train loss 4.966513633728027 val loss 4.9659810066223145\n",
      "Epoch 469: train loss 4.9659810066223145 val loss 4.965734481811523\n",
      "Epoch 470: train loss 4.965734481811523 val loss 4.965973377227783\n",
      "Epoch 471: train loss 4.965973377227783 val loss 4.965862274169922\n",
      "Epoch 472: train loss 4.965862274169922 val loss 4.96610164642334\n",
      "Epoch 473: train loss 4.96610164642334 val loss 4.965860366821289\n",
      "Epoch 474: train loss 4.965860366821289 val loss 4.966166019439697\n",
      "Epoch 475: train loss 4.966166019439697 val loss 4.96568489074707\n",
      "Epoch 476: train loss 4.96568489074707 val loss 4.967528343200684\n",
      "Epoch 477: train loss 4.967528343200684 val loss 4.966111183166504\n",
      "Epoch 478: train loss 4.966111183166504 val loss 4.967751979827881\n",
      "Epoch 479: train loss 4.967751979827881 val loss 4.967119216918945\n",
      "Epoch 480: train loss 4.967119216918945 val loss 4.966147422790527\n",
      "Epoch 481: train loss 4.966147422790527 val loss 4.968708515167236\n",
      "Epoch 482: train loss 4.968708515167236 val loss 4.967206001281738\n",
      "Epoch 483: train loss 4.967206001281738 val loss 4.966705322265625\n",
      "Epoch 484: train loss 4.966705322265625 val loss 4.969930648803711\n",
      "Epoch 485: train loss 4.969930648803711 val loss 4.967574119567871\n",
      "Epoch 486: train loss 4.967574119567871 val loss 4.967148303985596\n",
      "Epoch 487: train loss 4.967148303985596 val loss 4.969538688659668\n",
      "Epoch 488: train loss 4.969538688659668 val loss 4.967582702636719\n",
      "Epoch 489: train loss 4.967582702636719 val loss 4.967012882232666\n",
      "Epoch 490: train loss 4.967012882232666 val loss 4.968764305114746\n",
      "Epoch 491: train loss 4.968764305114746 val loss 4.9667158126831055\n",
      "Epoch 492: train loss 4.9667158126831055 val loss 4.966651439666748\n",
      "Epoch 493: train loss 4.966651439666748 val loss 4.96696662902832\n",
      "Epoch 494: train loss 4.96696662902832 val loss 4.9663848876953125\n",
      "Epoch 495: train loss 4.9663848876953125 val loss 4.967094421386719\n",
      "Epoch 496: train loss 4.967094421386719 val loss 4.966670989990234\n",
      "Epoch 497: train loss 4.966670989990234 val loss 4.965909957885742\n",
      "Epoch 498: train loss 4.965909957885742 val loss 4.967123031616211\n",
      "Epoch 499: train loss 4.967123031616211 val loss 4.966172218322754\n",
      "Epoch 500: train loss 4.966172218322754 val loss 4.966470241546631\n",
      "Epoch 501: train loss 4.966470241546631 val loss 4.966742992401123\n",
      "Epoch 502: train loss 4.966742992401123 val loss 4.965746879577637\n",
      "Epoch 503: train loss 4.965746879577637 val loss 4.967081546783447\n",
      "Epoch 504: train loss 4.967081546783447 val loss 4.96582555770874\n",
      "Epoch 505: train loss 4.96582555770874 val loss 4.967144012451172\n",
      "Epoch 506: train loss 4.967144012451172 val loss 4.966337203979492\n",
      "Epoch 507: train loss 4.966337203979492 val loss 4.966814041137695\n",
      "Epoch 508: train loss 4.966814041137695 val loss 4.967120170593262\n",
      "Epoch 509: train loss 4.967120170593262 val loss 4.966109752655029\n",
      "Epoch 510: train loss 4.966109752655029 val loss 4.967230796813965\n",
      "Epoch 511: train loss 4.967230796813965 val loss 4.966038703918457\n",
      "Epoch 512: train loss 4.966038703918457 val loss 4.96579122543335\n",
      "Epoch 513: train loss 4.96579122543335 val loss 4.965579986572266\n",
      "Epoch 514: train loss 4.965579986572266 val loss 4.965537071228027\n",
      "Epoch 515: train loss 4.965537071228027 val loss 4.965904235839844\n",
      "Epoch 516: train loss 4.965904235839844 val loss 4.966001510620117\n",
      "Epoch 517: train loss 4.966001510620117 val loss 4.965848445892334\n",
      "Epoch 518: train loss 4.965848445892334 val loss 4.9666972160339355\n",
      "Epoch 519: train loss 4.9666972160339355 val loss 4.9656758308410645\n",
      "Epoch 520: train loss 4.9656758308410645 val loss 4.966825485229492\n",
      "Epoch 521: train loss 4.966825485229492 val loss 4.966148376464844\n",
      "Epoch 522: train loss 4.966148376464844 val loss 4.966438293457031\n",
      "Epoch 523: train loss 4.966438293457031 val loss 4.9667439460754395\n",
      "Epoch 524: train loss 4.9667439460754395 val loss 4.966603755950928\n",
      "Epoch 525: train loss 4.966603755950928 val loss 4.967496395111084\n",
      "Epoch 526: train loss 4.967496395111084 val loss 4.966836929321289\n",
      "Epoch 527: train loss 4.966836929321289 val loss 4.966166019439697\n",
      "Epoch 528: train loss 4.966166019439697 val loss 4.967898368835449\n",
      "Epoch 529: train loss 4.967898368835449 val loss 4.9665327072143555\n",
      "Epoch 530: train loss 4.9665327072143555 val loss 4.966948509216309\n",
      "Epoch 531: train loss 4.966948509216309 val loss 4.9674577713012695\n",
      "Epoch 532: train loss 4.9674577713012695 val loss 4.965656280517578\n",
      "Epoch 533: train loss 4.965656280517578 val loss 4.967000484466553\n",
      "Epoch 534: train loss 4.967000484466553 val loss 4.965635299682617\n",
      "Epoch 535: train loss 4.965635299682617 val loss 4.9658098220825195\n",
      "Epoch 536: train loss 4.9658098220825195 val loss 4.965654373168945\n",
      "Epoch 537: train loss 4.965654373168945 val loss 4.966151237487793\n",
      "Epoch 538: train loss 4.966151237487793 val loss 4.965900897979736\n",
      "Epoch 539: train loss 4.965900897979736 val loss 4.966425895690918\n",
      "Epoch 540: train loss 4.966425895690918 val loss 4.9658122062683105\n",
      "Epoch 541: train loss 4.9658122062683105 val loss 4.966852188110352\n",
      "Epoch 542: train loss 4.966852188110352 val loss 4.965603828430176\n",
      "Epoch 543: train loss 4.965603828430176 val loss 4.9659013748168945\n",
      "Epoch 544: train loss 4.9659013748168945 val loss 4.965751647949219\n",
      "Epoch 545: train loss 4.965751647949219 val loss 4.965853214263916\n",
      "Epoch 546: train loss 4.965853214263916 val loss 4.965871334075928\n",
      "Epoch 547: train loss 4.965871334075928 val loss 4.965759754180908\n",
      "Epoch 548: train loss 4.965759754180908 val loss 4.96589469909668\n",
      "Epoch 549: train loss 4.96589469909668 val loss 4.965669631958008\n",
      "Epoch 550: train loss 4.965669631958008 val loss 4.965679168701172\n",
      "Epoch 551: train loss 4.965679168701172 val loss 4.965968132019043\n",
      "Epoch 552: train loss 4.965968132019043 val loss 4.965997695922852\n",
      "Epoch 553: train loss 4.965997695922852 val loss 4.965634346008301\n",
      "Epoch 554: train loss 4.965634346008301 val loss 4.966988563537598\n",
      "Epoch 555: train loss 4.966988563537598 val loss 4.96565580368042\n",
      "Epoch 556: train loss 4.96565580368042 val loss 4.967443466186523\n",
      "Epoch 557: train loss 4.967443466186523 val loss 4.966548919677734\n",
      "Epoch 558: train loss 4.966548919677734 val loss 4.966217994689941\n",
      "Epoch 559: train loss 4.966217994689941 val loss 4.9665985107421875\n",
      "Epoch 560: train loss 4.9665985107421875 val loss 4.965937614440918\n",
      "Epoch 561: train loss 4.965937614440918 val loss 4.966975212097168\n",
      "Epoch 562: train loss 4.966975212097168 val loss 4.965963840484619\n",
      "Epoch 563: train loss 4.965963840484619 val loss 4.965809345245361\n",
      "Epoch 564: train loss 4.965809345245361 val loss 4.966899871826172\n",
      "Epoch 565: train loss 4.966899871826172 val loss 4.965800762176514\n",
      "Epoch 566: train loss 4.965800762176514 val loss 4.966132164001465\n",
      "Epoch 567: train loss 4.966132164001465 val loss 4.96565580368042\n",
      "Epoch 568: train loss 4.96565580368042 val loss 4.966735363006592\n",
      "Epoch 569: train loss 4.966735363006592 val loss 4.965753078460693\n",
      "Epoch 570: train loss 4.965753078460693 val loss 4.966681480407715\n",
      "Epoch 571: train loss 4.966681480407715 val loss 4.965970993041992\n",
      "Epoch 572: train loss 4.965970993041992 val loss 4.9665069580078125\n",
      "Epoch 573: train loss 4.9665069580078125 val loss 4.965824604034424\n",
      "Epoch 574: train loss 4.965824604034424 val loss 4.966713905334473\n",
      "Epoch 575: train loss 4.966713905334473 val loss 4.966490745544434\n",
      "Epoch 576: train loss 4.966490745544434 val loss 4.965795040130615\n",
      "Epoch 577: train loss 4.965795040130615 val loss 4.96774435043335\n",
      "Epoch 578: train loss 4.96774435043335 val loss 4.96615743637085\n",
      "Epoch 579: train loss 4.96615743637085 val loss 4.96694803237915\n",
      "Epoch 580: train loss 4.96694803237915 val loss 4.967403888702393\n",
      "Epoch 581: train loss 4.967403888702393 val loss 4.965941429138184\n",
      "Epoch 582: train loss 4.965941429138184 val loss 4.96820068359375\n",
      "Epoch 583: train loss 4.96820068359375 val loss 4.967184066772461\n",
      "Epoch 584: train loss 4.967184066772461 val loss 4.966530799865723\n",
      "Epoch 585: train loss 4.966530799865723 val loss 4.968137741088867\n",
      "Epoch 586: train loss 4.968137741088867 val loss 4.966259956359863\n",
      "Epoch 587: train loss 4.966259956359863 val loss 4.96647834777832\n",
      "Epoch 588: train loss 4.96647834777832 val loss 4.96663236618042\n",
      "Epoch 589: train loss 4.96663236618042 val loss 4.965908527374268\n",
      "Epoch 590: train loss 4.965908527374268 val loss 4.966690540313721\n",
      "Epoch 591: train loss 4.966690540313721 val loss 4.96601676940918\n",
      "Epoch 592: train loss 4.96601676940918 val loss 4.966172218322754\n",
      "Epoch 593: train loss 4.966172218322754 val loss 4.965846538543701\n",
      "Epoch 594: train loss 4.965846538543701 val loss 4.965775489807129\n",
      "Epoch 595: train loss 4.965775489807129 val loss 4.965633869171143\n",
      "Epoch 596: train loss 4.965633869171143 val loss 4.966257095336914\n",
      "Epoch 597: train loss 4.966257095336914 val loss 4.9659576416015625\n",
      "Epoch 598: train loss 4.9659576416015625 val loss 4.965950965881348\n",
      "Epoch 599: train loss 4.965950965881348 val loss 4.965824127197266\n",
      "Epoch 600: train loss 4.965824127197266 val loss 4.9665069580078125\n",
      "Epoch 601: train loss 4.9665069580078125 val loss 4.965804100036621\n",
      "Epoch 602: train loss 4.965804100036621 val loss 4.965949058532715\n",
      "Epoch 603: train loss 4.965949058532715 val loss 4.965947151184082\n",
      "Epoch 604: train loss 4.965947151184082 val loss 4.965949535369873\n",
      "Epoch 605: train loss 4.965949535369873 val loss 4.965764999389648\n",
      "Epoch 606: train loss 4.965764999389648 val loss 4.965600967407227\n",
      "Epoch 607: train loss 4.965600967407227 val loss 4.966246128082275\n",
      "Epoch 608: train loss 4.966246128082275 val loss 4.965824127197266\n",
      "Epoch 609: train loss 4.965824127197266 val loss 4.96697998046875\n",
      "Epoch 610: train loss 4.96697998046875 val loss 4.966098308563232\n",
      "Epoch 611: train loss 4.966098308563232 val loss 4.966394901275635\n",
      "Epoch 612: train loss 4.966394901275635 val loss 4.965889930725098\n",
      "Epoch 613: train loss 4.965889930725098 val loss 4.9662394523620605\n",
      "Epoch 614: train loss 4.9662394523620605 val loss 4.965996742248535\n",
      "Epoch 615: train loss 4.965996742248535 val loss 4.966526031494141\n",
      "Epoch 616: train loss 4.966526031494141 val loss 4.966373443603516\n",
      "Epoch 617: train loss 4.966373443603516 val loss 4.966003894805908\n",
      "Epoch 618: train loss 4.966003894805908 val loss 4.966139793395996\n",
      "Epoch 619: train loss 4.966139793395996 val loss 4.965847492218018\n",
      "Epoch 620: train loss 4.965847492218018 val loss 4.966254711151123\n",
      "Epoch 621: train loss 4.966254711151123 val loss 4.966194152832031\n",
      "Epoch 622: train loss 4.966194152832031 val loss 4.965899467468262\n",
      "Epoch 623: train loss 4.965899467468262 val loss 4.967098236083984\n",
      "Epoch 624: train loss 4.967098236083984 val loss 4.965670585632324\n",
      "Epoch 625: train loss 4.965670585632324 val loss 4.96574068069458\n",
      "Epoch 626: train loss 4.96574068069458 val loss 4.965615272521973\n",
      "Epoch 627: train loss 4.965615272521973 val loss 4.965709686279297\n",
      "Epoch 628: train loss 4.965709686279297 val loss 4.9656758308410645\n",
      "Epoch 629: train loss 4.9656758308410645 val loss 4.965978145599365\n",
      "Epoch 630: train loss 4.965978145599365 val loss 4.965855598449707\n",
      "Epoch 631: train loss 4.965855598449707 val loss 4.965581893920898\n",
      "Epoch 632: train loss 4.965581893920898 val loss 4.965543270111084\n",
      "Epoch 633: train loss 4.965543270111084 val loss 4.965590476989746\n",
      "Epoch 634: train loss 4.965590476989746 val loss 4.96596622467041\n",
      "Epoch 635: train loss 4.96596622467041 val loss 4.965700626373291\n",
      "Epoch 636: train loss 4.965700626373291 val loss 4.965785980224609\n",
      "Epoch 637: train loss 4.965785980224609 val loss 4.965613842010498\n",
      "Epoch 638: train loss 4.965613842010498 val loss 4.965969562530518\n",
      "Epoch 639: train loss 4.965969562530518 val loss 4.9656219482421875\n",
      "Epoch 640: train loss 4.9656219482421875 val loss 4.965695381164551\n",
      "Epoch 641: train loss 4.965695381164551 val loss 4.965701580047607\n",
      "Epoch 642: train loss 4.965701580047607 val loss 4.965662956237793\n",
      "Epoch 643: train loss 4.965662956237793 val loss 4.965749263763428\n",
      "Epoch 644: train loss 4.965749263763428 val loss 4.965553283691406\n",
      "Epoch 645: train loss 4.965553283691406 val loss 4.965895652770996\n",
      "Epoch 646: train loss 4.965895652770996 val loss 4.965597152709961\n",
      "Epoch 647: train loss 4.965597152709961 val loss 4.965779781341553\n",
      "Epoch 648: train loss 4.965779781341553 val loss 4.965643882751465\n",
      "Epoch 649: train loss 4.965643882751465 val loss 4.965579032897949\n",
      "Epoch 650: train loss 4.965579032897949 val loss 4.965435028076172\n",
      "Epoch 651: train loss 4.965435028076172 val loss 4.965777397155762\n",
      "Epoch 652: train loss 4.965777397155762 val loss 4.96572732925415\n",
      "Epoch 653: train loss 4.96572732925415 val loss 4.9657135009765625\n",
      "Epoch 654: train loss 4.9657135009765625 val loss 4.965354919433594\n",
      "Epoch 655: train loss 4.965354919433594 val loss 4.965578079223633\n",
      "Epoch 656: train loss 4.965578079223633 val loss 4.96565580368042\n",
      "Epoch 657: train loss 4.96565580368042 val loss 4.965588092803955\n",
      "Epoch 658: train loss 4.965588092803955 val loss 4.965811729431152\n",
      "Epoch 659: train loss 4.965811729431152 val loss 4.966157913208008\n",
      "Epoch 660: train loss 4.966157913208008 val loss 4.965779781341553\n",
      "Epoch 661: train loss 4.965779781341553 val loss 4.965871810913086\n",
      "Epoch 662: train loss 4.965871810913086 val loss 4.965641975402832\n",
      "Epoch 663: train loss 4.965641975402832 val loss 4.96553897857666\n",
      "Epoch 664: train loss 4.96553897857666 val loss 4.9657087326049805\n",
      "Epoch 665: train loss 4.9657087326049805 val loss 4.965820789337158\n",
      "Epoch 666: train loss 4.965820789337158 val loss 4.96575927734375\n",
      "Epoch 667: train loss 4.96575927734375 val loss 4.9657158851623535\n",
      "Epoch 668: train loss 4.9657158851623535 val loss 4.9656877517700195\n",
      "Epoch 669: train loss 4.9656877517700195 val loss 4.96568489074707\n",
      "Epoch 670: train loss 4.96568489074707 val loss 4.965956687927246\n",
      "Epoch 671: train loss 4.965956687927246 val loss 4.965821266174316\n",
      "Epoch 672: train loss 4.965821266174316 val loss 4.965641021728516\n",
      "Epoch 673: train loss 4.965641021728516 val loss 4.965868949890137\n",
      "Epoch 674: train loss 4.965868949890137 val loss 4.965414524078369\n",
      "Epoch 675: train loss 4.965414524078369 val loss 4.965712070465088\n",
      "Epoch 676: train loss 4.965712070465088 val loss 4.965785503387451\n",
      "Epoch 677: train loss 4.965785503387451 val loss 4.965573310852051\n",
      "Epoch 678: train loss 4.965573310852051 val loss 4.96553897857666\n",
      "Epoch 679: train loss 4.96553897857666 val loss 4.96602725982666\n",
      "Epoch 680: train loss 4.96602725982666 val loss 4.965694427490234\n",
      "Epoch 681: train loss 4.965694427490234 val loss 4.966509819030762\n",
      "Epoch 682: train loss 4.966509819030762 val loss 4.96611213684082\n",
      "Epoch 683: train loss 4.96611213684082 val loss 4.966065883636475\n",
      "Epoch 684: train loss 4.966065883636475 val loss 4.965862274169922\n",
      "Epoch 685: train loss 4.965862274169922 val loss 4.965794086456299\n",
      "Epoch 686: train loss 4.965794086456299 val loss 4.965854644775391\n",
      "Epoch 687: train loss 4.965854644775391 val loss 4.965964317321777\n",
      "Epoch 688: train loss 4.965964317321777 val loss 4.9657979011535645\n",
      "Epoch 689: train loss 4.9657979011535645 val loss 4.966179370880127\n",
      "Epoch 690: train loss 4.966179370880127 val loss 4.9658379554748535\n",
      "Epoch 691: train loss 4.9658379554748535 val loss 4.966014862060547\n",
      "Epoch 692: train loss 4.966014862060547 val loss 4.965742111206055\n",
      "Epoch 693: train loss 4.965742111206055 val loss 4.965756416320801\n",
      "Epoch 694: train loss 4.965756416320801 val loss 4.96581506729126\n",
      "Epoch 695: train loss 4.96581506729126 val loss 4.965572357177734\n",
      "Epoch 696: train loss 4.965572357177734 val loss 4.9658074378967285\n",
      "Epoch 697: train loss 4.9658074378967285 val loss 4.96630859375\n",
      "Epoch 698: train loss 4.96630859375 val loss 4.965757369995117\n",
      "Epoch 699: train loss 4.965757369995117 val loss 4.966296195983887\n",
      "Epoch 700: train loss 4.966296195983887 val loss 4.965728759765625\n",
      "Epoch 701: train loss 4.965728759765625 val loss 4.96617317199707\n",
      "Epoch 702: train loss 4.96617317199707 val loss 4.965815544128418\n",
      "Epoch 703: train loss 4.965815544128418 val loss 4.966440677642822\n",
      "Epoch 704: train loss 4.966440677642822 val loss 4.966193675994873\n",
      "Epoch 705: train loss 4.966193675994873 val loss 4.966274738311768\n",
      "Epoch 706: train loss 4.966274738311768 val loss 4.965801239013672\n",
      "Epoch 707: train loss 4.965801239013672 val loss 4.965906620025635\n",
      "Epoch 708: train loss 4.965906620025635 val loss 4.965871810913086\n",
      "Epoch 709: train loss 4.965871810913086 val loss 4.965932846069336\n",
      "Epoch 710: train loss 4.965932846069336 val loss 4.96577262878418\n",
      "Epoch 711: train loss 4.96577262878418 val loss 4.965747833251953\n",
      "Epoch 712: train loss 4.965747833251953 val loss 4.965465545654297\n",
      "Epoch 713: train loss 4.965465545654297 val loss 4.965534210205078\n",
      "Epoch 714: train loss 4.965534210205078 val loss 4.965529441833496\n",
      "Epoch 715: train loss 4.965529441833496 val loss 4.965624809265137\n",
      "Epoch 716: train loss 4.965624809265137 val loss 4.966336727142334\n",
      "Epoch 717: train loss 4.966336727142334 val loss 4.965609550476074\n",
      "Epoch 718: train loss 4.965609550476074 val loss 4.967601299285889\n",
      "Epoch 719: train loss 4.967601299285889 val loss 4.966609001159668\n",
      "Epoch 720: train loss 4.966609001159668 val loss 4.967355728149414\n",
      "Epoch 721: train loss 4.967355728149414 val loss 4.967432498931885\n",
      "Epoch 722: train loss 4.967432498931885 val loss 4.966245651245117\n",
      "Epoch 723: train loss 4.966245651245117 val loss 4.967534065246582\n",
      "Epoch 724: train loss 4.967534065246582 val loss 4.966097354888916\n",
      "Epoch 725: train loss 4.966097354888916 val loss 4.967066764831543\n",
      "Epoch 726: train loss 4.967066764831543 val loss 4.966944217681885\n",
      "Epoch 727: train loss 4.966944217681885 val loss 4.965918064117432\n",
      "Epoch 728: train loss 4.965918064117432 val loss 4.968344211578369\n",
      "Epoch 729: train loss 4.968344211578369 val loss 4.967161655426025\n",
      "Epoch 730: train loss 4.967161655426025 val loss 4.966769218444824\n",
      "Epoch 731: train loss 4.966769218444824 val loss 4.968216419219971\n",
      "Epoch 732: train loss 4.968216419219971 val loss 4.9662370681762695\n",
      "Epoch 733: train loss 4.9662370681762695 val loss 4.9676337242126465\n",
      "Epoch 734: train loss 4.9676337242126465 val loss 4.967878341674805\n",
      "Epoch 735: train loss 4.967878341674805 val loss 4.965944290161133\n",
      "Epoch 736: train loss 4.965944290161133 val loss 4.966869354248047\n",
      "Epoch 737: train loss 4.966869354248047 val loss 4.966495513916016\n",
      "Epoch 738: train loss 4.966495513916016 val loss 4.965842247009277\n",
      "Epoch 739: train loss 4.965842247009277 val loss 4.966091632843018\n",
      "Epoch 740: train loss 4.966091632843018 val loss 4.965850830078125\n",
      "Epoch 741: train loss 4.965850830078125 val loss 4.966067314147949\n",
      "Epoch 742: train loss 4.966067314147949 val loss 4.965919017791748\n",
      "Epoch 743: train loss 4.965919017791748 val loss 4.966376781463623\n",
      "Epoch 744: train loss 4.966376781463623 val loss 4.96596622467041\n",
      "Epoch 745: train loss 4.96596622467041 val loss 4.966722011566162\n",
      "Epoch 746: train loss 4.966722011566162 val loss 4.96652364730835\n",
      "Epoch 747: train loss 4.96652364730835 val loss 4.965660572052002\n",
      "Epoch 748: train loss 4.965660572052002 val loss 4.96561861038208\n",
      "Epoch 749: train loss 4.96561861038208 val loss 4.966470718383789\n",
      "Epoch 750: train loss 4.966470718383789 val loss 4.966166019439697\n",
      "Epoch 751: train loss 4.966166019439697 val loss 4.966096878051758\n",
      "Epoch 752: train loss 4.966096878051758 val loss 4.966237545013428\n",
      "Epoch 753: train loss 4.966237545013428 val loss 4.966009616851807\n",
      "Epoch 754: train loss 4.966009616851807 val loss 4.966180324554443\n",
      "Epoch 755: train loss 4.966180324554443 val loss 4.965882301330566\n",
      "Epoch 756: train loss 4.965882301330566 val loss 4.967426776885986\n",
      "Epoch 757: train loss 4.967426776885986 val loss 4.965950965881348\n",
      "Epoch 758: train loss 4.965950965881348 val loss 4.967146873474121\n",
      "Epoch 759: train loss 4.967146873474121 val loss 4.966968536376953\n",
      "Epoch 760: train loss 4.966968536376953 val loss 4.966163635253906\n",
      "Epoch 761: train loss 4.966163635253906 val loss 4.9681501388549805\n",
      "Epoch 762: train loss 4.9681501388549805 val loss 4.966629981994629\n",
      "Epoch 763: train loss 4.966629981994629 val loss 4.96676778793335\n",
      "Epoch 764: train loss 4.96676778793335 val loss 4.96754264831543\n",
      "Epoch 765: train loss 4.96754264831543 val loss 4.965594291687012\n",
      "Epoch 766: train loss 4.965594291687012 val loss 4.967429161071777\n",
      "Epoch 767: train loss 4.967429161071777 val loss 4.9661431312561035\n",
      "Epoch 768: train loss 4.9661431312561035 val loss 4.966741561889648\n",
      "Epoch 769: train loss 4.966741561889648 val loss 4.967019081115723\n",
      "Epoch 770: train loss 4.967019081115723 val loss 4.965476036071777\n",
      "Epoch 771: train loss 4.965476036071777 val loss 4.967498779296875\n",
      "Epoch 772: train loss 4.967498779296875 val loss 4.966023921966553\n",
      "Epoch 773: train loss 4.966023921966553 val loss 4.9667253494262695\n",
      "Epoch 774: train loss 4.9667253494262695 val loss 4.966952800750732\n",
      "Epoch 775: train loss 4.966952800750732 val loss 4.965488433837891\n",
      "Epoch 776: train loss 4.965488433837891 val loss 4.968762397766113\n",
      "Epoch 777: train loss 4.968762397766113 val loss 4.968315124511719\n",
      "Epoch 778: train loss 4.968315124511719 val loss 4.965859413146973\n",
      "Epoch 779: train loss 4.965859413146973 val loss 4.968422889709473\n",
      "Epoch 780: train loss 4.968422889709473 val loss 4.967812538146973\n",
      "Epoch 781: train loss 4.967812538146973 val loss 4.96572208404541\n",
      "Epoch 782: train loss 4.96572208404541 val loss 4.968762397766113\n",
      "Epoch 783: train loss 4.968762397766113 val loss 4.967888832092285\n",
      "Epoch 784: train loss 4.967888832092285 val loss 4.9662017822265625\n",
      "Epoch 785: train loss 4.9662017822265625 val loss 4.968415260314941\n",
      "Epoch 786: train loss 4.968415260314941 val loss 4.967172622680664\n",
      "Epoch 787: train loss 4.967172622680664 val loss 4.966154098510742\n",
      "Epoch 788: train loss 4.966154098510742 val loss 4.967655181884766\n",
      "Epoch 789: train loss 4.967655181884766 val loss 4.965726852416992\n",
      "Epoch 790: train loss 4.965726852416992 val loss 4.966745376586914\n",
      "Epoch 791: train loss 4.966745376586914 val loss 4.967276573181152\n",
      "Epoch 792: train loss 4.967276573181152 val loss 4.966061115264893\n",
      "Epoch 793: train loss 4.966061115264893 val loss 4.96779203414917\n",
      "Epoch 794: train loss 4.96779203414917 val loss 4.966795444488525\n",
      "Epoch 795: train loss 4.966795444488525 val loss 4.966728210449219\n",
      "Epoch 796: train loss 4.966728210449219 val loss 4.96848201751709\n",
      "Epoch 797: train loss 4.96848201751709 val loss 4.967021942138672\n",
      "Epoch 798: train loss 4.967021942138672 val loss 4.966142654418945\n",
      "Epoch 799: train loss 4.966142654418945 val loss 4.967683792114258\n",
      "Epoch 800: train loss 4.967683792114258 val loss 4.966085910797119\n",
      "Epoch 801: train loss 4.966085910797119 val loss 4.967240810394287\n",
      "Epoch 802: train loss 4.967240810394287 val loss 4.96791934967041\n",
      "Epoch 803: train loss 4.96791934967041 val loss 4.966244697570801\n",
      "Epoch 804: train loss 4.966244697570801 val loss 4.96856689453125\n",
      "Epoch 805: train loss 4.96856689453125 val loss 4.968632698059082\n",
      "Epoch 806: train loss 4.968632698059082 val loss 4.966032028198242\n",
      "Epoch 807: train loss 4.966032028198242 val loss 4.9666948318481445\n",
      "Epoch 808: train loss 4.9666948318481445 val loss 4.967148303985596\n",
      "Epoch 809: train loss 4.967148303985596 val loss 4.966087341308594\n",
      "Epoch 810: train loss 4.966087341308594 val loss 4.9665727615356445\n",
      "Epoch 811: train loss 4.9665727615356445 val loss 4.966366767883301\n",
      "Epoch 812: train loss 4.966366767883301 val loss 4.966184139251709\n",
      "Epoch 813: train loss 4.966184139251709 val loss 4.966910362243652\n",
      "Epoch 814: train loss 4.966910362243652 val loss 4.965903282165527\n",
      "Epoch 815: train loss 4.965903282165527 val loss 4.966731071472168\n",
      "Epoch 816: train loss 4.966731071472168 val loss 4.9656877517700195\n",
      "Epoch 817: train loss 4.9656877517700195 val loss 4.967059135437012\n",
      "Epoch 818: train loss 4.967059135437012 val loss 4.967188835144043\n",
      "Epoch 819: train loss 4.967188835144043 val loss 4.965771198272705\n",
      "Epoch 820: train loss 4.965771198272705 val loss 4.966450214385986\n",
      "Epoch 821: train loss 4.966450214385986 val loss 4.965843200683594\n",
      "Epoch 822: train loss 4.965843200683594 val loss 4.966482639312744\n",
      "Epoch 823: train loss 4.966482639312744 val loss 4.966411590576172\n",
      "Epoch 824: train loss 4.966411590576172 val loss 4.966063022613525\n",
      "Epoch 825: train loss 4.966063022613525 val loss 4.967116832733154\n",
      "Epoch 826: train loss 4.967116832733154 val loss 4.966366767883301\n",
      "Epoch 827: train loss 4.966366767883301 val loss 4.9661865234375\n",
      "Epoch 828: train loss 4.9661865234375 val loss 4.965981483459473\n",
      "Epoch 829: train loss 4.965981483459473 val loss 4.965822696685791\n",
      "Epoch 830: train loss 4.965822696685791 val loss 4.965343475341797\n",
      "Epoch 831: train loss 4.965343475341797 val loss 4.965422630310059\n",
      "Epoch 832: train loss 4.965422630310059 val loss 4.96584939956665\n",
      "Epoch 833: train loss 4.96584939956665 val loss 4.965622901916504\n",
      "Epoch 834: train loss 4.965622901916504 val loss 4.966091156005859\n",
      "Epoch 835: train loss 4.966091156005859 val loss 4.965581893920898\n",
      "Epoch 836: train loss 4.965581893920898 val loss 4.965607166290283\n",
      "Epoch 837: train loss 4.965607166290283 val loss 4.965446949005127\n",
      "Epoch 838: train loss 4.965446949005127 val loss 4.965761661529541\n",
      "Epoch 839: train loss 4.965761661529541 val loss 4.965435028076172\n",
      "Epoch 840: train loss 4.965435028076172 val loss 4.96567440032959\n",
      "Epoch 841: train loss 4.96567440032959 val loss 4.96575927734375\n",
      "Epoch 842: train loss 4.96575927734375 val loss 4.965353012084961\n",
      "Epoch 843: train loss 4.965353012084961 val loss 4.965943336486816\n",
      "Epoch 844: train loss 4.965943336486816 val loss 4.965609073638916\n",
      "Epoch 845: train loss 4.965609073638916 val loss 4.965808868408203\n",
      "Epoch 846: train loss 4.965808868408203 val loss 4.965813636779785\n",
      "Epoch 847: train loss 4.965813636779785 val loss 4.96623420715332\n",
      "Epoch 848: train loss 4.96623420715332 val loss 4.965539455413818\n",
      "Epoch 849: train loss 4.965539455413818 val loss 4.966121196746826\n",
      "Epoch 850: train loss 4.966121196746826 val loss 4.965470314025879\n",
      "Epoch 851: train loss 4.965470314025879 val loss 4.966027736663818\n",
      "Epoch 852: train loss 4.966027736663818 val loss 4.965579032897949\n",
      "Epoch 853: train loss 4.965579032897949 val loss 4.966329097747803\n",
      "Epoch 854: train loss 4.966329097747803 val loss 4.965402126312256\n",
      "Epoch 855: train loss 4.965402126312256 val loss 4.965838432312012\n",
      "Epoch 856: train loss 4.965838432312012 val loss 4.965631008148193\n",
      "Epoch 857: train loss 4.965631008148193 val loss 4.965850353240967\n",
      "Epoch 858: train loss 4.965850353240967 val loss 4.965679168701172\n",
      "Epoch 859: train loss 4.965679168701172 val loss 4.966187477111816\n",
      "Epoch 860: train loss 4.966187477111816 val loss 4.965586185455322\n",
      "Epoch 861: train loss 4.965586185455322 val loss 4.966302394866943\n",
      "Epoch 862: train loss 4.966302394866943 val loss 4.965634346008301\n",
      "Epoch 863: train loss 4.965634346008301 val loss 4.966397285461426\n",
      "Epoch 864: train loss 4.966397285461426 val loss 4.966128349304199\n",
      "Epoch 865: train loss 4.966128349304199 val loss 4.965595245361328\n",
      "Epoch 866: train loss 4.965595245361328 val loss 4.966848850250244\n",
      "Epoch 867: train loss 4.966848850250244 val loss 4.965644836425781\n",
      "Epoch 868: train loss 4.965644836425781 val loss 4.9660964012146\n",
      "Epoch 869: train loss 4.9660964012146 val loss 4.965698719024658\n",
      "Epoch 870: train loss 4.965698719024658 val loss 4.965743541717529\n",
      "Epoch 871: train loss 4.965743541717529 val loss 4.966082572937012\n",
      "Epoch 872: train loss 4.966082572937012 val loss 4.965685844421387\n",
      "Epoch 873: train loss 4.965685844421387 val loss 4.9659624099731445\n",
      "Epoch 874: train loss 4.9659624099731445 val loss 4.965341091156006\n",
      "Epoch 875: train loss 4.965341091156006 val loss 4.966432571411133\n",
      "Epoch 876: train loss 4.966432571411133 val loss 4.965459823608398\n",
      "Epoch 877: train loss 4.965459823608398 val loss 4.965996742248535\n",
      "Epoch 878: train loss 4.965996742248535 val loss 4.965405464172363\n",
      "Epoch 879: train loss 4.965405464172363 val loss 4.966450214385986\n",
      "Epoch 880: train loss 4.966450214385986 val loss 4.965380668640137\n",
      "Epoch 881: train loss 4.965380668640137 val loss 4.9675116539001465\n",
      "Epoch 882: train loss 4.9675116539001465 val loss 4.967761039733887\n",
      "Epoch 883: train loss 4.967761039733887 val loss 4.965611457824707\n",
      "Epoch 884: train loss 4.965611457824707 val loss 4.967107772827148\n",
      "Epoch 885: train loss 4.967107772827148 val loss 4.965575695037842\n",
      "Epoch 886: train loss 4.965575695037842 val loss 4.965912342071533\n",
      "Epoch 887: train loss 4.965912342071533 val loss 4.96567964553833\n",
      "Epoch 888: train loss 4.96567964553833 val loss 4.965516567230225\n",
      "Epoch 889: train loss 4.965516567230225 val loss 4.965386390686035\n",
      "Epoch 890: train loss 4.965386390686035 val loss 4.965906143188477\n",
      "Epoch 891: train loss 4.965906143188477 val loss 4.965866565704346\n",
      "Epoch 892: train loss 4.965866565704346 val loss 4.96607780456543\n",
      "Epoch 893: train loss 4.96607780456543 val loss 4.965641975402832\n",
      "Epoch 894: train loss 4.965641975402832 val loss 4.965540885925293\n",
      "Epoch 895: train loss 4.965540885925293 val loss 4.965584754943848\n",
      "Epoch 896: train loss 4.965584754943848 val loss 4.9654951095581055\n",
      "Epoch 897: train loss 4.9654951095581055 val loss 4.965730667114258\n",
      "Epoch 898: train loss 4.965730667114258 val loss 4.965611457824707\n",
      "Epoch 899: train loss 4.965611457824707 val loss 4.965556621551514\n",
      "Epoch 900: train loss 4.965556621551514 val loss 4.965892791748047\n",
      "Epoch 901: train loss 4.965892791748047 val loss 4.965490341186523\n",
      "Epoch 902: train loss 4.965490341186523 val loss 4.965564250946045\n",
      "Epoch 903: train loss 4.965564250946045 val loss 4.965380668640137\n",
      "Epoch 904: train loss 4.965380668640137 val loss 4.965389251708984\n",
      "Epoch 905: train loss 4.965389251708984 val loss 4.965336322784424\n",
      "Epoch 906: train loss 4.965336322784424 val loss 4.965939044952393\n",
      "Epoch 907: train loss 4.965939044952393 val loss 4.965381145477295\n",
      "Epoch 908: train loss 4.965381145477295 val loss 4.965913772583008\n",
      "Epoch 909: train loss 4.965913772583008 val loss 4.965201377868652\n",
      "Epoch 910: train loss 4.965201377868652 val loss 4.965766906738281\n",
      "Epoch 911: train loss 4.965766906738281 val loss 4.965596675872803\n",
      "Epoch 912: train loss 4.965596675872803 val loss 4.965429306030273\n",
      "Epoch 913: train loss 4.965429306030273 val loss 4.96557092666626\n",
      "Epoch 914: train loss 4.96557092666626 val loss 4.965661525726318\n",
      "Epoch 915: train loss 4.965661525726318 val loss 4.965790748596191\n",
      "Epoch 916: train loss 4.965790748596191 val loss 4.965846061706543\n",
      "Epoch 917: train loss 4.965846061706543 val loss 4.965686798095703\n",
      "Epoch 918: train loss 4.965686798095703 val loss 4.96531867980957\n",
      "Epoch 919: train loss 4.96531867980957 val loss 4.96534538269043\n",
      "Epoch 920: train loss 4.96534538269043 val loss 4.965702056884766\n",
      "Epoch 921: train loss 4.965702056884766 val loss 4.965571403503418\n",
      "Epoch 922: train loss 4.965571403503418 val loss 4.966320991516113\n",
      "Epoch 923: train loss 4.966320991516113 val loss 4.965249061584473\n",
      "Epoch 924: train loss 4.965249061584473 val loss 4.967243194580078\n",
      "Epoch 925: train loss 4.967243194580078 val loss 4.966207027435303\n",
      "Epoch 926: train loss 4.966207027435303 val loss 4.9668073654174805\n",
      "Epoch 927: train loss 4.9668073654174805 val loss 4.968238353729248\n",
      "Epoch 928: train loss 4.968238353729248 val loss 4.966488838195801\n",
      "Epoch 929: train loss 4.966488838195801 val loss 4.965866565704346\n",
      "Epoch 930: train loss 4.965866565704346 val loss 4.9662370681762695\n",
      "Epoch 931: train loss 4.9662370681762695 val loss 4.966026782989502\n",
      "Epoch 932: train loss 4.966026782989502 val loss 4.967339515686035\n",
      "Epoch 933: train loss 4.967339515686035 val loss 4.966041088104248\n",
      "Epoch 934: train loss 4.966041088104248 val loss 4.966437816619873\n",
      "Epoch 935: train loss 4.966437816619873 val loss 4.965664386749268\n",
      "Epoch 936: train loss 4.965664386749268 val loss 4.966019153594971\n",
      "Epoch 937: train loss 4.966019153594971 val loss 4.965658664703369\n",
      "Epoch 938: train loss 4.965658664703369 val loss 4.96572208404541\n",
      "Epoch 939: train loss 4.96572208404541 val loss 4.965816974639893\n",
      "Epoch 940: train loss 4.965816974639893 val loss 4.965677738189697\n",
      "Epoch 941: train loss 4.965677738189697 val loss 4.965517520904541\n",
      "Epoch 942: train loss 4.965517520904541 val loss 4.965514183044434\n",
      "Epoch 943: train loss 4.965514183044434 val loss 4.965587139129639\n",
      "Epoch 944: train loss 4.965587139129639 val loss 4.965458869934082\n",
      "Epoch 945: train loss 4.965458869934082 val loss 4.96539831161499\n",
      "Epoch 946: train loss 4.96539831161499 val loss 4.965377330780029\n",
      "Epoch 947: train loss 4.965377330780029 val loss 4.965419769287109\n",
      "Epoch 948: train loss 4.965419769287109 val loss 4.965414524078369\n",
      "Epoch 949: train loss 4.965414524078369 val loss 4.965324401855469\n",
      "Epoch 950: train loss 4.965324401855469 val loss 4.965649127960205\n",
      "Epoch 951: train loss 4.965649127960205 val loss 4.965266704559326\n",
      "Epoch 952: train loss 4.965266704559326 val loss 4.9656219482421875\n",
      "Epoch 953: train loss 4.9656219482421875 val loss 4.965320587158203\n",
      "Epoch 954: train loss 4.965320587158203 val loss 4.965718746185303\n",
      "Epoch 955: train loss 4.965718746185303 val loss 4.9655866622924805\n",
      "Epoch 956: train loss 4.9655866622924805 val loss 4.96516227722168\n",
      "Epoch 957: train loss 4.96516227722168 val loss 4.965818405151367\n",
      "Epoch 958: train loss 4.965818405151367 val loss 4.96543550491333\n",
      "Epoch 959: train loss 4.96543550491333 val loss 4.9656829833984375\n",
      "Epoch 960: train loss 4.9656829833984375 val loss 4.965472221374512\n",
      "Epoch 961: train loss 4.965472221374512 val loss 4.965908050537109\n",
      "Epoch 962: train loss 4.965908050537109 val loss 4.965414047241211\n",
      "Epoch 963: train loss 4.965414047241211 val loss 4.965697765350342\n",
      "Epoch 964: train loss 4.965697765350342 val loss 4.965476036071777\n",
      "Epoch 965: train loss 4.965476036071777 val loss 4.965503215789795\n",
      "Epoch 966: train loss 4.965503215789795 val loss 4.9654154777526855\n",
      "Epoch 967: train loss 4.9654154777526855 val loss 4.965801239013672\n",
      "Epoch 968: train loss 4.965801239013672 val loss 4.965426445007324\n",
      "Epoch 969: train loss 4.965426445007324 val loss 4.965647220611572\n",
      "Epoch 970: train loss 4.965647220611572 val loss 4.965307235717773\n",
      "Epoch 971: train loss 4.965307235717773 val loss 4.96597957611084\n",
      "Epoch 972: train loss 4.96597957611084 val loss 4.965601921081543\n",
      "Epoch 973: train loss 4.965601921081543 val loss 4.965638160705566\n",
      "Epoch 974: train loss 4.965638160705566 val loss 4.965736389160156\n",
      "Epoch 975: train loss 4.965736389160156 val loss 4.965598106384277\n",
      "Epoch 976: train loss 4.965598106384277 val loss 4.965460777282715\n",
      "Epoch 977: train loss 4.965460777282715 val loss 4.965324401855469\n",
      "Epoch 978: train loss 4.965324401855469 val loss 4.965519905090332\n",
      "Epoch 979: train loss 4.965519905090332 val loss 4.965629577636719\n",
      "Epoch 980: train loss 4.965629577636719 val loss 4.965827941894531\n",
      "Epoch 981: train loss 4.965827941894531 val loss 4.965288162231445\n",
      "Epoch 982: train loss 4.965288162231445 val loss 4.965244293212891\n",
      "Epoch 983: train loss 4.965244293212891 val loss 4.965503692626953\n",
      "Epoch 984: train loss 4.965503692626953 val loss 4.965965270996094\n",
      "Epoch 985: train loss 4.965965270996094 val loss 4.965699195861816\n",
      "Epoch 986: train loss 4.965699195861816 val loss 4.965726852416992\n",
      "Epoch 987: train loss 4.965726852416992 val loss 4.96549654006958\n",
      "Epoch 988: train loss 4.96549654006958 val loss 4.966239929199219\n",
      "Epoch 989: train loss 4.966239929199219 val loss 4.965850830078125\n",
      "Epoch 990: train loss 4.965850830078125 val loss 4.965747356414795\n",
      "Epoch 991: train loss 4.965747356414795 val loss 4.966161727905273\n",
      "Epoch 992: train loss 4.966161727905273 val loss 4.965504169464111\n",
      "Epoch 993: train loss 4.965504169464111 val loss 4.965690612792969\n",
      "Epoch 994: train loss 4.965690612792969 val loss 4.96560001373291\n",
      "Epoch 995: train loss 4.96560001373291 val loss 4.965681552886963\n",
      "Epoch 996: train loss 4.965681552886963 val loss 4.966042518615723\n",
      "Epoch 997: train loss 4.966042518615723 val loss 4.965681552886963\n",
      "Epoch 998: train loss 4.965681552886963 val loss 4.965803146362305\n",
      "Epoch 999: train loss 4.965803146362305 val loss 4.9654107093811035\n",
      "Epoch 1000: train loss 4.9654107093811035 val loss 4.965701103210449\n",
      "Epoch 1001: train loss 4.965701103210449 val loss 4.96560001373291\n",
      "Epoch 1002: train loss 4.96560001373291 val loss 4.9652605056762695\n",
      "Epoch 1003: train loss 4.9652605056762695 val loss 4.966116428375244\n",
      "Epoch 1004: train loss 4.966116428375244 val loss 4.965425491333008\n",
      "Epoch 1005: train loss 4.965425491333008 val loss 4.965616703033447\n",
      "Epoch 1006: train loss 4.965616703033447 val loss 4.965258598327637\n",
      "Epoch 1007: train loss 4.965258598327637 val loss 4.965542793273926\n",
      "Epoch 1008: train loss 4.965542793273926 val loss 4.965342044830322\n",
      "Epoch 1009: train loss 4.965342044830322 val loss 4.965633869171143\n",
      "Epoch 1010: train loss 4.965633869171143 val loss 4.965619087219238\n",
      "Epoch 1011: train loss 4.965619087219238 val loss 4.9654541015625\n",
      "Epoch 1012: train loss 4.9654541015625 val loss 4.965837478637695\n",
      "Epoch 1013: train loss 4.965837478637695 val loss 4.9653239250183105\n",
      "Epoch 1014: train loss 4.9653239250183105 val loss 4.965437412261963\n",
      "Epoch 1015: train loss 4.965437412261963 val loss 4.965385437011719\n",
      "Epoch 1016: train loss 4.965385437011719 val loss 4.965323448181152\n",
      "Epoch 1017: train loss 4.965323448181152 val loss 4.965458869934082\n",
      "Epoch 1018: train loss 4.965458869934082 val loss 4.965355396270752\n",
      "Epoch 1019: train loss 4.965355396270752 val loss 4.965145111083984\n",
      "Epoch 1020: train loss 4.965145111083984 val loss 4.965314865112305\n",
      "Epoch 1021: train loss 4.965314865112305 val loss 4.965625762939453\n",
      "Epoch 1022: train loss 4.965625762939453 val loss 4.965313911437988\n",
      "Epoch 1023: train loss 4.965313911437988 val loss 4.965373516082764\n",
      "Epoch 1024: train loss 4.965373516082764 val loss 4.965666770935059\n",
      "Epoch 1025: train loss 4.965666770935059 val loss 4.965241432189941\n",
      "Epoch 1026: train loss 4.965241432189941 val loss 4.965217590332031\n",
      "Epoch 1027: train loss 4.965217590332031 val loss 4.965319633483887\n",
      "Epoch 1028: train loss 4.965319633483887 val loss 4.9659318923950195\n",
      "Epoch 1029: train loss 4.9659318923950195 val loss 4.965383529663086\n",
      "Epoch 1030: train loss 4.965383529663086 val loss 4.966391563415527\n",
      "Epoch 1031: train loss 4.966391563415527 val loss 4.965389728546143\n",
      "Epoch 1032: train loss 4.965389728546143 val loss 4.9658403396606445\n",
      "Epoch 1033: train loss 4.9658403396606445 val loss 4.965360164642334\n",
      "Epoch 1034: train loss 4.965360164642334 val loss 4.965697288513184\n",
      "Epoch 1035: train loss 4.965697288513184 val loss 4.965387344360352\n",
      "Epoch 1036: train loss 4.965387344360352 val loss 4.965519905090332\n",
      "Epoch 1037: train loss 4.965519905090332 val loss 4.965330123901367\n",
      "Epoch 1038: train loss 4.965330123901367 val loss 4.965660095214844\n",
      "Epoch 1039: train loss 4.965660095214844 val loss 4.965322971343994\n",
      "Epoch 1040: train loss 4.965322971343994 val loss 4.965860843658447\n",
      "Epoch 1041: train loss 4.965860843658447 val loss 4.9652323722839355\n",
      "Epoch 1042: train loss 4.9652323722839355 val loss 4.9664530754089355\n",
      "Epoch 1043: train loss 4.9664530754089355 val loss 4.965576648712158\n",
      "Epoch 1044: train loss 4.965576648712158 val loss 4.965937614440918\n",
      "Epoch 1045: train loss 4.965937614440918 val loss 4.965640544891357\n",
      "Epoch 1046: train loss 4.965640544891357 val loss 4.9652814865112305\n",
      "Epoch 1047: train loss 4.9652814865112305 val loss 4.965969085693359\n",
      "Epoch 1048: train loss 4.965969085693359 val loss 4.965612888336182\n",
      "Epoch 1049: train loss 4.965612888336182 val loss 4.9658074378967285\n",
      "Epoch 1050: train loss 4.9658074378967285 val loss 4.96647310256958\n",
      "Epoch 1051: train loss 4.96647310256958 val loss 4.96567440032959\n",
      "Epoch 1052: train loss 4.96567440032959 val loss 4.966580867767334\n",
      "Epoch 1053: train loss 4.966580867767334 val loss 4.965698719024658\n",
      "Epoch 1054: train loss 4.965698719024658 val loss 4.965926170349121\n",
      "Epoch 1055: train loss 4.965926170349121 val loss 4.965470314025879\n",
      "Epoch 1056: train loss 4.965470314025879 val loss 4.965320587158203\n",
      "Epoch 1057: train loss 4.965320587158203 val loss 4.965274810791016\n",
      "Epoch 1058: train loss 4.965274810791016 val loss 4.965468883514404\n",
      "Epoch 1059: train loss 4.965468883514404 val loss 4.965487003326416\n",
      "Epoch 1060: train loss 4.965487003326416 val loss 4.965563774108887\n",
      "Epoch 1061: train loss 4.965563774108887 val loss 4.965616703033447\n",
      "Epoch 1062: train loss 4.965616703033447 val loss 4.965575695037842\n",
      "Epoch 1063: train loss 4.965575695037842 val loss 4.965378761291504\n",
      "Epoch 1064: train loss 4.965378761291504 val loss 4.965132713317871\n",
      "Epoch 1065: train loss 4.965132713317871 val loss 4.965554237365723\n",
      "Epoch 1066: train loss 4.965554237365723 val loss 4.965425968170166\n",
      "Epoch 1067: train loss 4.965425968170166 val loss 4.965531826019287\n",
      "Epoch 1068: train loss 4.965531826019287 val loss 4.965241432189941\n",
      "Epoch 1069: train loss 4.965241432189941 val loss 4.965460777282715\n",
      "Epoch 1070: train loss 4.965460777282715 val loss 4.965480804443359\n",
      "Epoch 1071: train loss 4.965480804443359 val loss 4.965689182281494\n",
      "Epoch 1072: train loss 4.965689182281494 val loss 4.965648651123047\n",
      "Epoch 1073: train loss 4.965648651123047 val loss 4.965384483337402\n",
      "Epoch 1074: train loss 4.965384483337402 val loss 4.965472221374512\n",
      "Epoch 1075: train loss 4.965472221374512 val loss 4.9654130935668945\n",
      "Epoch 1076: train loss 4.9654130935668945 val loss 4.965993881225586\n",
      "Epoch 1077: train loss 4.965993881225586 val loss 4.965380668640137\n",
      "Epoch 1078: train loss 4.965380668640137 val loss 4.965456485748291\n",
      "Epoch 1079: train loss 4.965456485748291 val loss 4.9654998779296875\n",
      "Epoch 1080: train loss 4.9654998779296875 val loss 4.965697288513184\n",
      "Epoch 1081: train loss 4.965697288513184 val loss 4.9656219482421875\n",
      "Epoch 1082: train loss 4.9656219482421875 val loss 4.965620994567871\n",
      "Epoch 1083: train loss 4.965620994567871 val loss 4.965847969055176\n",
      "Epoch 1084: train loss 4.965847969055176 val loss 4.965546607971191\n",
      "Epoch 1085: train loss 4.965546607971191 val loss 4.965639114379883\n",
      "Epoch 1086: train loss 4.965639114379883 val loss 4.965235233306885\n",
      "Epoch 1087: train loss 4.965235233306885 val loss 4.965390205383301\n",
      "Epoch 1088: train loss 4.965390205383301 val loss 4.96552038192749\n",
      "Epoch 1089: train loss 4.96552038192749 val loss 4.96550989151001\n",
      "Epoch 1090: train loss 4.96550989151001 val loss 4.965373992919922\n",
      "Epoch 1091: train loss 4.965373992919922 val loss 4.965337753295898\n",
      "Epoch 1092: train loss 4.965337753295898 val loss 4.965409278869629\n",
      "Epoch 1093: train loss 4.965409278869629 val loss 4.965476989746094\n",
      "Epoch 1094: train loss 4.965476989746094 val loss 4.965333461761475\n",
      "Epoch 1095: train loss 4.965333461761475 val loss 4.965279579162598\n",
      "Epoch 1096: train loss 4.965279579162598 val loss 4.965226173400879\n",
      "Epoch 1097: train loss 4.965226173400879 val loss 4.965373992919922\n",
      "Epoch 1098: train loss 4.965373992919922 val loss 4.965418815612793\n",
      "Epoch 1099: train loss 4.965418815612793 val loss 4.9651923179626465\n",
      "Epoch 1100: train loss 4.9651923179626465 val loss 4.965271949768066\n",
      "Epoch 1101: train loss 4.965271949768066 val loss 4.9655303955078125\n",
      "Epoch 1102: train loss 4.9655303955078125 val loss 4.9652581214904785\n",
      "Epoch 1103: train loss 4.9652581214904785 val loss 4.96516752243042\n",
      "Epoch 1104: train loss 4.96516752243042 val loss 4.965176105499268\n",
      "Epoch 1105: train loss 4.965176105499268 val loss 4.965164661407471\n",
      "Epoch 1106: train loss 4.965164661407471 val loss 4.965691566467285\n",
      "Epoch 1107: train loss 4.965691566467285 val loss 4.965516090393066\n",
      "Epoch 1108: train loss 4.965516090393066 val loss 4.965184211730957\n",
      "Epoch 1109: train loss 4.965184211730957 val loss 4.965561866760254\n",
      "Epoch 1110: train loss 4.965561866760254 val loss 4.96516752243042\n",
      "Epoch 1111: train loss 4.96516752243042 val loss 4.965407848358154\n",
      "Epoch 1112: train loss 4.965407848358154 val loss 4.965364456176758\n",
      "Epoch 1113: train loss 4.965364456176758 val loss 4.9654669761657715\n",
      "Epoch 1114: train loss 4.9654669761657715 val loss 4.965449810028076\n",
      "Epoch 1115: train loss 4.965449810028076 val loss 4.965314865112305\n",
      "Epoch 1116: train loss 4.965314865112305 val loss 4.9654765129089355\n",
      "Epoch 1117: train loss 4.9654765129089355 val loss 4.965242385864258\n",
      "Epoch 1118: train loss 4.965242385864258 val loss 4.965871810913086\n",
      "Epoch 1119: train loss 4.965871810913086 val loss 4.965200424194336\n",
      "Epoch 1120: train loss 4.965200424194336 val loss 4.966412544250488\n",
      "Epoch 1121: train loss 4.966412544250488 val loss 4.965223789215088\n",
      "Epoch 1122: train loss 4.965223789215088 val loss 4.965463638305664\n",
      "Epoch 1123: train loss 4.965463638305664 val loss 4.965275764465332\n",
      "Epoch 1124: train loss 4.965275764465332 val loss 4.965863227844238\n",
      "Epoch 1125: train loss 4.965863227844238 val loss 4.965346336364746\n",
      "Epoch 1126: train loss 4.965346336364746 val loss 4.966227054595947\n",
      "Epoch 1127: train loss 4.966227054595947 val loss 4.965183258056641\n",
      "Epoch 1128: train loss 4.965183258056641 val loss 4.965558052062988\n",
      "Epoch 1129: train loss 4.965558052062988 val loss 4.965476036071777\n",
      "Epoch 1130: train loss 4.965476036071777 val loss 4.965195655822754\n",
      "Epoch 1131: train loss 4.965195655822754 val loss 4.966000556945801\n",
      "Epoch 1132: train loss 4.966000556945801 val loss 4.965178489685059\n",
      "Epoch 1133: train loss 4.965178489685059 val loss 4.965094566345215\n",
      "Epoch 1134: train loss 4.965094566345215 val loss 4.965411186218262\n",
      "Epoch 1135: train loss 4.965411186218262 val loss 4.965663909912109\n",
      "Epoch 1136: train loss 4.965663909912109 val loss 4.965819358825684\n",
      "Epoch 1137: train loss 4.965819358825684 val loss 4.966156005859375\n",
      "Epoch 1138: train loss 4.966156005859375 val loss 4.96539306640625\n",
      "Epoch 1139: train loss 4.96539306640625 val loss 4.966536521911621\n",
      "Epoch 1140: train loss 4.966536521911621 val loss 4.965300559997559\n",
      "Epoch 1141: train loss 4.965300559997559 val loss 4.966021537780762\n",
      "Epoch 1142: train loss 4.966021537780762 val loss 4.965392589569092\n",
      "Epoch 1143: train loss 4.965392589569092 val loss 4.965336322784424\n",
      "Epoch 1144: train loss 4.965336322784424 val loss 4.965417861938477\n",
      "Epoch 1145: train loss 4.965417861938477 val loss 4.965518951416016\n",
      "Epoch 1146: train loss 4.965518951416016 val loss 4.965400218963623\n",
      "Epoch 1147: train loss 4.965400218963623 val loss 4.965424537658691\n",
      "Epoch 1148: train loss 4.965424537658691 val loss 4.965397834777832\n",
      "Epoch 1149: train loss 4.965397834777832 val loss 4.965236663818359\n",
      "Epoch 1150: train loss 4.965236663818359 val loss 4.966279029846191\n",
      "Epoch 1151: train loss 4.966279029846191 val loss 4.9661383628845215\n",
      "Epoch 1152: train loss 4.9661383628845215 val loss 4.9659552574157715\n",
      "Epoch 1153: train loss 4.9659552574157715 val loss 4.966338157653809\n",
      "Epoch 1154: train loss 4.966338157653809 val loss 4.965631008148193\n",
      "Epoch 1155: train loss 4.965631008148193 val loss 4.965991020202637\n",
      "Epoch 1156: train loss 4.965991020202637 val loss 4.965574741363525\n",
      "Epoch 1157: train loss 4.965574741363525 val loss 4.96707820892334\n",
      "Epoch 1158: train loss 4.96707820892334 val loss 4.965725898742676\n",
      "Epoch 1159: train loss 4.965725898742676 val loss 4.966438293457031\n",
      "Epoch 1160: train loss 4.966438293457031 val loss 4.966114044189453\n",
      "Epoch 1161: train loss 4.966114044189453 val loss 4.965556621551514\n",
      "Epoch 1162: train loss 4.965556621551514 val loss 4.965706825256348\n",
      "Epoch 1163: train loss 4.965706825256348 val loss 4.965446472167969\n",
      "Epoch 1164: train loss 4.965446472167969 val loss 4.965474605560303\n",
      "Epoch 1165: train loss 4.965474605560303 val loss 4.9655537605285645\n",
      "Epoch 1166: train loss 4.9655537605285645 val loss 4.9654860496521\n",
      "Epoch 1167: train loss 4.9654860496521 val loss 4.965446472167969\n",
      "Epoch 1168: train loss 4.965446472167969 val loss 4.965738296508789\n",
      "Epoch 1169: train loss 4.965738296508789 val loss 4.965554237365723\n",
      "Epoch 1170: train loss 4.965554237365723 val loss 4.965579986572266\n",
      "Epoch 1171: train loss 4.965579986572266 val loss 4.965417861938477\n",
      "Epoch 1172: train loss 4.965417861938477 val loss 4.9652323722839355\n",
      "Epoch 1173: train loss 4.9652323722839355 val loss 4.965219974517822\n",
      "Epoch 1174: train loss 4.965219974517822 val loss 4.965379238128662\n",
      "Epoch 1175: train loss 4.965379238128662 val loss 4.966107368469238\n",
      "Epoch 1176: train loss 4.966107368469238 val loss 4.96534538269043\n",
      "Epoch 1177: train loss 4.96534538269043 val loss 4.965799331665039\n",
      "Epoch 1178: train loss 4.965799331665039 val loss 4.965435981750488\n",
      "Epoch 1179: train loss 4.965435981750488 val loss 4.965781211853027\n",
      "Epoch 1180: train loss 4.965781211853027 val loss 4.965596675872803\n",
      "Epoch 1181: train loss 4.965596675872803 val loss 4.965887546539307\n",
      "Epoch 1182: train loss 4.965887546539307 val loss 4.965435028076172\n",
      "Epoch 1183: train loss 4.965435028076172 val loss 4.965885639190674\n",
      "Epoch 1184: train loss 4.965885639190674 val loss 4.965297698974609\n",
      "Epoch 1185: train loss 4.965297698974609 val loss 4.965809345245361\n",
      "Epoch 1186: train loss 4.965809345245361 val loss 4.965394020080566\n",
      "Epoch 1187: train loss 4.965394020080566 val loss 4.965569019317627\n",
      "Epoch 1188: train loss 4.965569019317627 val loss 4.965568542480469\n",
      "Epoch 1189: train loss 4.965568542480469 val loss 4.965486526489258\n",
      "Epoch 1190: train loss 4.965486526489258 val loss 4.965424537658691\n",
      "Epoch 1191: train loss 4.965424537658691 val loss 4.965422630310059\n",
      "Epoch 1192: train loss 4.965422630310059 val loss 4.965279579162598\n",
      "Epoch 1193: train loss 4.965279579162598 val loss 4.965475082397461\n",
      "Epoch 1194: train loss 4.965475082397461 val loss 4.965583324432373\n",
      "Epoch 1195: train loss 4.965583324432373 val loss 4.965998649597168\n",
      "Epoch 1196: train loss 4.965998649597168 val loss 4.965985298156738\n",
      "Epoch 1197: train loss 4.965985298156738 val loss 4.965604782104492\n",
      "Epoch 1198: train loss 4.965604782104492 val loss 4.965527057647705\n",
      "Epoch 1199: train loss 4.965527057647705 val loss 4.966014862060547\n",
      "Epoch 1200: train loss 4.966014862060547 val loss 4.965773582458496\n",
      "Epoch 1201: train loss 4.965773582458496 val loss 4.965607166290283\n",
      "Epoch 1202: train loss 4.965607166290283 val loss 4.965953826904297\n",
      "Epoch 1203: train loss 4.965953826904297 val loss 4.9658379554748535\n",
      "Epoch 1204: train loss 4.9658379554748535 val loss 4.966678619384766\n",
      "Epoch 1205: train loss 4.966678619384766 val loss 4.965586185455322\n",
      "Epoch 1206: train loss 4.965586185455322 val loss 4.965678691864014\n",
      "Epoch 1207: train loss 4.965678691864014 val loss 4.965451717376709\n",
      "Epoch 1208: train loss 4.965451717376709 val loss 4.965481281280518\n",
      "Epoch 1209: train loss 4.965481281280518 val loss 4.965804576873779\n",
      "Epoch 1210: train loss 4.965804576873779 val loss 4.9658122062683105\n",
      "Epoch 1211: train loss 4.9658122062683105 val loss 4.965656280517578\n",
      "Epoch 1212: train loss 4.965656280517578 val loss 4.965641975402832\n",
      "Epoch 1213: train loss 4.965641975402832 val loss 4.965447902679443\n",
      "Epoch 1214: train loss 4.965447902679443 val loss 4.965425968170166\n",
      "Epoch 1215: train loss 4.965425968170166 val loss 4.965915203094482\n",
      "Epoch 1216: train loss 4.965915203094482 val loss 4.965818881988525\n",
      "Epoch 1217: train loss 4.965818881988525 val loss 4.966256141662598\n",
      "Epoch 1218: train loss 4.966256141662598 val loss 4.965831279754639\n",
      "Epoch 1219: train loss 4.965831279754639 val loss 4.965151309967041\n",
      "Epoch 1220: train loss 4.965151309967041 val loss 4.966525554656982\n",
      "Epoch 1221: train loss 4.966525554656982 val loss 4.965706825256348\n",
      "Epoch 1222: train loss 4.965706825256348 val loss 4.967144966125488\n",
      "Epoch 1223: train loss 4.967144966125488 val loss 4.966978549957275\n",
      "Epoch 1224: train loss 4.966978549957275 val loss 4.965557098388672\n",
      "Epoch 1225: train loss 4.965557098388672 val loss 4.966123580932617\n",
      "Epoch 1226: train loss 4.966123580932617 val loss 4.96536922454834\n",
      "Epoch 1227: train loss 4.96536922454834 val loss 4.965763092041016\n",
      "Epoch 1228: train loss 4.965763092041016 val loss 4.965627193450928\n",
      "Epoch 1229: train loss 4.965627193450928 val loss 4.965519905090332\n",
      "Epoch 1230: train loss 4.965519905090332 val loss 4.96541166305542\n",
      "Epoch 1231: train loss 4.96541166305542 val loss 4.965104103088379\n",
      "Epoch 1232: train loss 4.965104103088379 val loss 4.965200901031494\n",
      "Epoch 1233: train loss 4.965200901031494 val loss 4.965544700622559\n",
      "Epoch 1234: train loss 4.965544700622559 val loss 4.965587615966797\n",
      "Epoch 1235: train loss 4.965587615966797 val loss 4.965244293212891\n",
      "Epoch 1236: train loss 4.965244293212891 val loss 4.9658050537109375\n",
      "Epoch 1237: train loss 4.9658050537109375 val loss 4.965151786804199\n",
      "Epoch 1238: train loss 4.965151786804199 val loss 4.965465545654297\n",
      "Epoch 1239: train loss 4.965465545654297 val loss 4.965098857879639\n",
      "Epoch 1240: train loss 4.965098857879639 val loss 4.965147495269775\n",
      "Epoch 1241: train loss 4.965147495269775 val loss 4.965365409851074\n",
      "Epoch 1242: train loss 4.965365409851074 val loss 4.96510124206543\n",
      "Epoch 1243: train loss 4.96510124206543 val loss 4.965508937835693\n",
      "Epoch 1244: train loss 4.965508937835693 val loss 4.9654717445373535\n",
      "Epoch 1245: train loss 4.9654717445373535 val loss 4.965434551239014\n",
      "Epoch 1246: train loss 4.965434551239014 val loss 4.965343952178955\n",
      "Epoch 1247: train loss 4.965343952178955 val loss 4.966169834136963\n",
      "Epoch 1248: train loss 4.966169834136963 val loss 4.965447425842285\n",
      "Epoch 1249: train loss 4.965447425842285 val loss 4.965801239013672\n",
      "Epoch 1250: train loss 4.965801239013672 val loss 4.965363502502441\n",
      "Epoch 1251: train loss 4.965363502502441 val loss 4.96640682220459\n",
      "Epoch 1252: train loss 4.96640682220459 val loss 4.965463161468506\n",
      "Epoch 1253: train loss 4.965463161468506 val loss 4.966147422790527\n",
      "Epoch 1254: train loss 4.966147422790527 val loss 4.96538782119751\n",
      "Epoch 1255: train loss 4.96538782119751 val loss 4.965417861938477\n",
      "Epoch 1256: train loss 4.965417861938477 val loss 4.96519660949707\n",
      "Epoch 1257: train loss 4.96519660949707 val loss 4.965153217315674\n",
      "Epoch 1258: train loss 4.965153217315674 val loss 4.965271949768066\n",
      "Epoch 1259: train loss 4.965271949768066 val loss 4.965268135070801\n",
      "Epoch 1260: train loss 4.965268135070801 val loss 4.965122222900391\n",
      "Epoch 1261: train loss 4.965122222900391 val loss 4.965322494506836\n",
      "Epoch 1262: train loss 4.965322494506836 val loss 4.9651689529418945\n",
      "Epoch 1263: train loss 4.9651689529418945 val loss 4.965267658233643\n",
      "Epoch 1264: train loss 4.965267658233643 val loss 4.9652419090271\n",
      "Epoch 1265: train loss 4.9652419090271 val loss 4.965367317199707\n",
      "Epoch 1266: train loss 4.965367317199707 val loss 4.965671539306641\n",
      "Epoch 1267: train loss 4.965671539306641 val loss 4.965218544006348\n",
      "Epoch 1268: train loss 4.965218544006348 val loss 4.96500301361084\n",
      "Epoch 1269: train loss 4.96500301361084 val loss 4.965392589569092\n",
      "Epoch 1270: train loss 4.965392589569092 val loss 4.965389251708984\n",
      "Epoch 1271: train loss 4.965389251708984 val loss 4.9654388427734375\n",
      "Epoch 1272: train loss 4.9654388427734375 val loss 4.965408802032471\n",
      "Epoch 1273: train loss 4.965408802032471 val loss 4.965561866760254\n",
      "Epoch 1274: train loss 4.965561866760254 val loss 4.965301990509033\n",
      "Epoch 1275: train loss 4.965301990509033 val loss 4.965584754943848\n",
      "Epoch 1276: train loss 4.965584754943848 val loss 4.965231895446777\n",
      "Epoch 1277: train loss 4.965231895446777 val loss 4.966500282287598\n",
      "Epoch 1278: train loss 4.966500282287598 val loss 4.965356349945068\n",
      "Epoch 1279: train loss 4.965356349945068 val loss 4.9671478271484375\n",
      "Epoch 1280: train loss 4.9671478271484375 val loss 4.965425968170166\n",
      "Epoch 1281: train loss 4.965425968170166 val loss 4.966601371765137\n",
      "Epoch 1282: train loss 4.966601371765137 val loss 4.965636730194092\n",
      "Epoch 1283: train loss 4.965636730194092 val loss 4.965285301208496\n",
      "Epoch 1284: train loss 4.965285301208496 val loss 4.966118812561035\n",
      "Epoch 1285: train loss 4.966118812561035 val loss 4.965185165405273\n",
      "Epoch 1286: train loss 4.965185165405273 val loss 4.96605110168457\n",
      "Epoch 1287: train loss 4.96605110168457 val loss 4.9652180671691895\n",
      "Epoch 1288: train loss 4.9652180671691895 val loss 4.966543197631836\n",
      "Epoch 1289: train loss 4.966543197631836 val loss 4.965962886810303\n",
      "Epoch 1290: train loss 4.965962886810303 val loss 4.965928077697754\n",
      "Epoch 1291: train loss 4.965928077697754 val loss 4.965610980987549\n",
      "Epoch 1292: train loss 4.965610980987549 val loss 4.9659271240234375\n",
      "Epoch 1293: train loss 4.9659271240234375 val loss 4.9656219482421875\n",
      "Epoch 1294: train loss 4.9656219482421875 val loss 4.9662346839904785\n",
      "Epoch 1295: train loss 4.9662346839904785 val loss 4.965378761291504\n",
      "Epoch 1296: train loss 4.965378761291504 val loss 4.966333389282227\n",
      "Epoch 1297: train loss 4.966333389282227 val loss 4.966126441955566\n",
      "Epoch 1298: train loss 4.966126441955566 val loss 4.965505599975586\n",
      "Epoch 1299: train loss 4.965505599975586 val loss 4.967653751373291\n",
      "Epoch 1300: train loss 4.967653751373291 val loss 4.9655375480651855\n",
      "Epoch 1301: train loss 4.9655375480651855 val loss 4.965866565704346\n",
      "Epoch 1302: train loss 4.965866565704346 val loss 4.965843200683594\n",
      "Epoch 1303: train loss 4.965843200683594 val loss 4.965471267700195\n",
      "Epoch 1304: train loss 4.965471267700195 val loss 4.965653419494629\n",
      "Epoch 1305: train loss 4.965653419494629 val loss 4.965322494506836\n",
      "Epoch 1306: train loss 4.965322494506836 val loss 4.965678691864014\n",
      "Epoch 1307: train loss 4.965678691864014 val loss 4.965640068054199\n",
      "Epoch 1308: train loss 4.965640068054199 val loss 4.96536922454834\n",
      "Epoch 1309: train loss 4.96536922454834 val loss 4.965062141418457\n",
      "Epoch 1310: train loss 4.965062141418457 val loss 4.965088844299316\n",
      "Epoch 1311: train loss 4.965088844299316 val loss 4.965399742126465\n",
      "Epoch 1312: train loss 4.965399742126465 val loss 4.9651665687561035\n",
      "Epoch 1313: train loss 4.9651665687561035 val loss 4.9655585289001465\n",
      "Epoch 1314: train loss 4.9655585289001465 val loss 4.965275764465332\n",
      "Epoch 1315: train loss 4.965275764465332 val loss 4.9658002853393555\n",
      "Epoch 1316: train loss 4.9658002853393555 val loss 4.9650774002075195\n",
      "Epoch 1317: train loss 4.9650774002075195 val loss 4.966728210449219\n",
      "Epoch 1318: train loss 4.966728210449219 val loss 4.9651360511779785\n",
      "Epoch 1319: train loss 4.9651360511779785 val loss 4.965649604797363\n",
      "Epoch 1320: train loss 4.965649604797363 val loss 4.965080261230469\n",
      "Epoch 1321: train loss 4.965080261230469 val loss 4.9659881591796875\n",
      "Epoch 1322: train loss 4.9659881591796875 val loss 4.96505069732666\n",
      "Epoch 1323: train loss 4.96505069732666 val loss 4.965445518493652\n",
      "Epoch 1324: train loss 4.965445518493652 val loss 4.965234279632568\n",
      "Epoch 1325: train loss 4.965234279632568 val loss 4.965144157409668\n",
      "Epoch 1326: train loss 4.965144157409668 val loss 4.965088367462158\n",
      "Epoch 1327: train loss 4.965088367462158 val loss 4.9652605056762695\n",
      "Epoch 1328: train loss 4.9652605056762695 val loss 4.9650139808654785\n",
      "Epoch 1329: train loss 4.9650139808654785 val loss 4.965266227722168\n",
      "Epoch 1330: train loss 4.965266227722168 val loss 4.965145587921143\n",
      "Epoch 1331: train loss 4.965145587921143 val loss 4.9649786949157715\n",
      "Epoch 1332: train loss 4.9649786949157715 val loss 4.965086460113525\n",
      "Epoch 1333: train loss 4.965086460113525 val loss 4.965240478515625\n",
      "Epoch 1334: train loss 4.965240478515625 val loss 4.96502685546875\n",
      "Epoch 1335: train loss 4.96502685546875 val loss 4.965367317199707\n",
      "Epoch 1336: train loss 4.965367317199707 val loss 4.965115070343018\n",
      "Epoch 1337: train loss 4.965115070343018 val loss 4.96506404876709\n",
      "Epoch 1338: train loss 4.96506404876709 val loss 4.965700149536133\n",
      "Epoch 1339: train loss 4.965700149536133 val loss 4.964951038360596\n",
      "Epoch 1340: train loss 4.964951038360596 val loss 4.965272903442383\n",
      "Epoch 1341: train loss 4.965272903442383 val loss 4.964924335479736\n",
      "Epoch 1342: train loss 4.964924335479736 val loss 4.965953350067139\n",
      "Epoch 1343: train loss 4.965953350067139 val loss 4.965063095092773\n",
      "Epoch 1344: train loss 4.965063095092773 val loss 4.966004848480225\n",
      "Epoch 1345: train loss 4.966004848480225 val loss 4.965065956115723\n",
      "Epoch 1346: train loss 4.965065956115723 val loss 4.966185092926025\n",
      "Epoch 1347: train loss 4.966185092926025 val loss 4.965041160583496\n",
      "Epoch 1348: train loss 4.965041160583496 val loss 4.966510772705078\n",
      "Epoch 1349: train loss 4.966510772705078 val loss 4.96522331237793\n",
      "Epoch 1350: train loss 4.96522331237793 val loss 4.9666748046875\n",
      "Epoch 1351: train loss 4.9666748046875 val loss 4.965184688568115\n",
      "Epoch 1352: train loss 4.965184688568115 val loss 4.965663909912109\n",
      "Epoch 1353: train loss 4.965663909912109 val loss 4.964995384216309\n",
      "Epoch 1354: train loss 4.964995384216309 val loss 4.964942932128906\n",
      "Epoch 1355: train loss 4.964942932128906 val loss 4.965217590332031\n",
      "Epoch 1356: train loss 4.965217590332031 val loss 4.9654083251953125\n",
      "Epoch 1357: train loss 4.9654083251953125 val loss 4.965151309967041\n",
      "Epoch 1358: train loss 4.965151309967041 val loss 4.965953350067139\n",
      "Epoch 1359: train loss 4.965953350067139 val loss 4.965206146240234\n",
      "Epoch 1360: train loss 4.965206146240234 val loss 4.964944839477539\n",
      "Epoch 1361: train loss 4.964944839477539 val loss 4.9652814865112305\n",
      "Epoch 1362: train loss 4.9652814865112305 val loss 4.96510124206543\n",
      "Epoch 1363: train loss 4.96510124206543 val loss 4.9659905433654785\n",
      "Epoch 1364: train loss 4.9659905433654785 val loss 4.965074062347412\n",
      "Epoch 1365: train loss 4.965074062347412 val loss 4.966436862945557\n",
      "Epoch 1366: train loss 4.966436862945557 val loss 4.96517276763916\n",
      "Epoch 1367: train loss 4.96517276763916 val loss 4.965672492980957\n",
      "Epoch 1368: train loss 4.965672492980957 val loss 4.965357303619385\n",
      "Epoch 1369: train loss 4.965357303619385 val loss 4.965176105499268\n",
      "Epoch 1370: train loss 4.965176105499268 val loss 4.965026378631592\n",
      "Epoch 1371: train loss 4.965026378631592 val loss 4.965137481689453\n",
      "Epoch 1372: train loss 4.965137481689453 val loss 4.964902877807617\n",
      "Epoch 1373: train loss 4.964902877807617 val loss 4.96531343460083\n",
      "Epoch 1374: train loss 4.96531343460083 val loss 4.965363025665283\n",
      "Epoch 1375: train loss 4.965363025665283 val loss 4.965027809143066\n",
      "Epoch 1376: train loss 4.965027809143066 val loss 4.966069221496582\n",
      "Epoch 1377: train loss 4.966069221496582 val loss 4.964941024780273\n",
      "Epoch 1378: train loss 4.964941024780273 val loss 4.965908527374268\n",
      "Epoch 1379: train loss 4.965908527374268 val loss 4.965076446533203\n",
      "Epoch 1380: train loss 4.965076446533203 val loss 4.96630859375\n",
      "Epoch 1381: train loss 4.96630859375 val loss 4.965022563934326\n",
      "Epoch 1382: train loss 4.965022563934326 val loss 4.965717315673828\n",
      "Epoch 1383: train loss 4.965717315673828 val loss 4.964897155761719\n",
      "Epoch 1384: train loss 4.964897155761719 val loss 4.96636438369751\n",
      "Epoch 1385: train loss 4.96636438369751 val loss 4.965157508850098\n",
      "Epoch 1386: train loss 4.965157508850098 val loss 4.966407775878906\n",
      "Epoch 1387: train loss 4.966407775878906 val loss 4.965741157531738\n",
      "Epoch 1388: train loss 4.965741157531738 val loss 4.966314315795898\n",
      "Epoch 1389: train loss 4.966314315795898 val loss 4.966094970703125\n",
      "Epoch 1390: train loss 4.966094970703125 val loss 4.965607166290283\n",
      "Epoch 1391: train loss 4.965607166290283 val loss 4.966058254241943\n",
      "Epoch 1392: train loss 4.966058254241943 val loss 4.965308666229248\n",
      "Epoch 1393: train loss 4.965308666229248 val loss 4.96773099899292\n",
      "Epoch 1394: train loss 4.96773099899292 val loss 4.96536922454834\n",
      "Epoch 1395: train loss 4.96536922454834 val loss 4.965686798095703\n",
      "Epoch 1396: train loss 4.965686798095703 val loss 4.965667247772217\n",
      "Epoch 1397: train loss 4.965667247772217 val loss 4.965027809143066\n",
      "Epoch 1398: train loss 4.965027809143066 val loss 4.9656805992126465\n",
      "Epoch 1399: train loss 4.9656805992126465 val loss 4.965251922607422\n",
      "Epoch 1400: train loss 4.965251922607422 val loss 4.96532678604126\n",
      "Epoch 1401: train loss 4.96532678604126 val loss 4.965461730957031\n",
      "Epoch 1402: train loss 4.965461730957031 val loss 4.965202331542969\n",
      "Epoch 1403: train loss 4.965202331542969 val loss 4.965827941894531\n",
      "Epoch 1404: train loss 4.965827941894531 val loss 4.965261936187744\n",
      "Epoch 1405: train loss 4.965261936187744 val loss 4.965343952178955\n",
      "Epoch 1406: train loss 4.965343952178955 val loss 4.9652605056762695\n",
      "Epoch 1407: train loss 4.9652605056762695 val loss 4.965400218963623\n",
      "Epoch 1408: train loss 4.965400218963623 val loss 4.965369701385498\n",
      "Epoch 1409: train loss 4.965369701385498 val loss 4.965210914611816\n",
      "Epoch 1410: train loss 4.965210914611816 val loss 4.9652581214904785\n",
      "Epoch 1411: train loss 4.9652581214904785 val loss 4.96534538269043\n",
      "Epoch 1412: train loss 4.96534538269043 val loss 4.965126991271973\n",
      "Epoch 1413: train loss 4.965126991271973 val loss 4.965680122375488\n",
      "Epoch 1414: train loss 4.965680122375488 val loss 4.965084075927734\n",
      "Epoch 1415: train loss 4.965084075927734 val loss 4.966218948364258\n",
      "Epoch 1416: train loss 4.966218948364258 val loss 4.9654974937438965\n",
      "Epoch 1417: train loss 4.9654974937438965 val loss 4.965689659118652\n",
      "Epoch 1418: train loss 4.965689659118652 val loss 4.965551376342773\n",
      "Epoch 1419: train loss 4.965551376342773 val loss 4.965845584869385\n",
      "Epoch 1420: train loss 4.965845584869385 val loss 4.966546535491943\n",
      "Epoch 1421: train loss 4.966546535491943 val loss 4.965714454650879\n",
      "Epoch 1422: train loss 4.965714454650879 val loss 4.966706275939941\n",
      "Epoch 1423: train loss 4.966706275939941 val loss 4.96552848815918\n",
      "Epoch 1424: train loss 4.96552848815918 val loss 4.966444969177246\n",
      "Epoch 1425: train loss 4.966444969177246 val loss 4.965399742126465\n",
      "Epoch 1426: train loss 4.965399742126465 val loss 4.966253757476807\n",
      "Epoch 1427: train loss 4.966253757476807 val loss 4.965080261230469\n",
      "Epoch 1428: train loss 4.965080261230469 val loss 4.965442657470703\n",
      "Epoch 1429: train loss 4.965442657470703 val loss 4.965206146240234\n",
      "Epoch 1430: train loss 4.965206146240234 val loss 4.965330123901367\n",
      "Epoch 1431: train loss 4.965330123901367 val loss 4.9655561447143555\n",
      "Epoch 1432: train loss 4.9655561447143555 val loss 4.965100288391113\n",
      "Epoch 1433: train loss 4.965100288391113 val loss 4.965468406677246\n",
      "Epoch 1434: train loss 4.965468406677246 val loss 4.965144157409668\n",
      "Epoch 1435: train loss 4.965144157409668 val loss 4.965002059936523\n",
      "Epoch 1436: train loss 4.965002059936523 val loss 4.965775489807129\n",
      "Epoch 1437: train loss 4.965775489807129 val loss 4.9650421142578125\n",
      "Epoch 1438: train loss 4.9650421142578125 val loss 4.965506076812744\n",
      "Epoch 1439: train loss 4.965506076812744 val loss 4.965157508850098\n",
      "Epoch 1440: train loss 4.965157508850098 val loss 4.965173244476318\n",
      "Epoch 1441: train loss 4.965173244476318 val loss 4.965171813964844\n",
      "Epoch 1442: train loss 4.965171813964844 val loss 4.965174674987793\n",
      "Epoch 1443: train loss 4.965174674987793 val loss 4.96489953994751\n",
      "Epoch 1444: train loss 4.96489953994751 val loss 4.965023040771484\n",
      "Epoch 1445: train loss 4.965023040771484 val loss 4.96492862701416\n",
      "Epoch 1446: train loss 4.96492862701416 val loss 4.965132236480713\n",
      "Epoch 1447: train loss 4.965132236480713 val loss 4.9649882316589355\n",
      "Epoch 1448: train loss 4.9649882316589355 val loss 4.965696334838867\n",
      "Epoch 1449: train loss 4.965696334838867 val loss 4.965009689331055\n",
      "Epoch 1450: train loss 4.965009689331055 val loss 4.96504020690918\n",
      "Epoch 1451: train loss 4.96504020690918 val loss 4.965755939483643\n",
      "Epoch 1452: train loss 4.965755939483643 val loss 4.965277671813965\n",
      "Epoch 1453: train loss 4.965277671813965 val loss 4.965155601501465\n",
      "Epoch 1454: train loss 4.965155601501465 val loss 4.965454578399658\n",
      "Epoch 1455: train loss 4.965454578399658 val loss 4.965050220489502\n",
      "Epoch 1456: train loss 4.965050220489502 val loss 4.9663238525390625\n",
      "Epoch 1457: train loss 4.9663238525390625 val loss 4.965625762939453\n",
      "Epoch 1458: train loss 4.965625762939453 val loss 4.966653823852539\n",
      "Epoch 1459: train loss 4.966653823852539 val loss 4.965295791625977\n",
      "Epoch 1460: train loss 4.965295791625977 val loss 4.966467380523682\n",
      "Epoch 1461: train loss 4.966467380523682 val loss 4.96500825881958\n",
      "Epoch 1462: train loss 4.96500825881958 val loss 4.965162754058838\n",
      "Epoch 1463: train loss 4.965162754058838 val loss 4.965429306030273\n",
      "Epoch 1464: train loss 4.965429306030273 val loss 4.965517044067383\n",
      "Epoch 1465: train loss 4.965517044067383 val loss 4.965472221374512\n",
      "Epoch 1466: train loss 4.965472221374512 val loss 4.965219497680664\n",
      "Epoch 1467: train loss 4.965219497680664 val loss 4.965163707733154\n",
      "Epoch 1468: train loss 4.965163707733154 val loss 4.9649457931518555\n",
      "Epoch 1469: train loss 4.9649457931518555 val loss 4.965892314910889\n",
      "Epoch 1470: train loss 4.965892314910889 val loss 4.965121746063232\n",
      "Epoch 1471: train loss 4.965121746063232 val loss 4.965233325958252\n",
      "Epoch 1472: train loss 4.965233325958252 val loss 4.965673923492432\n",
      "Epoch 1473: train loss 4.965673923492432 val loss 4.9650468826293945\n",
      "Epoch 1474: train loss 4.9650468826293945 val loss 4.965572834014893\n",
      "Epoch 1475: train loss 4.965572834014893 val loss 4.96512508392334\n",
      "Epoch 1476: train loss 4.96512508392334 val loss 4.9656291007995605\n",
      "Epoch 1477: train loss 4.9656291007995605 val loss 4.964898109436035\n",
      "Epoch 1478: train loss 4.964898109436035 val loss 4.965064525604248\n",
      "Epoch 1479: train loss 4.965064525604248 val loss 4.965082168579102\n",
      "Epoch 1480: train loss 4.965082168579102 val loss 4.965044021606445\n",
      "Epoch 1481: train loss 4.965044021606445 val loss 4.965086936950684\n",
      "Epoch 1482: train loss 4.965086936950684 val loss 4.965357780456543\n",
      "Epoch 1483: train loss 4.965357780456543 val loss 4.965006351470947\n",
      "Epoch 1484: train loss 4.965006351470947 val loss 4.965394973754883\n",
      "Epoch 1485: train loss 4.965394973754883 val loss 4.9649658203125\n",
      "Epoch 1486: train loss 4.9649658203125 val loss 4.965814113616943\n",
      "Epoch 1487: train loss 4.965814113616943 val loss 4.964850902557373\n",
      "Epoch 1488: train loss 4.964850902557373 val loss 4.965522766113281\n",
      "Epoch 1489: train loss 4.965522766113281 val loss 4.965242862701416\n",
      "Epoch 1490: train loss 4.965242862701416 val loss 4.9656829833984375\n",
      "Epoch 1491: train loss 4.9656829833984375 val loss 4.964980125427246\n",
      "Epoch 1492: train loss 4.964980125427246 val loss 4.967322826385498\n",
      "Epoch 1493: train loss 4.967322826385498 val loss 4.964976787567139\n",
      "Epoch 1494: train loss 4.964976787567139 val loss 4.965785980224609\n",
      "Epoch 1495: train loss 4.965785980224609 val loss 4.9649457931518555\n",
      "Epoch 1496: train loss 4.9649457931518555 val loss 4.965131759643555\n",
      "Epoch 1497: train loss 4.965131759643555 val loss 4.965324878692627\n",
      "Epoch 1498: train loss 4.965324878692627 val loss 4.965145111083984\n",
      "Epoch 1499: train loss 4.965145111083984 val loss 4.9656596183776855\n",
      "Epoch 1500: train loss 4.9656596183776855 val loss 4.96522855758667\n",
      "Epoch 1501: train loss 4.96522855758667 val loss 4.965485095977783\n",
      "Epoch 1502: train loss 4.965485095977783 val loss 4.965704441070557\n",
      "Epoch 1503: train loss 4.965704441070557 val loss 4.965239524841309\n",
      "Epoch 1504: train loss 4.965239524841309 val loss 4.965540885925293\n",
      "Epoch 1505: train loss 4.965540885925293 val loss 4.965085029602051\n",
      "Epoch 1506: train loss 4.965085029602051 val loss 4.9651384353637695\n",
      "Epoch 1507: train loss 4.9651384353637695 val loss 4.965163707733154\n",
      "Epoch 1508: train loss 4.965163707733154 val loss 4.964938163757324\n",
      "Epoch 1509: train loss 4.964938163757324 val loss 4.965542793273926\n",
      "Epoch 1510: train loss 4.965542793273926 val loss 4.965114593505859\n",
      "Epoch 1511: train loss 4.965114593505859 val loss 4.966064929962158\n",
      "Epoch 1512: train loss 4.966064929962158 val loss 4.964942932128906\n",
      "Epoch 1513: train loss 4.964942932128906 val loss 4.966455459594727\n",
      "Epoch 1514: train loss 4.966455459594727 val loss 4.964937210083008\n",
      "Epoch 1515: train loss 4.964937210083008 val loss 4.967140197753906\n",
      "Epoch 1516: train loss 4.967140197753906 val loss 4.965704917907715\n",
      "Epoch 1517: train loss 4.965704917907715 val loss 4.967176914215088\n",
      "Epoch 1518: train loss 4.967176914215088 val loss 4.9663615226745605\n",
      "Epoch 1519: train loss 4.9663615226745605 val loss 4.966901779174805\n",
      "Epoch 1520: train loss 4.966901779174805 val loss 4.967700481414795\n",
      "Epoch 1521: train loss 4.967700481414795 val loss 4.9654154777526855\n",
      "Epoch 1522: train loss 4.9654154777526855 val loss 4.966907501220703\n",
      "Epoch 1523: train loss 4.966907501220703 val loss 4.965434551239014\n",
      "Epoch 1524: train loss 4.965434551239014 val loss 4.965399742126465\n",
      "Epoch 1525: train loss 4.965399742126465 val loss 4.965079307556152\n",
      "Epoch 1526: train loss 4.965079307556152 val loss 4.965000152587891\n",
      "Epoch 1527: train loss 4.965000152587891 val loss 4.965157508850098\n",
      "Epoch 1528: train loss 4.965157508850098 val loss 4.964966773986816\n",
      "Epoch 1529: train loss 4.964966773986816 val loss 4.965757369995117\n",
      "Epoch 1530: train loss 4.965757369995117 val loss 4.964931488037109\n",
      "Epoch 1531: train loss 4.964931488037109 val loss 4.96497106552124\n",
      "Epoch 1532: train loss 4.96497106552124 val loss 4.965391159057617\n",
      "Epoch 1533: train loss 4.965391159057617 val loss 4.964954853057861\n",
      "Epoch 1534: train loss 4.964954853057861 val loss 4.965235233306885\n",
      "Epoch 1535: train loss 4.965235233306885 val loss 4.96498966217041\n",
      "Epoch 1536: train loss 4.96498966217041 val loss 4.965225696563721\n",
      "Epoch 1537: train loss 4.965225696563721 val loss 4.965060234069824\n",
      "Epoch 1538: train loss 4.965060234069824 val loss 4.965290069580078\n",
      "Epoch 1539: train loss 4.965290069580078 val loss 4.964940071105957\n",
      "Epoch 1540: train loss 4.964940071105957 val loss 4.965678691864014\n",
      "Epoch 1541: train loss 4.965678691864014 val loss 4.96492862701416\n",
      "Epoch 1542: train loss 4.96492862701416 val loss 4.964689254760742\n",
      "Epoch 1543: train loss 4.964689254760742 val loss 4.965793609619141\n",
      "Epoch 1544: train loss 4.965793609619141 val loss 4.964764595031738\n",
      "Epoch 1545: train loss 4.964764595031738 val loss 4.965659141540527\n",
      "Epoch 1546: train loss 4.965659141540527 val loss 4.964787483215332\n",
      "Epoch 1547: train loss 4.964787483215332 val loss 4.9665398597717285\n",
      "Epoch 1548: train loss 4.9665398597717285 val loss 4.964999675750732\n",
      "Epoch 1549: train loss 4.964999675750732 val loss 4.9668145179748535\n",
      "Epoch 1550: train loss 4.9668145179748535 val loss 4.966086387634277\n",
      "Epoch 1551: train loss 4.966086387634277 val loss 4.965723037719727\n",
      "Epoch 1552: train loss 4.965723037719727 val loss 4.9659037590026855\n",
      "Epoch 1553: train loss 4.9659037590026855 val loss 4.96530818939209\n",
      "Epoch 1554: train loss 4.96530818939209 val loss 4.966322898864746\n",
      "Epoch 1555: train loss 4.966322898864746 val loss 4.965225696563721\n",
      "Epoch 1556: train loss 4.965225696563721 val loss 4.965949058532715\n",
      "Epoch 1557: train loss 4.965949058532715 val loss 4.965108394622803\n",
      "Epoch 1558: train loss 4.965108394622803 val loss 4.965726852416992\n",
      "Epoch 1559: train loss 4.965726852416992 val loss 4.965153694152832\n",
      "Epoch 1560: train loss 4.965153694152832 val loss 4.9650797843933105\n",
      "Epoch 1561: train loss 4.9650797843933105 val loss 4.965823650360107\n",
      "Epoch 1562: train loss 4.965823650360107 val loss 4.965741157531738\n",
      "Epoch 1563: train loss 4.965741157531738 val loss 4.966289520263672\n",
      "Epoch 1564: train loss 4.966289520263672 val loss 4.964945316314697\n",
      "Epoch 1565: train loss 4.964945316314697 val loss 4.966718673706055\n",
      "Epoch 1566: train loss 4.966718673706055 val loss 4.964857578277588\n",
      "Epoch 1567: train loss 4.964857578277588 val loss 4.966170310974121\n",
      "Epoch 1568: train loss 4.966170310974121 val loss 4.965211868286133\n",
      "Epoch 1569: train loss 4.965211868286133 val loss 4.966024875640869\n",
      "Epoch 1570: train loss 4.966024875640869 val loss 4.965602874755859\n",
      "Epoch 1571: train loss 4.965602874755859 val loss 4.965363025665283\n",
      "Epoch 1572: train loss 4.965363025665283 val loss 4.965292453765869\n",
      "Epoch 1573: train loss 4.965292453765869 val loss 4.964944362640381\n",
      "Epoch 1574: train loss 4.964944362640381 val loss 4.966382026672363\n",
      "Epoch 1575: train loss 4.966382026672363 val loss 4.965070724487305\n",
      "Epoch 1576: train loss 4.965070724487305 val loss 4.9660964012146\n",
      "Epoch 1577: train loss 4.9660964012146 val loss 4.964869976043701\n",
      "Epoch 1578: train loss 4.964869976043701 val loss 4.966710090637207\n",
      "Epoch 1579: train loss 4.966710090637207 val loss 4.965031623840332\n",
      "Epoch 1580: train loss 4.965031623840332 val loss 4.965577125549316\n",
      "Epoch 1581: train loss 4.965577125549316 val loss 4.965053558349609\n",
      "Epoch 1582: train loss 4.965053558349609 val loss 4.965569972991943\n",
      "Epoch 1583: train loss 4.965569972991943 val loss 4.964851379394531\n",
      "Epoch 1584: train loss 4.964851379394531 val loss 4.9651103019714355\n",
      "Epoch 1585: train loss 4.9651103019714355 val loss 4.964954376220703\n",
      "Epoch 1586: train loss 4.964954376220703 val loss 4.965205192565918\n",
      "Epoch 1587: train loss 4.965205192565918 val loss 4.9647979736328125\n",
      "Epoch 1588: train loss 4.9647979736328125 val loss 4.964608192443848\n",
      "Epoch 1589: train loss 4.964608192443848 val loss 4.965184688568115\n",
      "Epoch 1590: train loss 4.965184688568115 val loss 4.965419769287109\n",
      "Epoch 1591: train loss 4.965419769287109 val loss 4.96519136428833\n",
      "Epoch 1592: train loss 4.96519136428833 val loss 4.966231346130371\n",
      "Epoch 1593: train loss 4.966231346130371 val loss 4.964771747589111\n",
      "Epoch 1594: train loss 4.964771747589111 val loss 4.9664306640625\n",
      "Epoch 1595: train loss 4.9664306640625 val loss 4.9657487869262695\n",
      "Epoch 1596: train loss 4.9657487869262695 val loss 4.96588134765625\n",
      "Epoch 1597: train loss 4.96588134765625 val loss 4.966737270355225\n",
      "Epoch 1598: train loss 4.966737270355225 val loss 4.965854167938232\n",
      "Epoch 1599: train loss 4.965854167938232 val loss 4.967301368713379\n",
      "Epoch 1600: train loss 4.967301368713379 val loss 4.966587543487549\n",
      "Epoch 1601: train loss 4.966587543487549 val loss 4.965415954589844\n",
      "Epoch 1602: train loss 4.965415954589844 val loss 4.967693328857422\n",
      "Epoch 1603: train loss 4.967693328857422 val loss 4.96516752243042\n",
      "Epoch 1604: train loss 4.96516752243042 val loss 4.967029571533203\n",
      "Epoch 1605: train loss 4.967029571533203 val loss 4.9669060707092285\n",
      "Epoch 1606: train loss 4.9669060707092285 val loss 4.965298652648926\n",
      "Epoch 1607: train loss 4.965298652648926 val loss 4.967580318450928\n",
      "Epoch 1608: train loss 4.967580318450928 val loss 4.965312957763672\n",
      "Epoch 1609: train loss 4.965312957763672 val loss 4.966784954071045\n",
      "Epoch 1610: train loss 4.966784954071045 val loss 4.967474937438965\n",
      "Epoch 1611: train loss 4.967474937438965 val loss 4.965296745300293\n",
      "Epoch 1612: train loss 4.965296745300293 val loss 4.966643333435059\n",
      "Epoch 1613: train loss 4.966643333435059 val loss 4.965394020080566\n",
      "Epoch 1614: train loss 4.965394020080566 val loss 4.965513229370117\n",
      "Epoch 1615: train loss 4.965513229370117 val loss 4.965548992156982\n",
      "Epoch 1616: train loss 4.965548992156982 val loss 4.965112686157227\n",
      "Epoch 1617: train loss 4.965112686157227 val loss 4.965743541717529\n",
      "Epoch 1618: train loss 4.965743541717529 val loss 4.964938163757324\n",
      "Epoch 1619: train loss 4.964938163757324 val loss 4.965118885040283\n",
      "Epoch 1620: train loss 4.965118885040283 val loss 4.964935302734375\n",
      "Epoch 1621: train loss 4.964935302734375 val loss 4.965468406677246\n",
      "Epoch 1622: train loss 4.965468406677246 val loss 4.964697360992432\n",
      "Epoch 1623: train loss 4.964697360992432 val loss 4.966233253479004\n",
      "Epoch 1624: train loss 4.966233253479004 val loss 4.965422630310059\n",
      "Epoch 1625: train loss 4.965422630310059 val loss 4.9653120040893555\n",
      "Epoch 1626: train loss 4.9653120040893555 val loss 4.96511173248291\n",
      "Epoch 1627: train loss 4.96511173248291 val loss 4.965002059936523\n",
      "Epoch 1628: train loss 4.965002059936523 val loss 4.965088367462158\n",
      "Epoch 1629: train loss 4.965088367462158 val loss 4.964839935302734\n",
      "Epoch 1630: train loss 4.964839935302734 val loss 4.966391086578369\n",
      "Epoch 1631: train loss 4.966391086578369 val loss 4.965222358703613\n",
      "Epoch 1632: train loss 4.965222358703613 val loss 4.966640472412109\n",
      "Epoch 1633: train loss 4.966640472412109 val loss 4.966830730438232\n",
      "Epoch 1634: train loss 4.966830730438232 val loss 4.965000152587891\n",
      "Epoch 1635: train loss 4.965000152587891 val loss 4.96860408782959\n",
      "Epoch 1636: train loss 4.96860408782959 val loss 4.9665117263793945\n",
      "Epoch 1637: train loss 4.9665117263793945 val loss 4.967691898345947\n",
      "Epoch 1638: train loss 4.967691898345947 val loss 4.9695587158203125\n",
      "Epoch 1639: train loss 4.9695587158203125 val loss 4.967704772949219\n",
      "Epoch 1640: train loss 4.967704772949219 val loss 4.965261459350586\n",
      "Epoch 1641: train loss 4.965261459350586 val loss 4.969936847686768\n",
      "Epoch 1642: train loss 4.969936847686768 val loss 4.967515468597412\n",
      "Epoch 1643: train loss 4.967515468597412 val loss 4.966224193572998\n",
      "Epoch 1644: train loss 4.966224193572998 val loss 4.969669818878174\n",
      "Epoch 1645: train loss 4.969669818878174 val loss 4.968563079833984\n",
      "Epoch 1646: train loss 4.968563079833984 val loss 4.96531343460083\n",
      "Epoch 1647: train loss 4.96531343460083 val loss 4.969359397888184\n",
      "Epoch 1648: train loss 4.969359397888184 val loss 4.967818260192871\n",
      "Epoch 1649: train loss 4.967818260192871 val loss 4.965478897094727\n",
      "Epoch 1650: train loss 4.965478897094727 val loss 4.968562126159668\n",
      "Epoch 1651: train loss 4.968562126159668 val loss 4.967500686645508\n",
      "Epoch 1652: train loss 4.967500686645508 val loss 4.965395927429199\n",
      "Epoch 1653: train loss 4.965395927429199 val loss 4.968984603881836\n",
      "Epoch 1654: train loss 4.968984603881836 val loss 4.967759132385254\n",
      "Epoch 1655: train loss 4.967759132385254 val loss 4.9656243324279785\n",
      "Epoch 1656: train loss 4.9656243324279785 val loss 4.96801233291626\n",
      "Epoch 1657: train loss 4.96801233291626 val loss 4.966980934143066\n",
      "Epoch 1658: train loss 4.966980934143066 val loss 4.965226650238037\n",
      "Epoch 1659: train loss 4.965226650238037 val loss 4.966740608215332\n",
      "Epoch 1660: train loss 4.966740608215332 val loss 4.9653730392456055\n",
      "Epoch 1661: train loss 4.9653730392456055 val loss 4.966134071350098\n",
      "Epoch 1662: train loss 4.966134071350098 val loss 4.966148853302002\n",
      "Epoch 1663: train loss 4.966148853302002 val loss 4.965465545654297\n",
      "Epoch 1664: train loss 4.965465545654297 val loss 4.965940952301025\n",
      "Epoch 1665: train loss 4.965940952301025 val loss 4.965382099151611\n",
      "Epoch 1666: train loss 4.965382099151611 val loss 4.965354919433594\n",
      "Epoch 1667: train loss 4.965354919433594 val loss 4.965161323547363\n",
      "Epoch 1668: train loss 4.965161323547363 val loss 4.965102195739746\n",
      "Epoch 1669: train loss 4.965102195739746 val loss 4.965062141418457\n",
      "Epoch 1670: train loss 4.965062141418457 val loss 4.965184211730957\n",
      "Epoch 1671: train loss 4.965184211730957 val loss 4.965024948120117\n",
      "Epoch 1672: train loss 4.965024948120117 val loss 4.965421676635742\n",
      "Epoch 1673: train loss 4.965421676635742 val loss 4.9648332595825195\n",
      "Epoch 1674: train loss 4.9648332595825195 val loss 4.9651198387146\n",
      "Epoch 1675: train loss 4.9651198387146 val loss 4.964906692504883\n",
      "Epoch 1676: train loss 4.964906692504883 val loss 4.964803218841553\n",
      "Epoch 1677: train loss 4.964803218841553 val loss 4.964944362640381\n",
      "Epoch 1678: train loss 4.964944362640381 val loss 4.964796543121338\n",
      "Epoch 1679: train loss 4.964796543121338 val loss 4.964797019958496\n",
      "Epoch 1680: train loss 4.964797019958496 val loss 4.965658187866211\n",
      "Epoch 1681: train loss 4.965658187866211 val loss 4.964995384216309\n",
      "Epoch 1682: train loss 4.964995384216309 val loss 4.966159343719482\n",
      "Epoch 1683: train loss 4.966159343719482 val loss 4.965057849884033\n",
      "Epoch 1684: train loss 4.965057849884033 val loss 4.966035842895508\n",
      "Epoch 1685: train loss 4.966035842895508 val loss 4.965643882751465\n",
      "Epoch 1686: train loss 4.965643882751465 val loss 4.96533203125\n",
      "Epoch 1687: train loss 4.96533203125 val loss 4.965474605560303\n",
      "Epoch 1688: train loss 4.965474605560303 val loss 4.9652628898620605\n",
      "Epoch 1689: train loss 4.9652628898620605 val loss 4.965603351593018\n",
      "Epoch 1690: train loss 4.965603351593018 val loss 4.965085029602051\n",
      "Epoch 1691: train loss 4.965085029602051 val loss 4.96494722366333\n",
      "Epoch 1692: train loss 4.96494722366333 val loss 4.965431213378906\n",
      "Epoch 1693: train loss 4.965431213378906 val loss 4.965573310852051\n",
      "Epoch 1694: train loss 4.965573310852051 val loss 4.966179847717285\n",
      "Epoch 1695: train loss 4.966179847717285 val loss 4.965219497680664\n",
      "Epoch 1696: train loss 4.965219497680664 val loss 4.965890407562256\n",
      "Epoch 1697: train loss 4.965890407562256 val loss 4.965023040771484\n",
      "Epoch 1698: train loss 4.965023040771484 val loss 4.965767860412598\n",
      "Epoch 1699: train loss 4.965767860412598 val loss 4.965315341949463\n",
      "Epoch 1700: train loss 4.965315341949463 val loss 4.9654645919799805\n",
      "Epoch 1701: train loss 4.9654645919799805 val loss 4.965592861175537\n",
      "Epoch 1702: train loss 4.965592861175537 val loss 4.965444564819336\n",
      "Epoch 1703: train loss 4.965444564819336 val loss 4.965817451477051\n",
      "Epoch 1704: train loss 4.965817451477051 val loss 4.9655070304870605\n",
      "Epoch 1705: train loss 4.9655070304870605 val loss 4.965127944946289\n",
      "Epoch 1706: train loss 4.965127944946289 val loss 4.96636438369751\n",
      "Epoch 1707: train loss 4.96636438369751 val loss 4.965001106262207\n",
      "Epoch 1708: train loss 4.965001106262207 val loss 4.966124057769775\n",
      "Epoch 1709: train loss 4.966124057769775 val loss 4.964986324310303\n",
      "Epoch 1710: train loss 4.964986324310303 val loss 4.966584205627441\n",
      "Epoch 1711: train loss 4.966584205627441 val loss 4.965329170227051\n",
      "Epoch 1712: train loss 4.965329170227051 val loss 4.966456413269043\n",
      "Epoch 1713: train loss 4.966456413269043 val loss 4.966638565063477\n",
      "Epoch 1714: train loss 4.966638565063477 val loss 4.965036869049072\n",
      "Epoch 1715: train loss 4.965036869049072 val loss 4.967741966247559\n",
      "Epoch 1716: train loss 4.967741966247559 val loss 4.965510368347168\n",
      "Epoch 1717: train loss 4.965510368347168 val loss 4.96658182144165\n",
      "Epoch 1718: train loss 4.96658182144165 val loss 4.966664791107178\n",
      "Epoch 1719: train loss 4.966664791107178 val loss 4.965222358703613\n",
      "Epoch 1720: train loss 4.965222358703613 val loss 4.967827796936035\n",
      "Epoch 1721: train loss 4.967827796936035 val loss 4.965643882751465\n",
      "Epoch 1722: train loss 4.965643882751465 val loss 4.966195106506348\n",
      "Epoch 1723: train loss 4.966195106506348 val loss 4.966867446899414\n",
      "Epoch 1724: train loss 4.966867446899414 val loss 4.9651641845703125\n",
      "Epoch 1725: train loss 4.9651641845703125 val loss 4.9679179191589355\n",
      "Epoch 1726: train loss 4.9679179191589355 val loss 4.966128826141357\n",
      "Epoch 1727: train loss 4.966128826141357 val loss 4.966202735900879\n",
      "Epoch 1728: train loss 4.966202735900879 val loss 4.967360019683838\n",
      "Epoch 1729: train loss 4.967360019683838 val loss 4.96524715423584\n",
      "Epoch 1730: train loss 4.96524715423584 val loss 4.966489791870117\n",
      "Epoch 1731: train loss 4.966489791870117 val loss 4.965765476226807\n",
      "Epoch 1732: train loss 4.965765476226807 val loss 4.966015338897705\n",
      "Epoch 1733: train loss 4.966015338897705 val loss 4.966154098510742\n",
      "Epoch 1734: train loss 4.966154098510742 val loss 4.965339660644531\n",
      "Epoch 1735: train loss 4.965339660644531 val loss 4.9660539627075195\n",
      "Epoch 1736: train loss 4.9660539627075195 val loss 4.964960098266602\n",
      "Epoch 1737: train loss 4.964960098266602 val loss 4.965384006500244\n",
      "Epoch 1738: train loss 4.965384006500244 val loss 4.965217113494873\n",
      "Epoch 1739: train loss 4.965217113494873 val loss 4.965487480163574\n",
      "Epoch 1740: train loss 4.965487480163574 val loss 4.965590476989746\n",
      "Epoch 1741: train loss 4.965590476989746 val loss 4.965159893035889\n",
      "Epoch 1742: train loss 4.965159893035889 val loss 4.965236663818359\n",
      "Epoch 1743: train loss 4.965236663818359 val loss 4.9648895263671875\n",
      "Epoch 1744: train loss 4.9648895263671875 val loss 4.965109825134277\n",
      "Epoch 1745: train loss 4.965109825134277 val loss 4.965441703796387\n",
      "Epoch 1746: train loss 4.965441703796387 val loss 4.965063095092773\n",
      "Epoch 1747: train loss 4.965063095092773 val loss 4.965216159820557\n",
      "Epoch 1748: train loss 4.965216159820557 val loss 4.9648637771606445\n",
      "Epoch 1749: train loss 4.9648637771606445 val loss 4.965219497680664\n",
      "Epoch 1750: train loss 4.965219497680664 val loss 4.9647650718688965\n",
      "Epoch 1751: train loss 4.9647650718688965 val loss 4.964907169342041\n",
      "Epoch 1752: train loss 4.964907169342041 val loss 4.965166091918945\n",
      "Epoch 1753: train loss 4.965166091918945 val loss 4.964767932891846\n",
      "Epoch 1754: train loss 4.964767932891846 val loss 4.9657673835754395\n",
      "Epoch 1755: train loss 4.9657673835754395 val loss 4.965145587921143\n",
      "Epoch 1756: train loss 4.965145587921143 val loss 4.9651665687561035\n",
      "Epoch 1757: train loss 4.9651665687561035 val loss 4.965056419372559\n",
      "Epoch 1758: train loss 4.965056419372559 val loss 4.964837074279785\n",
      "Epoch 1759: train loss 4.964837074279785 val loss 4.965072154998779\n",
      "Epoch 1760: train loss 4.965072154998779 val loss 4.964625358581543\n",
      "Epoch 1761: train loss 4.964625358581543 val loss 4.966437339782715\n",
      "Epoch 1762: train loss 4.966437339782715 val loss 4.964750289916992\n",
      "Epoch 1763: train loss 4.964750289916992 val loss 4.966338157653809\n",
      "Epoch 1764: train loss 4.966338157653809 val loss 4.965925216674805\n",
      "Epoch 1765: train loss 4.965925216674805 val loss 4.965274810791016\n",
      "Epoch 1766: train loss 4.965274810791016 val loss 4.9658427238464355\n",
      "Epoch 1767: train loss 4.9658427238464355 val loss 4.965388298034668\n",
      "Epoch 1768: train loss 4.965388298034668 val loss 4.967153549194336\n",
      "Epoch 1769: train loss 4.967153549194336 val loss 4.965362548828125\n",
      "Epoch 1770: train loss 4.965362548828125 val loss 4.965422630310059\n",
      "Epoch 1771: train loss 4.965422630310059 val loss 4.965703010559082\n",
      "Epoch 1772: train loss 4.965703010559082 val loss 4.965424060821533\n",
      "Epoch 1773: train loss 4.965424060821533 val loss 4.9659833908081055\n",
      "Epoch 1774: train loss 4.9659833908081055 val loss 4.965043544769287\n",
      "Epoch 1775: train loss 4.965043544769287 val loss 4.968087673187256\n",
      "Epoch 1776: train loss 4.968087673187256 val loss 4.965071678161621\n",
      "Epoch 1777: train loss 4.965071678161621 val loss 4.967073440551758\n",
      "Epoch 1778: train loss 4.967073440551758 val loss 4.967207431793213\n",
      "Epoch 1779: train loss 4.967207431793213 val loss 4.964797019958496\n",
      "Epoch 1780: train loss 4.964797019958496 val loss 4.967998504638672\n",
      "Epoch 1781: train loss 4.967998504638672 val loss 4.966100692749023\n",
      "Epoch 1782: train loss 4.966100692749023 val loss 4.967306137084961\n",
      "Epoch 1783: train loss 4.967306137084961 val loss 4.96884298324585\n",
      "Epoch 1784: train loss 4.96884298324585 val loss 4.966001510620117\n",
      "Epoch 1785: train loss 4.966001510620117 val loss 4.967962265014648\n",
      "Epoch 1786: train loss 4.967962265014648 val loss 4.968473434448242\n",
      "Epoch 1787: train loss 4.968473434448242 val loss 4.965336799621582\n",
      "Epoch 1788: train loss 4.965336799621582 val loss 4.967999458312988\n",
      "Epoch 1789: train loss 4.967999458312988 val loss 4.967296123504639\n",
      "Epoch 1790: train loss 4.967296123504639 val loss 4.9650702476501465\n",
      "Epoch 1791: train loss 4.9650702476501465 val loss 4.968610763549805\n",
      "Epoch 1792: train loss 4.968610763549805 val loss 4.966085910797119\n",
      "Epoch 1793: train loss 4.966085910797119 val loss 4.967186450958252\n",
      "Epoch 1794: train loss 4.967186450958252 val loss 4.968609809875488\n",
      "Epoch 1795: train loss 4.968609809875488 val loss 4.965922832489014\n",
      "Epoch 1796: train loss 4.965922832489014 val loss 4.967534065246582\n",
      "Epoch 1797: train loss 4.967534065246582 val loss 4.9681267738342285\n",
      "Epoch 1798: train loss 4.9681267738342285 val loss 4.964725971221924\n",
      "Epoch 1799: train loss 4.964725971221924 val loss 4.966276168823242\n",
      "Epoch 1800: train loss 4.966276168823242 val loss 4.965374946594238\n",
      "Epoch 1801: train loss 4.965374946594238 val loss 4.9661383628845215\n",
      "Epoch 1802: train loss 4.9661383628845215 val loss 4.9665446281433105\n",
      "Epoch 1803: train loss 4.9665446281433105 val loss 4.965308666229248\n",
      "Epoch 1804: train loss 4.965308666229248 val loss 4.967218399047852\n",
      "Epoch 1805: train loss 4.967218399047852 val loss 4.966074466705322\n",
      "Epoch 1806: train loss 4.966074466705322 val loss 4.965605735778809\n",
      "Epoch 1807: train loss 4.965605735778809 val loss 4.966855525970459\n",
      "Epoch 1808: train loss 4.966855525970459 val loss 4.965156555175781\n",
      "Epoch 1809: train loss 4.965156555175781 val loss 4.967257976531982\n",
      "Epoch 1810: train loss 4.967257976531982 val loss 4.965902805328369\n",
      "Epoch 1811: train loss 4.965902805328369 val loss 4.965198516845703\n",
      "Epoch 1812: train loss 4.965198516845703 val loss 4.966387748718262\n",
      "Epoch 1813: train loss 4.966387748718262 val loss 4.965045928955078\n",
      "Epoch 1814: train loss 4.965045928955078 val loss 4.966045379638672\n",
      "Epoch 1815: train loss 4.966045379638672 val loss 4.965043544769287\n",
      "Epoch 1816: train loss 4.965043544769287 val loss 4.965182781219482\n",
      "Epoch 1817: train loss 4.965182781219482 val loss 4.965112686157227\n",
      "Epoch 1818: train loss 4.965112686157227 val loss 4.9649457931518555\n",
      "Epoch 1819: train loss 4.9649457931518555 val loss 4.965414047241211\n",
      "Epoch 1820: train loss 4.965414047241211 val loss 4.964874267578125\n",
      "Epoch 1821: train loss 4.964874267578125 val loss 4.965661525726318\n",
      "Epoch 1822: train loss 4.965661525726318 val loss 4.964853286743164\n",
      "Epoch 1823: train loss 4.964853286743164 val loss 4.964931488037109\n",
      "Epoch 1824: train loss 4.964931488037109 val loss 4.964958667755127\n",
      "Epoch 1825: train loss 4.964958667755127 val loss 4.9647040367126465\n",
      "Epoch 1826: train loss 4.9647040367126465 val loss 4.965082168579102\n",
      "Epoch 1827: train loss 4.965082168579102 val loss 4.9649977684021\n",
      "Epoch 1828: train loss 4.9649977684021 val loss 4.965146541595459\n",
      "Epoch 1829: train loss 4.965146541595459 val loss 4.964961528778076\n",
      "Epoch 1830: train loss 4.964961528778076 val loss 4.964867115020752\n",
      "Epoch 1831: train loss 4.964867115020752 val loss 4.965087413787842\n",
      "Epoch 1832: train loss 4.965087413787842 val loss 4.964887619018555\n",
      "Epoch 1833: train loss 4.964887619018555 val loss 4.964837551116943\n",
      "Epoch 1834: train loss 4.964837551116943 val loss 4.964788436889648\n",
      "Epoch 1835: train loss 4.964788436889648 val loss 4.96481990814209\n",
      "Epoch 1836: train loss 4.96481990814209 val loss 4.964788913726807\n",
      "Epoch 1837: train loss 4.964788913726807 val loss 4.964580535888672\n",
      "Epoch 1838: train loss 4.964580535888672 val loss 4.965801239013672\n",
      "Epoch 1839: train loss 4.965801239013672 val loss 4.9645490646362305\n",
      "Epoch 1840: train loss 4.9645490646362305 val loss 4.964738845825195\n",
      "Epoch 1841: train loss 4.964738845825195 val loss 4.964554309844971\n",
      "Epoch 1842: train loss 4.964554309844971 val loss 4.964725017547607\n",
      "Epoch 1843: train loss 4.964725017547607 val loss 4.964689254760742\n",
      "Epoch 1844: train loss 4.964689254760742 val loss 4.964724540710449\n",
      "Epoch 1845: train loss 4.964724540710449 val loss 4.964671611785889\n",
      "Epoch 1846: train loss 4.964671611785889 val loss 4.964598178863525\n",
      "Epoch 1847: train loss 4.964598178863525 val loss 4.9646100997924805\n",
      "Epoch 1848: train loss 4.9646100997924805 val loss 4.965129852294922\n",
      "Epoch 1849: train loss 4.965129852294922 val loss 4.964676856994629\n",
      "Epoch 1850: train loss 4.964676856994629 val loss 4.964570045471191\n",
      "Epoch 1851: train loss 4.964570045471191 val loss 4.965391159057617\n",
      "Epoch 1852: train loss 4.965391159057617 val loss 4.965003490447998\n",
      "Epoch 1853: train loss 4.965003490447998 val loss 4.964810848236084\n",
      "Epoch 1854: train loss 4.964810848236084 val loss 4.964725494384766\n",
      "Epoch 1855: train loss 4.964725494384766 val loss 4.96467399597168\n",
      "Epoch 1856: train loss 4.96467399597168 val loss 4.96481990814209\n",
      "Epoch 1857: train loss 4.96481990814209 val loss 4.964893341064453\n",
      "Epoch 1858: train loss 4.964893341064453 val loss 4.964776515960693\n",
      "Epoch 1859: train loss 4.964776515960693 val loss 4.964718341827393\n",
      "Epoch 1860: train loss 4.964718341827393 val loss 4.964714527130127\n",
      "Epoch 1861: train loss 4.964714527130127 val loss 4.964713096618652\n",
      "Epoch 1862: train loss 4.964713096618652 val loss 4.96470832824707\n",
      "Epoch 1863: train loss 4.96470832824707 val loss 4.964719295501709\n",
      "Epoch 1864: train loss 4.964719295501709 val loss 4.965281009674072\n",
      "Epoch 1865: train loss 4.965281009674072 val loss 4.964741230010986\n",
      "Epoch 1866: train loss 4.964741230010986 val loss 4.965641975402832\n",
      "Epoch 1867: train loss 4.965641975402832 val loss 4.964726448059082\n",
      "Epoch 1868: train loss 4.964726448059082 val loss 4.965091228485107\n",
      "Epoch 1869: train loss 4.965091228485107 val loss 4.9650421142578125\n",
      "Epoch 1870: train loss 4.9650421142578125 val loss 4.964807033538818\n",
      "Epoch 1871: train loss 4.964807033538818 val loss 4.965498924255371\n",
      "Epoch 1872: train loss 4.965498924255371 val loss 4.965025901794434\n",
      "Epoch 1873: train loss 4.965025901794434 val loss 4.965231418609619\n",
      "Epoch 1874: train loss 4.965231418609619 val loss 4.964888095855713\n",
      "Epoch 1875: train loss 4.964888095855713 val loss 4.964799880981445\n",
      "Epoch 1876: train loss 4.964799880981445 val loss 4.965972423553467\n",
      "Epoch 1877: train loss 4.965972423553467 val loss 4.965211868286133\n",
      "Epoch 1878: train loss 4.965211868286133 val loss 4.966175079345703\n",
      "Epoch 1879: train loss 4.966175079345703 val loss 4.9651336669921875\n",
      "Epoch 1880: train loss 4.9651336669921875 val loss 4.9660563468933105\n",
      "Epoch 1881: train loss 4.9660563468933105 val loss 4.964689254760742\n",
      "Epoch 1882: train loss 4.964689254760742 val loss 4.964798927307129\n",
      "Epoch 1883: train loss 4.964798927307129 val loss 4.965073585510254\n",
      "Epoch 1884: train loss 4.965073585510254 val loss 4.965010643005371\n",
      "Epoch 1885: train loss 4.965010643005371 val loss 4.965010643005371\n",
      "Epoch 1886: train loss 4.965010643005371 val loss 4.964881896972656\n",
      "Epoch 1887: train loss 4.964881896972656 val loss 4.96510648727417\n",
      "Epoch 1888: train loss 4.96510648727417 val loss 4.964869976043701\n",
      "Epoch 1889: train loss 4.964869976043701 val loss 4.966000080108643\n",
      "Epoch 1890: train loss 4.966000080108643 val loss 4.964803695678711\n",
      "Epoch 1891: train loss 4.964803695678711 val loss 4.966006755828857\n",
      "Epoch 1892: train loss 4.966006755828857 val loss 4.965663909912109\n",
      "Epoch 1893: train loss 4.965663909912109 val loss 4.965217590332031\n",
      "Epoch 1894: train loss 4.965217590332031 val loss 4.9648590087890625\n",
      "Epoch 1895: train loss 4.9648590087890625 val loss 4.964748859405518\n",
      "Epoch 1896: train loss 4.964748859405518 val loss 4.965057373046875\n",
      "Epoch 1897: train loss 4.965057373046875 val loss 4.964615345001221\n",
      "Epoch 1898: train loss 4.964615345001221 val loss 4.965673446655273\n",
      "Epoch 1899: train loss 4.965673446655273 val loss 4.964817047119141\n",
      "Epoch 1900: train loss 4.964817047119141 val loss 4.965752601623535\n",
      "Epoch 1901: train loss 4.965752601623535 val loss 4.964801788330078\n",
      "Epoch 1902: train loss 4.964801788330078 val loss 4.96540641784668\n",
      "Epoch 1903: train loss 4.96540641784668 val loss 4.964562892913818\n",
      "Epoch 1904: train loss 4.964562892913818 val loss 4.965324878692627\n",
      "Epoch 1905: train loss 4.965324878692627 val loss 4.96466588973999\n",
      "Epoch 1906: train loss 4.96466588973999 val loss 4.965566635131836\n",
      "Epoch 1907: train loss 4.965566635131836 val loss 4.964670181274414\n",
      "Epoch 1908: train loss 4.964670181274414 val loss 4.965414047241211\n",
      "Epoch 1909: train loss 4.965414047241211 val loss 4.964534759521484\n",
      "Epoch 1910: train loss 4.964534759521484 val loss 4.965888500213623\n",
      "Epoch 1911: train loss 4.965888500213623 val loss 4.964592456817627\n",
      "Epoch 1912: train loss 4.964592456817627 val loss 4.965226650238037\n",
      "Epoch 1913: train loss 4.965226650238037 val loss 4.965044021606445\n",
      "Epoch 1914: train loss 4.965044021606445 val loss 4.964845180511475\n",
      "Epoch 1915: train loss 4.964845180511475 val loss 4.964761734008789\n",
      "Epoch 1916: train loss 4.964761734008789 val loss 4.96528434753418\n",
      "Epoch 1917: train loss 4.96528434753418 val loss 4.9650115966796875\n",
      "Epoch 1918: train loss 4.9650115966796875 val loss 4.964949607849121\n",
      "Epoch 1919: train loss 4.964949607849121 val loss 4.965183734893799\n",
      "Epoch 1920: train loss 4.965183734893799 val loss 4.964602470397949\n",
      "Epoch 1921: train loss 4.964602470397949 val loss 4.9650726318359375\n",
      "Epoch 1922: train loss 4.9650726318359375 val loss 4.964648246765137\n",
      "Epoch 1923: train loss 4.964648246765137 val loss 4.965282440185547\n",
      "Epoch 1924: train loss 4.965282440185547 val loss 4.96473503112793\n",
      "Epoch 1925: train loss 4.96473503112793 val loss 4.964613914489746\n",
      "Epoch 1926: train loss 4.964613914489746 val loss 4.964787483215332\n",
      "Epoch 1927: train loss 4.964787483215332 val loss 4.964738368988037\n",
      "Epoch 1928: train loss 4.964738368988037 val loss 4.964944362640381\n",
      "Epoch 1929: train loss 4.964944362640381 val loss 4.964814186096191\n",
      "Epoch 1930: train loss 4.964814186096191 val loss 4.9646501541137695\n",
      "Epoch 1931: train loss 4.9646501541137695 val loss 4.964685916900635\n",
      "Epoch 1932: train loss 4.964685916900635 val loss 4.964632034301758\n",
      "Epoch 1933: train loss 4.964632034301758 val loss 4.964879989624023\n",
      "Epoch 1934: train loss 4.964879989624023 val loss 4.964627265930176\n",
      "Epoch 1935: train loss 4.964627265930176 val loss 4.96435546875\n",
      "Epoch 1936: train loss 4.96435546875 val loss 4.964813232421875\n",
      "Epoch 1937: train loss 4.964813232421875 val loss 4.965127944946289\n",
      "Epoch 1938: train loss 4.965127944946289 val loss 4.965012073516846\n",
      "Epoch 1939: train loss 4.965012073516846 val loss 4.965442657470703\n",
      "Epoch 1940: train loss 4.965442657470703 val loss 4.964630126953125\n",
      "Epoch 1941: train loss 4.964630126953125 val loss 4.96462345123291\n",
      "Epoch 1942: train loss 4.96462345123291 val loss 4.964812278747559\n",
      "Epoch 1943: train loss 4.964812278747559 val loss 4.964684963226318\n",
      "Epoch 1944: train loss 4.964684963226318 val loss 4.965224266052246\n",
      "Epoch 1945: train loss 4.965224266052246 val loss 4.964765548706055\n",
      "Epoch 1946: train loss 4.964765548706055 val loss 4.966696739196777\n",
      "Epoch 1947: train loss 4.966696739196777 val loss 4.9646525382995605\n",
      "Epoch 1948: train loss 4.9646525382995605 val loss 4.967185974121094\n",
      "Epoch 1949: train loss 4.967185974121094 val loss 4.965984344482422\n",
      "Epoch 1950: train loss 4.965984344482422 val loss 4.965452194213867\n",
      "Epoch 1951: train loss 4.965452194213867 val loss 4.96531867980957\n",
      "Epoch 1952: train loss 4.96531867980957 val loss 4.96609354019165\n",
      "Epoch 1953: train loss 4.96609354019165 val loss 4.966214179992676\n",
      "Epoch 1954: train loss 4.966214179992676 val loss 4.964770317077637\n",
      "Epoch 1955: train loss 4.964770317077637 val loss 4.967883110046387\n",
      "Epoch 1956: train loss 4.967883110046387 val loss 4.9647698402404785\n",
      "Epoch 1957: train loss 4.9647698402404785 val loss 4.966814994812012\n",
      "Epoch 1958: train loss 4.966814994812012 val loss 4.966864585876465\n",
      "Epoch 1959: train loss 4.966864585876465 val loss 4.964775085449219\n",
      "Epoch 1960: train loss 4.964775085449219 val loss 4.968336582183838\n",
      "Epoch 1961: train loss 4.968336582183838 val loss 4.965457916259766\n",
      "Epoch 1962: train loss 4.965457916259766 val loss 4.967137336730957\n",
      "Epoch 1963: train loss 4.967137336730957 val loss 4.968140125274658\n",
      "Epoch 1964: train loss 4.968140125274658 val loss 4.965193271636963\n",
      "Epoch 1965: train loss 4.965193271636963 val loss 4.967218399047852\n",
      "Epoch 1966: train loss 4.967218399047852 val loss 4.96591854095459\n",
      "Epoch 1967: train loss 4.96591854095459 val loss 4.966551303863525\n",
      "Epoch 1968: train loss 4.966551303863525 val loss 4.968034744262695\n",
      "Epoch 1969: train loss 4.968034744262695 val loss 4.965358734130859\n",
      "Epoch 1970: train loss 4.965358734130859 val loss 4.966978073120117\n",
      "Epoch 1971: train loss 4.966978073120117 val loss 4.966062068939209\n",
      "Epoch 1972: train loss 4.966062068939209 val loss 4.966155052185059\n",
      "Epoch 1973: train loss 4.966155052185059 val loss 4.966976642608643\n",
      "Epoch 1974: train loss 4.966976642608643 val loss 4.965051651000977\n",
      "Epoch 1975: train loss 4.965051651000977 val loss 4.967164516448975\n",
      "Epoch 1976: train loss 4.967164516448975 val loss 4.9652814865112305\n",
      "Epoch 1977: train loss 4.9652814865112305 val loss 4.965404510498047\n",
      "Epoch 1978: train loss 4.965404510498047 val loss 4.965939521789551\n",
      "Epoch 1979: train loss 4.965939521789551 val loss 4.964997291564941\n",
      "Epoch 1980: train loss 4.964997291564941 val loss 4.966985702514648\n",
      "Epoch 1981: train loss 4.966985702514648 val loss 4.9648847579956055\n",
      "Epoch 1982: train loss 4.9648847579956055 val loss 4.9661736488342285\n",
      "Epoch 1983: train loss 4.9661736488342285 val loss 4.965785980224609\n",
      "Epoch 1984: train loss 4.965785980224609 val loss 4.965237617492676\n",
      "Epoch 1985: train loss 4.965237617492676 val loss 4.964938640594482\n",
      "Epoch 1986: train loss 4.964938640594482 val loss 4.964937210083008\n",
      "Epoch 1987: train loss 4.964937210083008 val loss 4.9647393226623535\n",
      "Epoch 1988: train loss 4.9647393226623535 val loss 4.965231418609619\n",
      "Epoch 1989: train loss 4.965231418609619 val loss 4.964774131774902\n",
      "Epoch 1990: train loss 4.964774131774902 val loss 4.964853763580322\n",
      "Epoch 1991: train loss 4.964853763580322 val loss 4.9649658203125\n",
      "Epoch 1992: train loss 4.9649658203125 val loss 4.964908599853516\n",
      "Epoch 1993: train loss 4.964908599853516 val loss 4.964683532714844\n",
      "Epoch 1994: train loss 4.964683532714844 val loss 4.964389324188232\n",
      "Epoch 1995: train loss 4.964389324188232 val loss 4.9645915031433105\n",
      "Epoch 1996: train loss 4.9645915031433105 val loss 4.965026378631592\n",
      "Epoch 1997: train loss 4.965026378631592 val loss 4.964822769165039\n",
      "Epoch 1998: train loss 4.964822769165039 val loss 4.964649677276611\n",
      "Epoch 1999: train loss 4.964649677276611 val loss 4.964716911315918\n",
      "Epoch 2000: train loss 4.964716911315918 val loss 4.964564323425293\n",
      "Epoch 2001: train loss 4.964564323425293 val loss 4.964674472808838\n",
      "Epoch 2002: train loss 4.964674472808838 val loss 4.964625358581543\n",
      "Epoch 2003: train loss 4.964625358581543 val loss 4.964544296264648\n",
      "Epoch 2004: train loss 4.964544296264648 val loss 4.9644775390625\n",
      "Epoch 2005: train loss 4.9644775390625 val loss 4.964706897735596\n",
      "Epoch 2006: train loss 4.964706897735596 val loss 4.9645795822143555\n",
      "Epoch 2007: train loss 4.9645795822143555 val loss 4.964385032653809\n",
      "Epoch 2008: train loss 4.964385032653809 val loss 4.964609622955322\n",
      "Epoch 2009: train loss 4.964609622955322 val loss 4.964400768280029\n",
      "Epoch 2010: train loss 4.964400768280029 val loss 4.964478015899658\n",
      "Epoch 2011: train loss 4.964478015899658 val loss 4.965466499328613\n",
      "Epoch 2012: train loss 4.965466499328613 val loss 4.965171813964844\n",
      "Epoch 2013: train loss 4.965171813964844 val loss 4.965540885925293\n",
      "Epoch 2014: train loss 4.965540885925293 val loss 4.9646453857421875\n",
      "Epoch 2015: train loss 4.9646453857421875 val loss 4.966474533081055\n",
      "Epoch 2016: train loss 4.966474533081055 val loss 4.9660844802856445\n",
      "Epoch 2017: train loss 4.9660844802856445 val loss 4.965152740478516\n",
      "Epoch 2018: train loss 4.965152740478516 val loss 4.965655326843262\n",
      "Epoch 2019: train loss 4.965655326843262 val loss 4.965101718902588\n",
      "Epoch 2020: train loss 4.965101718902588 val loss 4.966335296630859\n",
      "Epoch 2021: train loss 4.966335296630859 val loss 4.965013027191162\n",
      "Epoch 2022: train loss 4.965013027191162 val loss 4.965247631072998\n",
      "Epoch 2023: train loss 4.965247631072998 val loss 4.9647979736328125\n",
      "Epoch 2024: train loss 4.9647979736328125 val loss 4.965263843536377\n",
      "Epoch 2025: train loss 4.965263843536377 val loss 4.964817047119141\n",
      "Epoch 2026: train loss 4.964817047119141 val loss 4.964546203613281\n",
      "Epoch 2027: train loss 4.964546203613281 val loss 4.964545726776123\n",
      "Epoch 2028: train loss 4.964545726776123 val loss 4.964556694030762\n",
      "Epoch 2029: train loss 4.964556694030762 val loss 4.964612007141113\n",
      "Epoch 2030: train loss 4.964612007141113 val loss 4.964503765106201\n",
      "Epoch 2031: train loss 4.964503765106201 val loss 4.965090751647949\n",
      "Epoch 2032: train loss 4.965090751647949 val loss 4.965041637420654\n",
      "Epoch 2033: train loss 4.965041637420654 val loss 4.964850902557373\n",
      "Epoch 2034: train loss 4.964850902557373 val loss 4.965322971343994\n",
      "Epoch 2035: train loss 4.965322971343994 val loss 4.964776992797852\n",
      "Epoch 2036: train loss 4.964776992797852 val loss 4.965065956115723\n",
      "Epoch 2037: train loss 4.965065956115723 val loss 4.964745044708252\n",
      "Epoch 2038: train loss 4.964745044708252 val loss 4.964658260345459\n",
      "Epoch 2039: train loss 4.964658260345459 val loss 4.964705467224121\n",
      "Epoch 2040: train loss 4.964705467224121 val loss 4.964714050292969\n",
      "Epoch 2041: train loss 4.964714050292969 val loss 4.96523904800415\n",
      "Epoch 2042: train loss 4.96523904800415 val loss 4.964674472808838\n",
      "Epoch 2043: train loss 4.964674472808838 val loss 4.964669227600098\n",
      "Epoch 2044: train loss 4.964669227600098 val loss 4.964507102966309\n",
      "Epoch 2045: train loss 4.964507102966309 val loss 4.9648590087890625\n",
      "Epoch 2046: train loss 4.9648590087890625 val loss 4.96469783782959\n",
      "Epoch 2047: train loss 4.96469783782959 val loss 4.965106010437012\n",
      "Epoch 2048: train loss 4.965106010437012 val loss 4.96476936340332\n",
      "Epoch 2049: train loss 4.96476936340332 val loss 4.965552806854248\n",
      "Epoch 2050: train loss 4.965552806854248 val loss 4.964649200439453\n",
      "Epoch 2051: train loss 4.964649200439453 val loss 4.965588569641113\n",
      "Epoch 2052: train loss 4.965588569641113 val loss 4.964690208435059\n",
      "Epoch 2053: train loss 4.964690208435059 val loss 4.965700149536133\n",
      "Epoch 2054: train loss 4.965700149536133 val loss 4.964571475982666\n",
      "Epoch 2055: train loss 4.964571475982666 val loss 4.9651384353637695\n",
      "Epoch 2056: train loss 4.9651384353637695 val loss 4.964437484741211\n",
      "Epoch 2057: train loss 4.964437484741211 val loss 4.964887619018555\n",
      "Epoch 2058: train loss 4.964887619018555 val loss 4.964968681335449\n",
      "Epoch 2059: train loss 4.964968681335449 val loss 4.964935779571533\n",
      "Epoch 2060: train loss 4.964935779571533 val loss 4.964785575866699\n",
      "Epoch 2061: train loss 4.964785575866699 val loss 4.964473247528076\n",
      "Epoch 2062: train loss 4.964473247528076 val loss 4.964523792266846\n",
      "Epoch 2063: train loss 4.964523792266846 val loss 4.9644317626953125\n",
      "Epoch 2064: train loss 4.9644317626953125 val loss 4.964458465576172\n",
      "Epoch 2065: train loss 4.964458465576172 val loss 4.965052127838135\n",
      "Epoch 2066: train loss 4.965052127838135 val loss 4.964390754699707\n",
      "Epoch 2067: train loss 4.964390754699707 val loss 4.965564727783203\n",
      "Epoch 2068: train loss 4.965564727783203 val loss 4.964713096618652\n",
      "Epoch 2069: train loss 4.964713096618652 val loss 4.965517044067383\n",
      "Epoch 2070: train loss 4.965517044067383 val loss 4.9648613929748535\n",
      "Epoch 2071: train loss 4.9648613929748535 val loss 4.965449810028076\n",
      "Epoch 2072: train loss 4.965449810028076 val loss 4.9646782875061035\n",
      "Epoch 2073: train loss 4.9646782875061035 val loss 4.9645915031433105\n",
      "Epoch 2074: train loss 4.9645915031433105 val loss 4.9652814865112305\n",
      "Epoch 2075: train loss 4.9652814865112305 val loss 4.964678764343262\n",
      "Epoch 2076: train loss 4.964678764343262 val loss 4.965275764465332\n",
      "Epoch 2077: train loss 4.965275764465332 val loss 4.964425086975098\n",
      "Epoch 2078: train loss 4.964425086975098 val loss 4.964454174041748\n",
      "Epoch 2079: train loss 4.964454174041748 val loss 4.9644694328308105\n",
      "Epoch 2080: train loss 4.9644694328308105 val loss 4.964508056640625\n",
      "Epoch 2081: train loss 4.964508056640625 val loss 4.964421272277832\n",
      "Epoch 2082: train loss 4.964421272277832 val loss 4.964566707611084\n",
      "Epoch 2083: train loss 4.964566707611084 val loss 4.964757442474365\n",
      "Epoch 2084: train loss 4.964757442474365 val loss 4.964631080627441\n",
      "Epoch 2085: train loss 4.964631080627441 val loss 4.964461326599121\n",
      "Epoch 2086: train loss 4.964461326599121 val loss 4.965051174163818\n",
      "Epoch 2087: train loss 4.965051174163818 val loss 4.964491367340088\n",
      "Epoch 2088: train loss 4.964491367340088 val loss 4.964658737182617\n",
      "Epoch 2089: train loss 4.964658737182617 val loss 4.964608192443848\n",
      "Epoch 2090: train loss 4.964608192443848 val loss 4.965438365936279\n",
      "Epoch 2091: train loss 4.965438365936279 val loss 4.964364051818848\n",
      "Epoch 2092: train loss 4.964364051818848 val loss 4.965548515319824\n",
      "Epoch 2093: train loss 4.965548515319824 val loss 4.9647698402404785\n",
      "Epoch 2094: train loss 4.9647698402404785 val loss 4.96526575088501\n",
      "Epoch 2095: train loss 4.96526575088501 val loss 4.964654445648193\n",
      "Epoch 2096: train loss 4.964654445648193 val loss 4.964784145355225\n",
      "Epoch 2097: train loss 4.964784145355225 val loss 4.964679718017578\n",
      "Epoch 2098: train loss 4.964679718017578 val loss 4.964862823486328\n",
      "Epoch 2099: train loss 4.964862823486328 val loss 4.964565277099609\n",
      "Epoch 2100: train loss 4.964565277099609 val loss 4.964545249938965\n",
      "Epoch 2101: train loss 4.964545249938965 val loss 4.9644575119018555\n",
      "Epoch 2102: train loss 4.9644575119018555 val loss 4.96455717086792\n",
      "Epoch 2103: train loss 4.96455717086792 val loss 4.964852333068848\n",
      "Epoch 2104: train loss 4.964852333068848 val loss 4.964550971984863\n",
      "Epoch 2105: train loss 4.964550971984863 val loss 4.965691566467285\n",
      "Epoch 2106: train loss 4.965691566467285 val loss 4.964230537414551\n",
      "Epoch 2107: train loss 4.964230537414551 val loss 4.964602470397949\n",
      "Epoch 2108: train loss 4.964602470397949 val loss 4.9644293785095215\n",
      "Epoch 2109: train loss 4.9644293785095215 val loss 4.964750289916992\n",
      "Epoch 2110: train loss 4.964750289916992 val loss 4.964982509613037\n",
      "Epoch 2111: train loss 4.964982509613037 val loss 4.964983940124512\n",
      "Epoch 2112: train loss 4.964983940124512 val loss 4.9649810791015625\n",
      "Epoch 2113: train loss 4.9649810791015625 val loss 4.964548587799072\n",
      "Epoch 2114: train loss 4.964548587799072 val loss 4.9646525382995605\n",
      "Epoch 2115: train loss 4.9646525382995605 val loss 4.964426040649414\n",
      "Epoch 2116: train loss 4.964426040649414 val loss 4.964397430419922\n",
      "Epoch 2117: train loss 4.964397430419922 val loss 4.964925765991211\n",
      "Epoch 2118: train loss 4.964925765991211 val loss 4.9651994705200195\n",
      "Epoch 2119: train loss 4.9651994705200195 val loss 4.964755058288574\n",
      "Epoch 2120: train loss 4.964755058288574 val loss 4.965166091918945\n",
      "Epoch 2121: train loss 4.965166091918945 val loss 4.964427947998047\n",
      "Epoch 2122: train loss 4.964427947998047 val loss 4.965043067932129\n",
      "Epoch 2123: train loss 4.965043067932129 val loss 4.964460372924805\n",
      "Epoch 2124: train loss 4.964460372924805 val loss 4.9645795822143555\n",
      "Epoch 2125: train loss 4.9645795822143555 val loss 4.96479606628418\n",
      "Epoch 2126: train loss 4.96479606628418 val loss 4.965066432952881\n",
      "Epoch 2127: train loss 4.965066432952881 val loss 4.964630126953125\n",
      "Epoch 2128: train loss 4.964630126953125 val loss 4.965172290802002\n",
      "Epoch 2129: train loss 4.965172290802002 val loss 4.964397430419922\n",
      "Epoch 2130: train loss 4.964397430419922 val loss 4.965073585510254\n",
      "Epoch 2131: train loss 4.965073585510254 val loss 4.964502334594727\n",
      "Epoch 2132: train loss 4.964502334594727 val loss 4.9648613929748535\n",
      "Epoch 2133: train loss 4.9648613929748535 val loss 4.9646220207214355\n",
      "Epoch 2134: train loss 4.9646220207214355 val loss 4.965025424957275\n",
      "Epoch 2135: train loss 4.965025424957275 val loss 4.964662551879883\n",
      "Epoch 2136: train loss 4.964662551879883 val loss 4.964437484741211\n",
      "Epoch 2137: train loss 4.964437484741211 val loss 4.964463710784912\n",
      "Epoch 2138: train loss 4.964463710784912 val loss 4.9643683433532715\n",
      "Epoch 2139: train loss 4.9643683433532715 val loss 4.964351654052734\n",
      "Epoch 2140: train loss 4.964351654052734 val loss 4.964328289031982\n",
      "Epoch 2141: train loss 4.964328289031982 val loss 4.9650726318359375\n",
      "Epoch 2142: train loss 4.9650726318359375 val loss 4.9643073081970215\n",
      "Epoch 2143: train loss 4.9643073081970215 val loss 4.9665069580078125\n",
      "Epoch 2144: train loss 4.9665069580078125 val loss 4.964521408081055\n",
      "Epoch 2145: train loss 4.964521408081055 val loss 4.966856002807617\n",
      "Epoch 2146: train loss 4.966856002807617 val loss 4.966780185699463\n",
      "Epoch 2147: train loss 4.966780185699463 val loss 4.964542388916016\n",
      "Epoch 2148: train loss 4.964542388916016 val loss 4.967642307281494\n",
      "Epoch 2149: train loss 4.967642307281494 val loss 4.964805603027344\n",
      "Epoch 2150: train loss 4.964805603027344 val loss 4.968573570251465\n",
      "Epoch 2151: train loss 4.968573570251465 val loss 4.9703369140625\n",
      "Epoch 2152: train loss 4.9703369140625 val loss 4.967811584472656\n",
      "Epoch 2153: train loss 4.967811584472656 val loss 4.965412139892578\n",
      "Epoch 2154: train loss 4.965412139892578 val loss 4.967095375061035\n",
      "Epoch 2155: train loss 4.967095375061035 val loss 4.964415550231934\n",
      "Epoch 2156: train loss 4.964415550231934 val loss 4.96607780456543\n",
      "Epoch 2157: train loss 4.96607780456543 val loss 4.965423583984375\n",
      "Epoch 2158: train loss 4.965423583984375 val loss 4.96562385559082\n",
      "Epoch 2159: train loss 4.96562385559082 val loss 4.965158462524414\n",
      "Epoch 2160: train loss 4.965158462524414 val loss 4.966033935546875\n",
      "Epoch 2161: train loss 4.966033935546875 val loss 4.96597146987915\n",
      "Epoch 2162: train loss 4.96597146987915 val loss 4.964783191680908\n",
      "Epoch 2163: train loss 4.964783191680908 val loss 4.965137004852295\n",
      "Epoch 2164: train loss 4.965137004852295 val loss 4.965314865112305\n",
      "Epoch 2165: train loss 4.965314865112305 val loss 4.96595573425293\n",
      "Epoch 2166: train loss 4.96595573425293 val loss 4.964540958404541\n",
      "Epoch 2167: train loss 4.964540958404541 val loss 4.966989517211914\n",
      "Epoch 2168: train loss 4.966989517211914 val loss 4.964474201202393\n",
      "Epoch 2169: train loss 4.964474201202393 val loss 4.965551376342773\n",
      "Epoch 2170: train loss 4.965551376342773 val loss 4.964670658111572\n",
      "Epoch 2171: train loss 4.964670658111572 val loss 4.965553283691406\n",
      "Epoch 2172: train loss 4.965553283691406 val loss 4.964471817016602\n",
      "Epoch 2173: train loss 4.964471817016602 val loss 4.965422630310059\n",
      "Epoch 2174: train loss 4.965422630310059 val loss 4.964615345001221\n",
      "Epoch 2175: train loss 4.964615345001221 val loss 4.965124130249023\n",
      "Epoch 2176: train loss 4.965124130249023 val loss 4.964382648468018\n",
      "Epoch 2177: train loss 4.964382648468018 val loss 4.965452671051025\n",
      "Epoch 2178: train loss 4.965452671051025 val loss 4.964493751525879\n",
      "Epoch 2179: train loss 4.964493751525879 val loss 4.966087818145752\n",
      "Epoch 2180: train loss 4.966087818145752 val loss 4.9645185470581055\n",
      "Epoch 2181: train loss 4.9645185470581055 val loss 4.966259956359863\n",
      "Epoch 2182: train loss 4.966259956359863 val loss 4.965620994567871\n",
      "Epoch 2183: train loss 4.965620994567871 val loss 4.9649176597595215\n",
      "Epoch 2184: train loss 4.9649176597595215 val loss 4.96580171585083\n",
      "Epoch 2185: train loss 4.96580171585083 val loss 4.9650397300720215\n",
      "Epoch 2186: train loss 4.9650397300720215 val loss 4.966804504394531\n",
      "Epoch 2187: train loss 4.966804504394531 val loss 4.964778423309326\n",
      "Epoch 2188: train loss 4.964778423309326 val loss 4.966128349304199\n",
      "Epoch 2189: train loss 4.966128349304199 val loss 4.96446418762207\n",
      "Epoch 2190: train loss 4.96446418762207 val loss 4.964905738830566\n",
      "Epoch 2191: train loss 4.964905738830566 val loss 4.964695453643799\n",
      "Epoch 2192: train loss 4.964695453643799 val loss 4.964822769165039\n",
      "Epoch 2193: train loss 4.964822769165039 val loss 4.964583396911621\n",
      "Epoch 2194: train loss 4.964583396911621 val loss 4.9645915031433105\n",
      "Epoch 2195: train loss 4.9645915031433105 val loss 4.964522838592529\n",
      "Epoch 2196: train loss 4.964522838592529 val loss 4.965081691741943\n",
      "Epoch 2197: train loss 4.965081691741943 val loss 4.964619159698486\n",
      "Epoch 2198: train loss 4.964619159698486 val loss 4.964958190917969\n",
      "Epoch 2199: train loss 4.964958190917969 val loss 4.964320182800293\n",
      "Epoch 2200: train loss 4.964320182800293 val loss 4.965652942657471\n",
      "Epoch 2201: train loss 4.965652942657471 val loss 4.96450138092041\n",
      "Epoch 2202: train loss 4.96450138092041 val loss 4.965290069580078\n",
      "Epoch 2203: train loss 4.965290069580078 val loss 4.964547157287598\n",
      "Epoch 2204: train loss 4.964547157287598 val loss 4.965715408325195\n",
      "Epoch 2205: train loss 4.965715408325195 val loss 4.964407920837402\n",
      "Epoch 2206: train loss 4.964407920837402 val loss 4.965909481048584\n",
      "Epoch 2207: train loss 4.965909481048584 val loss 4.965281963348389\n",
      "Epoch 2208: train loss 4.965281963348389 val loss 4.965448379516602\n",
      "Epoch 2209: train loss 4.965448379516602 val loss 4.965508460998535\n",
      "Epoch 2210: train loss 4.965508460998535 val loss 4.9654541015625\n",
      "Epoch 2211: train loss 4.9654541015625 val loss 4.966335296630859\n",
      "Epoch 2212: train loss 4.966335296630859 val loss 4.9646992683410645\n",
      "Epoch 2213: train loss 4.9646992683410645 val loss 4.965788841247559\n",
      "Epoch 2214: train loss 4.965788841247559 val loss 4.964296340942383\n",
      "Epoch 2215: train loss 4.964296340942383 val loss 4.964886665344238\n",
      "Epoch 2216: train loss 4.964886665344238 val loss 4.96461296081543\n",
      "Epoch 2217: train loss 4.96461296081543 val loss 4.9648613929748535\n",
      "Epoch 2218: train loss 4.9648613929748535 val loss 4.9644856452941895\n",
      "Epoch 2219: train loss 4.9644856452941895 val loss 4.964293003082275\n",
      "Epoch 2220: train loss 4.964293003082275 val loss 4.964468479156494\n",
      "Epoch 2221: train loss 4.964468479156494 val loss 4.964591026306152\n",
      "Epoch 2222: train loss 4.964591026306152 val loss 4.9647417068481445\n",
      "Epoch 2223: train loss 4.9647417068481445 val loss 4.9642415046691895\n",
      "Epoch 2224: train loss 4.9642415046691895 val loss 4.964242935180664\n",
      "Epoch 2225: train loss 4.964242935180664 val loss 4.965078353881836\n",
      "Epoch 2226: train loss 4.965078353881836 val loss 4.96462345123291\n",
      "Epoch 2227: train loss 4.96462345123291 val loss 4.965459823608398\n",
      "Epoch 2228: train loss 4.965459823608398 val loss 4.964302062988281\n",
      "Epoch 2229: train loss 4.964302062988281 val loss 4.965810298919678\n",
      "Epoch 2230: train loss 4.965810298919678 val loss 4.965498924255371\n",
      "Epoch 2231: train loss 4.965498924255371 val loss 4.964785575866699\n",
      "Epoch 2232: train loss 4.964785575866699 val loss 4.9670796394348145\n",
      "Epoch 2233: train loss 4.9670796394348145 val loss 4.964679718017578\n",
      "Epoch 2234: train loss 4.964679718017578 val loss 4.966620445251465\n",
      "Epoch 2235: train loss 4.966620445251465 val loss 4.965799331665039\n",
      "Epoch 2236: train loss 4.965799331665039 val loss 4.965039253234863\n",
      "Epoch 2237: train loss 4.965039253234863 val loss 4.96441125869751\n",
      "Epoch 2238: train loss 4.96441125869751 val loss 4.965857982635498\n",
      "Epoch 2239: train loss 4.965857982635498 val loss 4.965705394744873\n",
      "Epoch 2240: train loss 4.965705394744873 val loss 4.964709758758545\n",
      "Epoch 2241: train loss 4.964709758758545 val loss 4.96458101272583\n",
      "Epoch 2242: train loss 4.96458101272583 val loss 4.9645094871521\n",
      "Epoch 2243: train loss 4.9645094871521 val loss 4.964405059814453\n",
      "Epoch 2244: train loss 4.964405059814453 val loss 4.964382171630859\n",
      "Epoch 2245: train loss 4.964382171630859 val loss 4.964329242706299\n",
      "Epoch 2246: train loss 4.964329242706299 val loss 4.96434211730957\n",
      "Epoch 2247: train loss 4.96434211730957 val loss 4.964374542236328\n",
      "Epoch 2248: train loss 4.964374542236328 val loss 4.965012073516846\n",
      "Epoch 2249: train loss 4.965012073516846 val loss 4.964652061462402\n",
      "Epoch 2250: train loss 4.964652061462402 val loss 4.965632438659668\n",
      "Epoch 2251: train loss 4.965632438659668 val loss 4.9644775390625\n",
      "Epoch 2252: train loss 4.9644775390625 val loss 4.966083526611328\n",
      "Epoch 2253: train loss 4.966083526611328 val loss 4.965526580810547\n",
      "Epoch 2254: train loss 4.965526580810547 val loss 4.965086936950684\n",
      "Epoch 2255: train loss 4.965086936950684 val loss 4.965292930603027\n",
      "Epoch 2256: train loss 4.965292930603027 val loss 4.9651641845703125\n",
      "Epoch 2257: train loss 4.9651641845703125 val loss 4.965738296508789\n",
      "Epoch 2258: train loss 4.965738296508789 val loss 4.964605808258057\n",
      "Epoch 2259: train loss 4.964605808258057 val loss 4.966951370239258\n",
      "Epoch 2260: train loss 4.966951370239258 val loss 4.964296340942383\n",
      "Epoch 2261: train loss 4.964296340942383 val loss 4.965427398681641\n",
      "Epoch 2262: train loss 4.965427398681641 val loss 4.964539527893066\n",
      "Epoch 2263: train loss 4.964539527893066 val loss 4.9655632972717285\n",
      "Epoch 2264: train loss 4.9655632972717285 val loss 4.964398384094238\n",
      "Epoch 2265: train loss 4.964398384094238 val loss 4.964916229248047\n",
      "Epoch 2266: train loss 4.964916229248047 val loss 4.964292526245117\n",
      "Epoch 2267: train loss 4.964292526245117 val loss 4.96455717086792\n",
      "Epoch 2268: train loss 4.96455717086792 val loss 4.964679718017578\n",
      "Epoch 2269: train loss 4.964679718017578 val loss 4.9644012451171875\n",
      "Epoch 2270: train loss 4.9644012451171875 val loss 4.965057373046875\n",
      "Epoch 2271: train loss 4.965057373046875 val loss 4.964537620544434\n",
      "Epoch 2272: train loss 4.964537620544434 val loss 4.964907646179199\n",
      "Epoch 2273: train loss 4.964907646179199 val loss 4.964344024658203\n",
      "Epoch 2274: train loss 4.964344024658203 val loss 4.964719295501709\n",
      "Epoch 2275: train loss 4.964719295501709 val loss 4.964599132537842\n",
      "Epoch 2276: train loss 4.964599132537842 val loss 4.964418411254883\n",
      "Epoch 2277: train loss 4.964418411254883 val loss 4.9646501541137695\n",
      "Epoch 2278: train loss 4.9646501541137695 val loss 4.964366912841797\n",
      "Epoch 2279: train loss 4.964366912841797 val loss 4.964398384094238\n",
      "Epoch 2280: train loss 4.964398384094238 val loss 4.964487552642822\n",
      "Epoch 2281: train loss 4.964487552642822 val loss 4.9653639793396\n",
      "Epoch 2282: train loss 4.9653639793396 val loss 4.965028762817383\n",
      "Epoch 2283: train loss 4.965028762817383 val loss 4.965107440948486\n",
      "Epoch 2284: train loss 4.965107440948486 val loss 4.964698314666748\n",
      "Epoch 2285: train loss 4.964698314666748 val loss 4.966498374938965\n",
      "Epoch 2286: train loss 4.966498374938965 val loss 4.964436054229736\n",
      "Epoch 2287: train loss 4.964436054229736 val loss 4.965638637542725\n",
      "Epoch 2288: train loss 4.965638637542725 val loss 4.964288711547852\n",
      "Epoch 2289: train loss 4.964288711547852 val loss 4.965374946594238\n",
      "Epoch 2290: train loss 4.965374946594238 val loss 4.964338302612305\n",
      "Epoch 2291: train loss 4.964338302612305 val loss 4.964908123016357\n",
      "Epoch 2292: train loss 4.964908123016357 val loss 4.964176177978516\n",
      "Epoch 2293: train loss 4.964176177978516 val loss 4.964322090148926\n",
      "Epoch 2294: train loss 4.964322090148926 val loss 4.965024948120117\n",
      "Epoch 2295: train loss 4.965024948120117 val loss 4.964502811431885\n",
      "Epoch 2296: train loss 4.964502811431885 val loss 4.964746475219727\n",
      "Epoch 2297: train loss 4.964746475219727 val loss 4.96448278427124\n",
      "Epoch 2298: train loss 4.96448278427124 val loss 4.964496612548828\n",
      "Epoch 2299: train loss 4.964496612548828 val loss 4.964376449584961\n",
      "Epoch 2300: train loss 4.964376449584961 val loss 4.9642415046691895\n",
      "Epoch 2301: train loss 4.9642415046691895 val loss 4.964513778686523\n",
      "Epoch 2302: train loss 4.964513778686523 val loss 4.964582443237305\n",
      "Epoch 2303: train loss 4.964582443237305 val loss 4.964658737182617\n",
      "Epoch 2304: train loss 4.964658737182617 val loss 4.964595317840576\n",
      "Epoch 2305: train loss 4.964595317840576 val loss 4.964282989501953\n",
      "Epoch 2306: train loss 4.964282989501953 val loss 4.964186668395996\n",
      "Epoch 2307: train loss 4.964186668395996 val loss 4.964249610900879\n",
      "Epoch 2308: train loss 4.964249610900879 val loss 4.964225769042969\n",
      "Epoch 2309: train loss 4.964225769042969 val loss 4.964144706726074\n",
      "Epoch 2310: train loss 4.964144706726074 val loss 4.9641828536987305\n",
      "Epoch 2311: train loss 4.9641828536987305 val loss 4.96518611907959\n",
      "Epoch 2312: train loss 4.96518611907959 val loss 4.964352607727051\n",
      "Epoch 2313: train loss 4.964352607727051 val loss 4.966710567474365\n",
      "Epoch 2314: train loss 4.966710567474365 val loss 4.965260982513428\n",
      "Epoch 2315: train loss 4.965260982513428 val loss 4.966616630554199\n",
      "Epoch 2316: train loss 4.966616630554199 val loss 4.967329502105713\n",
      "Epoch 2317: train loss 4.967329502105713 val loss 4.964734077453613\n",
      "Epoch 2318: train loss 4.964734077453613 val loss 4.968238353729248\n",
      "Epoch 2319: train loss 4.968238353729248 val loss 4.96723747253418\n",
      "Epoch 2320: train loss 4.96723747253418 val loss 4.965819358825684\n",
      "Epoch 2321: train loss 4.965819358825684 val loss 4.967527389526367\n",
      "Epoch 2322: train loss 4.967527389526367 val loss 4.965292930603027\n",
      "Epoch 2323: train loss 4.965292930603027 val loss 4.967594146728516\n",
      "Epoch 2324: train loss 4.967594146728516 val loss 4.967629432678223\n",
      "Epoch 2325: train loss 4.967629432678223 val loss 4.964995384216309\n",
      "Epoch 2326: train loss 4.964995384216309 val loss 4.967373847961426\n",
      "Epoch 2327: train loss 4.967373847961426 val loss 4.965910911560059\n",
      "Epoch 2328: train loss 4.965910911560059 val loss 4.965950965881348\n",
      "Epoch 2329: train loss 4.965950965881348 val loss 4.966061592102051\n",
      "Epoch 2330: train loss 4.966061592102051 val loss 4.965169906616211\n",
      "Epoch 2331: train loss 4.965169906616211 val loss 4.9659104347229\n",
      "Epoch 2332: train loss 4.9659104347229 val loss 4.964401721954346\n",
      "Epoch 2333: train loss 4.964401721954346 val loss 4.966235160827637\n",
      "Epoch 2334: train loss 4.966235160827637 val loss 4.96418571472168\n",
      "Epoch 2335: train loss 4.96418571472168 val loss 4.965778350830078\n",
      "Epoch 2336: train loss 4.965778350830078 val loss 4.965217113494873\n",
      "Epoch 2337: train loss 4.965217113494873 val loss 4.965603828430176\n",
      "Epoch 2338: train loss 4.965603828430176 val loss 4.965619087219238\n",
      "Epoch 2339: train loss 4.965619087219238 val loss 4.965121746063232\n",
      "Epoch 2340: train loss 4.965121746063232 val loss 4.965286731719971\n",
      "Epoch 2341: train loss 4.965286731719971 val loss 4.964514255523682\n",
      "Epoch 2342: train loss 4.964514255523682 val loss 4.965048313140869\n",
      "Epoch 2343: train loss 4.965048313140869 val loss 4.964950084686279\n",
      "Epoch 2344: train loss 4.964950084686279 val loss 4.965486526489258\n",
      "Epoch 2345: train loss 4.965486526489258 val loss 4.964545249938965\n",
      "Epoch 2346: train loss 4.964545249938965 val loss 4.965579986572266\n",
      "Epoch 2347: train loss 4.965579986572266 val loss 4.964593410491943\n",
      "Epoch 2348: train loss 4.964593410491943 val loss 4.9664764404296875\n",
      "Epoch 2349: train loss 4.9664764404296875 val loss 4.965224266052246\n",
      "Epoch 2350: train loss 4.965224266052246 val loss 4.966004371643066\n",
      "Epoch 2351: train loss 4.966004371643066 val loss 4.964759826660156\n",
      "Epoch 2352: train loss 4.964759826660156 val loss 4.96552848815918\n",
      "Epoch 2353: train loss 4.96552848815918 val loss 4.9651947021484375\n",
      "Epoch 2354: train loss 4.9651947021484375 val loss 4.964847087860107\n",
      "Epoch 2355: train loss 4.964847087860107 val loss 4.964849472045898\n",
      "Epoch 2356: train loss 4.964849472045898 val loss 4.964759826660156\n",
      "Epoch 2357: train loss 4.964759826660156 val loss 4.964902400970459\n",
      "Epoch 2358: train loss 4.964902400970459 val loss 4.964710235595703\n",
      "Epoch 2359: train loss 4.964710235595703 val loss 4.964922904968262\n",
      "Epoch 2360: train loss 4.964922904968262 val loss 4.964385986328125\n",
      "Epoch 2361: train loss 4.964385986328125 val loss 4.965214252471924\n",
      "Epoch 2362: train loss 4.965214252471924 val loss 4.96434211730957\n",
      "Epoch 2363: train loss 4.96434211730957 val loss 4.965775489807129\n",
      "Epoch 2364: train loss 4.965775489807129 val loss 4.9643707275390625\n",
      "Epoch 2365: train loss 4.9643707275390625 val loss 4.964835166931152\n",
      "Epoch 2366: train loss 4.964835166931152 val loss 4.96435546875\n",
      "Epoch 2367: train loss 4.96435546875 val loss 4.965531349182129\n",
      "Epoch 2368: train loss 4.965531349182129 val loss 4.964581489562988\n",
      "Epoch 2369: train loss 4.964581489562988 val loss 4.965391635894775\n",
      "Epoch 2370: train loss 4.965391635894775 val loss 4.964666843414307\n",
      "Epoch 2371: train loss 4.964666843414307 val loss 4.965449333190918\n",
      "Epoch 2372: train loss 4.965449333190918 val loss 4.964347839355469\n",
      "Epoch 2373: train loss 4.964347839355469 val loss 4.964886665344238\n",
      "Epoch 2374: train loss 4.964886665344238 val loss 4.964373588562012\n",
      "Epoch 2375: train loss 4.964373588562012 val loss 4.964940071105957\n",
      "Epoch 2376: train loss 4.964940071105957 val loss 4.964516639709473\n",
      "Epoch 2377: train loss 4.964516639709473 val loss 4.964573860168457\n",
      "Epoch 2378: train loss 4.964573860168457 val loss 4.964516639709473\n",
      "Epoch 2379: train loss 4.964516639709473 val loss 4.964195251464844\n",
      "Epoch 2380: train loss 4.964195251464844 val loss 4.964297294616699\n",
      "Epoch 2381: train loss 4.964297294616699 val loss 4.964401721954346\n",
      "Epoch 2382: train loss 4.964401721954346 val loss 4.9643707275390625\n",
      "Epoch 2383: train loss 4.9643707275390625 val loss 4.964297771453857\n",
      "Epoch 2384: train loss 4.964297771453857 val loss 4.9643659591674805\n",
      "Epoch 2385: train loss 4.9643659591674805 val loss 4.964937210083008\n",
      "Epoch 2386: train loss 4.964937210083008 val loss 4.964611053466797\n",
      "Epoch 2387: train loss 4.964611053466797 val loss 4.965640544891357\n",
      "Epoch 2388: train loss 4.965640544891357 val loss 4.964463710784912\n",
      "Epoch 2389: train loss 4.964463710784912 val loss 4.96588134765625\n",
      "Epoch 2390: train loss 4.96588134765625 val loss 4.965877532958984\n",
      "Epoch 2391: train loss 4.965877532958984 val loss 4.964265823364258\n",
      "Epoch 2392: train loss 4.964265823364258 val loss 4.966280937194824\n",
      "Epoch 2393: train loss 4.966280937194824 val loss 4.9643449783325195\n",
      "Epoch 2394: train loss 4.9643449783325195 val loss 4.965184211730957\n",
      "Epoch 2395: train loss 4.965184211730957 val loss 4.964380264282227\n",
      "Epoch 2396: train loss 4.964380264282227 val loss 4.9649834632873535\n",
      "Epoch 2397: train loss 4.9649834632873535 val loss 4.964101314544678\n",
      "Epoch 2398: train loss 4.964101314544678 val loss 4.964593887329102\n",
      "Epoch 2399: train loss 4.964593887329102 val loss 4.964228630065918\n",
      "Epoch 2400: train loss 4.964228630065918 val loss 4.965228080749512\n",
      "Epoch 2401: train loss 4.965228080749512 val loss 4.964465141296387\n",
      "Epoch 2402: train loss 4.964465141296387 val loss 4.96492862701416\n",
      "Epoch 2403: train loss 4.96492862701416 val loss 4.964360237121582\n",
      "Epoch 2404: train loss 4.964360237121582 val loss 4.964821815490723\n",
      "Epoch 2405: train loss 4.964821815490723 val loss 4.964686393737793\n",
      "Epoch 2406: train loss 4.964686393737793 val loss 4.9647135734558105\n",
      "Epoch 2407: train loss 4.9647135734558105 val loss 4.964602470397949\n",
      "Epoch 2408: train loss 4.964602470397949 val loss 4.964552402496338\n",
      "Epoch 2409: train loss 4.964552402496338 val loss 4.964463233947754\n",
      "Epoch 2410: train loss 4.964463233947754 val loss 4.964330196380615\n",
      "Epoch 2411: train loss 4.964330196380615 val loss 4.964146614074707\n",
      "Epoch 2412: train loss 4.964146614074707 val loss 4.9645867347717285\n",
      "Epoch 2413: train loss 4.9645867347717285 val loss 4.964949131011963\n",
      "Epoch 2414: train loss 4.964949131011963 val loss 4.964778900146484\n",
      "Epoch 2415: train loss 4.964778900146484 val loss 4.964905738830566\n",
      "Epoch 2416: train loss 4.964905738830566 val loss 4.964363098144531\n",
      "Epoch 2417: train loss 4.964363098144531 val loss 4.964563846588135\n",
      "Epoch 2418: train loss 4.964563846588135 val loss 4.96406364440918\n",
      "Epoch 2419: train loss 4.96406364440918 val loss 4.96530818939209\n",
      "Epoch 2420: train loss 4.96530818939209 val loss 4.964459419250488\n",
      "Epoch 2421: train loss 4.964459419250488 val loss 4.965088844299316\n",
      "Epoch 2422: train loss 4.965088844299316 val loss 4.964624404907227\n",
      "Epoch 2423: train loss 4.964624404907227 val loss 4.966294288635254\n",
      "Epoch 2424: train loss 4.966294288635254 val loss 4.964661121368408\n",
      "Epoch 2425: train loss 4.964661121368408 val loss 4.965538024902344\n",
      "Epoch 2426: train loss 4.965538024902344 val loss 4.965034484863281\n",
      "Epoch 2427: train loss 4.965034484863281 val loss 4.964299201965332\n",
      "Epoch 2428: train loss 4.964299201965332 val loss 4.965792655944824\n",
      "Epoch 2429: train loss 4.965792655944824 val loss 4.964267730712891\n",
      "Epoch 2430: train loss 4.964267730712891 val loss 4.965117454528809\n",
      "Epoch 2431: train loss 4.965117454528809 val loss 4.96455717086792\n",
      "Epoch 2432: train loss 4.96455717086792 val loss 4.964521408081055\n",
      "Epoch 2433: train loss 4.964521408081055 val loss 4.964699745178223\n",
      "Epoch 2434: train loss 4.964699745178223 val loss 4.9641947746276855\n",
      "Epoch 2435: train loss 4.9641947746276855 val loss 4.9650559425354\n",
      "Epoch 2436: train loss 4.9650559425354 val loss 4.964583873748779\n",
      "Epoch 2437: train loss 4.964583873748779 val loss 4.964776039123535\n",
      "Epoch 2438: train loss 4.964776039123535 val loss 4.964510917663574\n",
      "Epoch 2439: train loss 4.964510917663574 val loss 4.965397834777832\n",
      "Epoch 2440: train loss 4.965397834777832 val loss 4.964595794677734\n",
      "Epoch 2441: train loss 4.964595794677734 val loss 4.965697288513184\n",
      "Epoch 2442: train loss 4.965697288513184 val loss 4.964173316955566\n",
      "Epoch 2443: train loss 4.964173316955566 val loss 4.966283321380615\n",
      "Epoch 2444: train loss 4.966283321380615 val loss 4.964241981506348\n",
      "Epoch 2445: train loss 4.964241981506348 val loss 4.964967250823975\n",
      "Epoch 2446: train loss 4.964967250823975 val loss 4.964277267456055\n",
      "Epoch 2447: train loss 4.964277267456055 val loss 4.964794635772705\n",
      "Epoch 2448: train loss 4.964794635772705 val loss 4.96440315246582\n",
      "Epoch 2449: train loss 4.96440315246582 val loss 4.964183807373047\n",
      "Epoch 2450: train loss 4.964183807373047 val loss 4.964306831359863\n",
      "Epoch 2451: train loss 4.964306831359863 val loss 4.964206695556641\n",
      "Epoch 2452: train loss 4.964206695556641 val loss 4.964250564575195\n",
      "Epoch 2453: train loss 4.964250564575195 val loss 4.964766979217529\n",
      "Epoch 2454: train loss 4.964766979217529 val loss 4.964267253875732\n",
      "Epoch 2455: train loss 4.964267253875732 val loss 4.964850425720215\n",
      "Epoch 2456: train loss 4.964850425720215 val loss 4.964212417602539\n",
      "Epoch 2457: train loss 4.964212417602539 val loss 4.964956283569336\n",
      "Epoch 2458: train loss 4.964956283569336 val loss 4.964289665222168\n",
      "Epoch 2459: train loss 4.964289665222168 val loss 4.964694499969482\n",
      "Epoch 2460: train loss 4.964694499969482 val loss 4.964289665222168\n",
      "Epoch 2461: train loss 4.964289665222168 val loss 4.965366363525391\n",
      "Epoch 2462: train loss 4.965366363525391 val loss 4.964499473571777\n",
      "Epoch 2463: train loss 4.964499473571777 val loss 4.9651899337768555\n",
      "Epoch 2464: train loss 4.9651899337768555 val loss 4.96424674987793\n",
      "Epoch 2465: train loss 4.96424674987793 val loss 4.965291976928711\n",
      "Epoch 2466: train loss 4.965291976928711 val loss 4.964467525482178\n",
      "Epoch 2467: train loss 4.964467525482178 val loss 4.964186191558838\n",
      "Epoch 2468: train loss 4.964186191558838 val loss 4.964869022369385\n",
      "Epoch 2469: train loss 4.964869022369385 val loss 4.964150428771973\n",
      "Epoch 2470: train loss 4.964150428771973 val loss 4.965052604675293\n",
      "Epoch 2471: train loss 4.965052604675293 val loss 4.964076042175293\n",
      "Epoch 2472: train loss 4.964076042175293 val loss 4.965756893157959\n",
      "Epoch 2473: train loss 4.965756893157959 val loss 4.963991165161133\n",
      "Epoch 2474: train loss 4.963991165161133 val loss 4.9644083976745605\n",
      "Epoch 2475: train loss 4.9644083976745605 val loss 4.964223861694336\n",
      "Epoch 2476: train loss 4.964223861694336 val loss 4.964861869812012\n",
      "Epoch 2477: train loss 4.964861869812012 val loss 4.964255332946777\n",
      "Epoch 2478: train loss 4.964255332946777 val loss 4.9648332595825195\n",
      "Epoch 2479: train loss 4.9648332595825195 val loss 4.964218616485596\n",
      "Epoch 2480: train loss 4.964218616485596 val loss 4.964135646820068\n",
      "Epoch 2481: train loss 4.964135646820068 val loss 4.964435577392578\n",
      "Epoch 2482: train loss 4.964435577392578 val loss 4.964105606079102\n",
      "Epoch 2483: train loss 4.964105606079102 val loss 4.965533256530762\n",
      "Epoch 2484: train loss 4.965533256530762 val loss 4.964373588562012\n",
      "Epoch 2485: train loss 4.964373588562012 val loss 4.9650702476501465\n",
      "Epoch 2486: train loss 4.9650702476501465 val loss 4.96414852142334\n",
      "Epoch 2487: train loss 4.96414852142334 val loss 4.96610164642334\n",
      "Epoch 2488: train loss 4.96610164642334 val loss 4.964130878448486\n",
      "Epoch 2489: train loss 4.964130878448486 val loss 4.965823173522949\n",
      "Epoch 2490: train loss 4.965823173522949 val loss 4.964728355407715\n",
      "Epoch 2491: train loss 4.964728355407715 val loss 4.966452598571777\n",
      "Epoch 2492: train loss 4.966452598571777 val loss 4.9651408195495605\n",
      "Epoch 2493: train loss 4.9651408195495605 val loss 4.9664201736450195\n",
      "Epoch 2494: train loss 4.9664201736450195 val loss 4.967080593109131\n",
      "Epoch 2495: train loss 4.967080593109131 val loss 4.964240074157715\n",
      "Epoch 2496: train loss 4.964240074157715 val loss 4.968838214874268\n",
      "Epoch 2497: train loss 4.968838214874268 val loss 4.968428611755371\n",
      "Epoch 2498: train loss 4.968428611755371 val loss 4.964563846588135\n",
      "Epoch 2499: train loss 4.964563846588135 val loss 4.966820240020752\n",
      "Epoch 2500: train loss 4.966820240020752 val loss 4.965463638305664\n",
      "Epoch 2501: train loss 4.965463638305664 val loss 4.9660325050354\n",
      "Epoch 2502: train loss 4.9660325050354 val loss 4.966070652008057\n",
      "Epoch 2503: train loss 4.966070652008057 val loss 4.965035915374756\n",
      "Epoch 2504: train loss 4.965035915374756 val loss 4.966300010681152\n",
      "Epoch 2505: train loss 4.966300010681152 val loss 4.964234352111816\n",
      "Epoch 2506: train loss 4.964234352111816 val loss 4.965480804443359\n",
      "Epoch 2507: train loss 4.965480804443359 val loss 4.964182376861572\n",
      "Epoch 2508: train loss 4.964182376861572 val loss 4.965088844299316\n",
      "Epoch 2509: train loss 4.965088844299316 val loss 4.964373588562012\n",
      "Epoch 2510: train loss 4.964373588562012 val loss 4.964569091796875\n",
      "Epoch 2511: train loss 4.964569091796875 val loss 4.964213848114014\n",
      "Epoch 2512: train loss 4.964213848114014 val loss 4.964850425720215\n",
      "Epoch 2513: train loss 4.964850425720215 val loss 4.964284420013428\n",
      "Epoch 2514: train loss 4.964284420013428 val loss 4.9644060134887695\n",
      "Epoch 2515: train loss 4.9644060134887695 val loss 4.964568614959717\n",
      "Epoch 2516: train loss 4.964568614959717 val loss 4.964620590209961\n",
      "Epoch 2517: train loss 4.964620590209961 val loss 4.965115070343018\n",
      "Epoch 2518: train loss 4.965115070343018 val loss 4.964347839355469\n",
      "Epoch 2519: train loss 4.964347839355469 val loss 4.966675281524658\n",
      "Epoch 2520: train loss 4.966675281524658 val loss 4.964022636413574\n",
      "Epoch 2521: train loss 4.964022636413574 val loss 4.965383052825928\n",
      "Epoch 2522: train loss 4.965383052825928 val loss 4.964519023895264\n",
      "Epoch 2523: train loss 4.964519023895264 val loss 4.966076850891113\n",
      "Epoch 2524: train loss 4.966076850891113 val loss 4.965444087982178\n",
      "Epoch 2525: train loss 4.965444087982178 val loss 4.965487480163574\n",
      "Epoch 2526: train loss 4.965487480163574 val loss 4.966174602508545\n",
      "Epoch 2527: train loss 4.966174602508545 val loss 4.9640350341796875\n",
      "Epoch 2528: train loss 4.9640350341796875 val loss 4.966192245483398\n",
      "Epoch 2529: train loss 4.966192245483398 val loss 4.9642462730407715\n",
      "Epoch 2530: train loss 4.9642462730407715 val loss 4.964727401733398\n",
      "Epoch 2531: train loss 4.964727401733398 val loss 4.9641923904418945\n",
      "Epoch 2532: train loss 4.9641923904418945 val loss 4.964526176452637\n",
      "Epoch 2533: train loss 4.964526176452637 val loss 4.964118957519531\n",
      "Epoch 2534: train loss 4.964118957519531 val loss 4.96423864364624\n",
      "Epoch 2535: train loss 4.96423864364624 val loss 4.964308261871338\n",
      "Epoch 2536: train loss 4.964308261871338 val loss 4.964033126831055\n",
      "Epoch 2537: train loss 4.964033126831055 val loss 4.964450836181641\n",
      "Epoch 2538: train loss 4.964450836181641 val loss 4.964443206787109\n",
      "Epoch 2539: train loss 4.964443206787109 val loss 4.964229583740234\n",
      "Epoch 2540: train loss 4.964229583740234 val loss 4.96422004699707\n",
      "Epoch 2541: train loss 4.96422004699707 val loss 4.964128494262695\n",
      "Epoch 2542: train loss 4.964128494262695 val loss 4.964051246643066\n",
      "Epoch 2543: train loss 4.964051246643066 val loss 4.964098930358887\n",
      "Epoch 2544: train loss 4.964098930358887 val loss 4.964912414550781\n",
      "Epoch 2545: train loss 4.964912414550781 val loss 4.964550018310547\n",
      "Epoch 2546: train loss 4.964550018310547 val loss 4.964320182800293\n",
      "Epoch 2547: train loss 4.964320182800293 val loss 4.965367317199707\n",
      "Epoch 2548: train loss 4.965367317199707 val loss 4.964244842529297\n",
      "Epoch 2549: train loss 4.964244842529297 val loss 4.965517044067383\n",
      "Epoch 2550: train loss 4.965517044067383 val loss 4.965072154998779\n",
      "Epoch 2551: train loss 4.965072154998779 val loss 4.964832305908203\n",
      "Epoch 2552: train loss 4.964832305908203 val loss 4.965121269226074\n",
      "Epoch 2553: train loss 4.965121269226074 val loss 4.964821815490723\n",
      "Epoch 2554: train loss 4.964821815490723 val loss 4.965439319610596\n",
      "Epoch 2555: train loss 4.965439319610596 val loss 4.964844703674316\n",
      "Epoch 2556: train loss 4.964844703674316 val loss 4.964608669281006\n",
      "Epoch 2557: train loss 4.964608669281006 val loss 4.964250564575195\n",
      "Epoch 2558: train loss 4.964250564575195 val loss 4.965374946594238\n",
      "Epoch 2559: train loss 4.965374946594238 val loss 4.9650726318359375\n",
      "Epoch 2560: train loss 4.9650726318359375 val loss 4.964920997619629\n",
      "Epoch 2561: train loss 4.964920997619629 val loss 4.964810371398926\n",
      "Epoch 2562: train loss 4.964810371398926 val loss 4.964755058288574\n",
      "Epoch 2563: train loss 4.964755058288574 val loss 4.964967727661133\n",
      "Epoch 2564: train loss 4.964967727661133 val loss 4.9647626876831055\n",
      "Epoch 2565: train loss 4.9647626876831055 val loss 4.9648590087890625\n",
      "Epoch 2566: train loss 4.9648590087890625 val loss 4.964618682861328\n",
      "Epoch 2567: train loss 4.964618682861328 val loss 4.964198589324951\n",
      "Epoch 2568: train loss 4.964198589324951 val loss 4.964412689208984\n",
      "Epoch 2569: train loss 4.964412689208984 val loss 4.964550495147705\n",
      "Epoch 2570: train loss 4.964550495147705 val loss 4.964614391326904\n",
      "Epoch 2571: train loss 4.964614391326904 val loss 4.964349746704102\n",
      "Epoch 2572: train loss 4.964349746704102 val loss 4.9650678634643555\n",
      "Epoch 2573: train loss 4.9650678634643555 val loss 4.9642863273620605\n",
      "Epoch 2574: train loss 4.9642863273620605 val loss 4.96591854095459\n",
      "Epoch 2575: train loss 4.96591854095459 val loss 4.964227199554443\n",
      "Epoch 2576: train loss 4.964227199554443 val loss 4.964786052703857\n",
      "Epoch 2577: train loss 4.964786052703857 val loss 4.9645256996154785\n",
      "Epoch 2578: train loss 4.9645256996154785 val loss 4.964500427246094\n",
      "Epoch 2579: train loss 4.964500427246094 val loss 4.964110374450684\n",
      "Epoch 2580: train loss 4.964110374450684 val loss 4.964160919189453\n",
      "Epoch 2581: train loss 4.964160919189453 val loss 4.964250564575195\n",
      "Epoch 2582: train loss 4.964250564575195 val loss 4.964568614959717\n",
      "Epoch 2583: train loss 4.964568614959717 val loss 4.964147090911865\n",
      "Epoch 2584: train loss 4.964147090911865 val loss 4.964180946350098\n",
      "Epoch 2585: train loss 4.964180946350098 val loss 4.964097499847412\n",
      "Epoch 2586: train loss 4.964097499847412 val loss 4.96417236328125\n",
      "Epoch 2587: train loss 4.96417236328125 val loss 4.96424674987793\n",
      "Epoch 2588: train loss 4.96424674987793 val loss 4.964118003845215\n",
      "Epoch 2589: train loss 4.964118003845215 val loss 4.964012145996094\n",
      "Epoch 2590: train loss 4.964012145996094 val loss 4.963974475860596\n",
      "Epoch 2591: train loss 4.963974475860596 val loss 4.964033126831055\n",
      "Epoch 2592: train loss 4.964033126831055 val loss 4.964291572570801\n",
      "Epoch 2593: train loss 4.964291572570801 val loss 4.964231491088867\n",
      "Epoch 2594: train loss 4.964231491088867 val loss 4.9644975662231445\n",
      "Epoch 2595: train loss 4.9644975662231445 val loss 4.964372158050537\n",
      "Epoch 2596: train loss 4.964372158050537 val loss 4.964361190795898\n",
      "Epoch 2597: train loss 4.964361190795898 val loss 4.9643049240112305\n",
      "Epoch 2598: train loss 4.9643049240112305 val loss 4.964089393615723\n",
      "Epoch 2599: train loss 4.964089393615723 val loss 4.964621543884277\n",
      "Epoch 2600: train loss 4.964621543884277 val loss 4.963935375213623\n",
      "Epoch 2601: train loss 4.963935375213623 val loss 4.964512825012207\n",
      "Epoch 2602: train loss 4.964512825012207 val loss 4.964242935180664\n",
      "Epoch 2603: train loss 4.964242935180664 val loss 4.964178085327148\n",
      "Epoch 2604: train loss 4.964178085327148 val loss 4.964452266693115\n",
      "Epoch 2605: train loss 4.964452266693115 val loss 4.964118480682373\n",
      "Epoch 2606: train loss 4.964118480682373 val loss 4.9645094871521\n",
      "Epoch 2607: train loss 4.9645094871521 val loss 4.964400768280029\n",
      "Epoch 2608: train loss 4.964400768280029 val loss 4.964899063110352\n",
      "Epoch 2609: train loss 4.964899063110352 val loss 4.964384078979492\n",
      "Epoch 2610: train loss 4.964384078979492 val loss 4.965023517608643\n",
      "Epoch 2611: train loss 4.965023517608643 val loss 4.964856147766113\n",
      "Epoch 2612: train loss 4.964856147766113 val loss 4.965126991271973\n",
      "Epoch 2613: train loss 4.965126991271973 val loss 4.964628219604492\n",
      "Epoch 2614: train loss 4.964628219604492 val loss 4.964381217956543\n",
      "Epoch 2615: train loss 4.964381217956543 val loss 4.964305877685547\n",
      "Epoch 2616: train loss 4.964305877685547 val loss 4.964115619659424\n",
      "Epoch 2617: train loss 4.964115619659424 val loss 4.9640092849731445\n",
      "Epoch 2618: train loss 4.9640092849731445 val loss 4.964850425720215\n",
      "Epoch 2619: train loss 4.964850425720215 val loss 4.964149475097656\n",
      "Epoch 2620: train loss 4.964149475097656 val loss 4.965371131896973\n",
      "Epoch 2621: train loss 4.965371131896973 val loss 4.964154243469238\n",
      "Epoch 2622: train loss 4.964154243469238 val loss 4.96462869644165\n",
      "Epoch 2623: train loss 4.96462869644165 val loss 4.963862419128418\n",
      "Epoch 2624: train loss 4.963862419128418 val loss 4.965699195861816\n",
      "Epoch 2625: train loss 4.965699195861816 val loss 4.964205741882324\n",
      "Epoch 2626: train loss 4.964205741882324 val loss 4.965048789978027\n",
      "Epoch 2627: train loss 4.965048789978027 val loss 4.964244365692139\n",
      "Epoch 2628: train loss 4.964244365692139 val loss 4.96513557434082\n",
      "Epoch 2629: train loss 4.96513557434082 val loss 4.964123725891113\n",
      "Epoch 2630: train loss 4.964123725891113 val loss 4.96494197845459\n",
      "Epoch 2631: train loss 4.96494197845459 val loss 4.964155197143555\n",
      "Epoch 2632: train loss 4.964155197143555 val loss 4.966032028198242\n",
      "Epoch 2633: train loss 4.966032028198242 val loss 4.964019298553467\n",
      "Epoch 2634: train loss 4.964019298553467 val loss 4.9665422439575195\n",
      "Epoch 2635: train loss 4.9665422439575195 val loss 4.96621036529541\n",
      "Epoch 2636: train loss 4.96621036529541 val loss 4.964497089385986\n",
      "Epoch 2637: train loss 4.964497089385986 val loss 4.967807769775391\n",
      "Epoch 2638: train loss 4.967807769775391 val loss 4.965062141418457\n",
      "Epoch 2639: train loss 4.965062141418457 val loss 4.967075824737549\n",
      "Epoch 2640: train loss 4.967075824737549 val loss 4.968245506286621\n",
      "Epoch 2641: train loss 4.968245506286621 val loss 4.966160774230957\n",
      "Epoch 2642: train loss 4.966160774230957 val loss 4.965865612030029\n",
      "Epoch 2643: train loss 4.965865612030029 val loss 4.966172218322754\n",
      "Epoch 2644: train loss 4.966172218322754 val loss 4.964715957641602\n",
      "Epoch 2645: train loss 4.964715957641602 val loss 4.965756893157959\n",
      "Epoch 2646: train loss 4.965756893157959 val loss 4.964148998260498\n",
      "Epoch 2647: train loss 4.964148998260498 val loss 4.9659342765808105\n",
      "Epoch 2648: train loss 4.9659342765808105 val loss 4.964268684387207\n",
      "Epoch 2649: train loss 4.964268684387207 val loss 4.966368675231934\n",
      "Epoch 2650: train loss 4.966368675231934 val loss 4.965912818908691\n",
      "Epoch 2651: train loss 4.965912818908691 val loss 4.9645562171936035\n",
      "Epoch 2652: train loss 4.9645562171936035 val loss 4.965152263641357\n",
      "Epoch 2653: train loss 4.965152263641357 val loss 4.964712619781494\n",
      "Epoch 2654: train loss 4.964712619781494 val loss 4.964857578277588\n",
      "Epoch 2655: train loss 4.964857578277588 val loss 4.964219093322754\n",
      "Epoch 2656: train loss 4.964219093322754 val loss 4.965869903564453\n",
      "Epoch 2657: train loss 4.965869903564453 val loss 4.964420795440674\n",
      "Epoch 2658: train loss 4.964420795440674 val loss 4.9649529457092285\n",
      "Epoch 2659: train loss 4.9649529457092285 val loss 4.96422004699707\n",
      "Epoch 2660: train loss 4.96422004699707 val loss 4.965476036071777\n",
      "Epoch 2661: train loss 4.965476036071777 val loss 4.964110851287842\n",
      "Epoch 2662: train loss 4.964110851287842 val loss 4.96477746963501\n",
      "Epoch 2663: train loss 4.96477746963501 val loss 4.964163780212402\n",
      "Epoch 2664: train loss 4.964163780212402 val loss 4.964341640472412\n",
      "Epoch 2665: train loss 4.964341640472412 val loss 4.9642014503479\n",
      "Epoch 2666: train loss 4.9642014503479 val loss 4.964580059051514\n",
      "Epoch 2667: train loss 4.964580059051514 val loss 4.964103698730469\n",
      "Epoch 2668: train loss 4.964103698730469 val loss 4.964046955108643\n",
      "Epoch 2669: train loss 4.964046955108643 val loss 4.9639997482299805\n",
      "Epoch 2670: train loss 4.9639997482299805 val loss 4.964029312133789\n",
      "Epoch 2671: train loss 4.964029312133789 val loss 4.964114189147949\n",
      "Epoch 2672: train loss 4.964114189147949 val loss 4.964250564575195\n",
      "Epoch 2673: train loss 4.964250564575195 val loss 4.964062213897705\n",
      "Epoch 2674: train loss 4.964062213897705 val loss 4.964741230010986\n",
      "Epoch 2675: train loss 4.964741230010986 val loss 4.9646148681640625\n",
      "Epoch 2676: train loss 4.9646148681640625 val loss 4.96425724029541\n",
      "Epoch 2677: train loss 4.96425724029541 val loss 4.965044975280762\n",
      "Epoch 2678: train loss 4.965044975280762 val loss 4.963976860046387\n",
      "Epoch 2679: train loss 4.963976860046387 val loss 4.965225696563721\n",
      "Epoch 2680: train loss 4.965225696563721 val loss 4.963923454284668\n",
      "Epoch 2681: train loss 4.963923454284668 val loss 4.964924335479736\n",
      "Epoch 2682: train loss 4.964924335479736 val loss 4.9640655517578125\n",
      "Epoch 2683: train loss 4.9640655517578125 val loss 4.964736461639404\n",
      "Epoch 2684: train loss 4.964736461639404 val loss 4.963995456695557\n",
      "Epoch 2685: train loss 4.963995456695557 val loss 4.96556282043457\n",
      "Epoch 2686: train loss 4.96556282043457 val loss 4.964106559753418\n",
      "Epoch 2687: train loss 4.964106559753418 val loss 4.964779376983643\n",
      "Epoch 2688: train loss 4.964779376983643 val loss 4.964005947113037\n",
      "Epoch 2689: train loss 4.964005947113037 val loss 4.963907241821289\n",
      "Epoch 2690: train loss 4.963907241821289 val loss 4.964115619659424\n",
      "Epoch 2691: train loss 4.964115619659424 val loss 4.9644880294799805\n",
      "Epoch 2692: train loss 4.9644880294799805 val loss 4.9639739990234375\n",
      "Epoch 2693: train loss 4.9639739990234375 val loss 4.964554309844971\n",
      "Epoch 2694: train loss 4.964554309844971 val loss 4.963878154754639\n",
      "Epoch 2695: train loss 4.963878154754639 val loss 4.965134620666504\n",
      "Epoch 2696: train loss 4.965134620666504 val loss 4.9643449783325195\n",
      "Epoch 2697: train loss 4.9643449783325195 val loss 4.964479446411133\n",
      "Epoch 2698: train loss 4.964479446411133 val loss 4.96483850479126\n",
      "Epoch 2699: train loss 4.96483850479126 val loss 4.963914394378662\n",
      "Epoch 2700: train loss 4.963914394378662 val loss 4.964348793029785\n",
      "Epoch 2701: train loss 4.964348793029785 val loss 4.96418571472168\n",
      "Epoch 2702: train loss 4.96418571472168 val loss 4.964031219482422\n",
      "Epoch 2703: train loss 4.964031219482422 val loss 4.964803695678711\n",
      "Epoch 2704: train loss 4.964803695678711 val loss 4.964031219482422\n",
      "Epoch 2705: train loss 4.964031219482422 val loss 4.964112281799316\n",
      "Epoch 2706: train loss 4.964112281799316 val loss 4.96445369720459\n",
      "Epoch 2707: train loss 4.96445369720459 val loss 4.963934898376465\n",
      "Epoch 2708: train loss 4.963934898376465 val loss 4.965166091918945\n",
      "Epoch 2709: train loss 4.965166091918945 val loss 4.963975429534912\n",
      "Epoch 2710: train loss 4.963975429534912 val loss 4.965109825134277\n",
      "Epoch 2711: train loss 4.965109825134277 val loss 4.964003562927246\n",
      "Epoch 2712: train loss 4.964003562927246 val loss 4.9648518562316895\n",
      "Epoch 2713: train loss 4.9648518562316895 val loss 4.964430809020996\n",
      "Epoch 2714: train loss 4.964430809020996 val loss 4.9641032218933105\n",
      "Epoch 2715: train loss 4.9641032218933105 val loss 4.965080738067627\n",
      "Epoch 2716: train loss 4.965080738067627 val loss 4.9639058113098145\n",
      "Epoch 2717: train loss 4.9639058113098145 val loss 4.96466588973999\n",
      "Epoch 2718: train loss 4.96466588973999 val loss 4.9639105796813965\n",
      "Epoch 2719: train loss 4.9639105796813965 val loss 4.964273452758789\n",
      "Epoch 2720: train loss 4.964273452758789 val loss 4.964407920837402\n",
      "Epoch 2721: train loss 4.964407920837402 val loss 4.963818550109863\n",
      "Epoch 2722: train loss 4.963818550109863 val loss 4.965247631072998\n",
      "Epoch 2723: train loss 4.965247631072998 val loss 4.963839054107666\n",
      "Epoch 2724: train loss 4.963839054107666 val loss 4.964636325836182\n",
      "Epoch 2725: train loss 4.964636325836182 val loss 4.96389102935791\n",
      "Epoch 2726: train loss 4.96389102935791 val loss 4.9642133712768555\n",
      "Epoch 2727: train loss 4.9642133712768555 val loss 4.964503765106201\n",
      "Epoch 2728: train loss 4.964503765106201 val loss 4.963809967041016\n",
      "Epoch 2729: train loss 4.963809967041016 val loss 4.965201377868652\n",
      "Epoch 2730: train loss 4.965201377868652 val loss 4.963873386383057\n",
      "Epoch 2731: train loss 4.963873386383057 val loss 4.9644856452941895\n",
      "Epoch 2732: train loss 4.9644856452941895 val loss 4.964134693145752\n",
      "Epoch 2733: train loss 4.964134693145752 val loss 4.963900566101074\n",
      "Epoch 2734: train loss 4.963900566101074 val loss 4.964300632476807\n",
      "Epoch 2735: train loss 4.964300632476807 val loss 4.964176654815674\n",
      "Epoch 2736: train loss 4.964176654815674 val loss 4.9639573097229\n",
      "Epoch 2737: train loss 4.9639573097229 val loss 4.964704990386963\n",
      "Epoch 2738: train loss 4.964704990386963 val loss 4.963881015777588\n",
      "Epoch 2739: train loss 4.963881015777588 val loss 4.964180946350098\n",
      "Epoch 2740: train loss 4.964180946350098 val loss 4.964869499206543\n",
      "Epoch 2741: train loss 4.964869499206543 val loss 4.964447975158691\n",
      "Epoch 2742: train loss 4.964447975158691 val loss 4.965022563934326\n",
      "Epoch 2743: train loss 4.965022563934326 val loss 4.96425199508667\n",
      "Epoch 2744: train loss 4.96425199508667 val loss 4.965053558349609\n",
      "Epoch 2745: train loss 4.965053558349609 val loss 4.964338779449463\n",
      "Epoch 2746: train loss 4.964338779449463 val loss 4.965487003326416\n",
      "Epoch 2747: train loss 4.965487003326416 val loss 4.9646220207214355\n",
      "Epoch 2748: train loss 4.9646220207214355 val loss 4.964550495147705\n",
      "Epoch 2749: train loss 4.964550495147705 val loss 4.964926719665527\n",
      "Epoch 2750: train loss 4.964926719665527 val loss 4.96439790725708\n",
      "Epoch 2751: train loss 4.96439790725708 val loss 4.964076519012451\n",
      "Epoch 2752: train loss 4.964076519012451 val loss 4.96523904800415\n",
      "Epoch 2753: train loss 4.96523904800415 val loss 4.9649152755737305\n",
      "Epoch 2754: train loss 4.9649152755737305 val loss 4.964658737182617\n",
      "Epoch 2755: train loss 4.964658737182617 val loss 4.9641008377075195\n",
      "Epoch 2756: train loss 4.9641008377075195 val loss 4.96429443359375\n",
      "Epoch 2757: train loss 4.96429443359375 val loss 4.9641313552856445\n",
      "Epoch 2758: train loss 4.9641313552856445 val loss 4.963878631591797\n",
      "Epoch 2759: train loss 4.963878631591797 val loss 4.964128494262695\n",
      "Epoch 2760: train loss 4.964128494262695 val loss 4.9648661613464355\n",
      "Epoch 2761: train loss 4.9648661613464355 val loss 4.964388847351074\n",
      "Epoch 2762: train loss 4.964388847351074 val loss 4.964773178100586\n",
      "Epoch 2763: train loss 4.964773178100586 val loss 4.964150905609131\n",
      "Epoch 2764: train loss 4.964150905609131 val loss 4.964180946350098\n",
      "Epoch 2765: train loss 4.964180946350098 val loss 4.964007377624512\n",
      "Epoch 2766: train loss 4.964007377624512 val loss 4.964944362640381\n",
      "Epoch 2767: train loss 4.964944362640381 val loss 4.964478492736816\n",
      "Epoch 2768: train loss 4.964478492736816 val loss 4.964521408081055\n",
      "Epoch 2769: train loss 4.964521408081055 val loss 4.9647345542907715\n",
      "Epoch 2770: train loss 4.9647345542907715 val loss 4.96423864364624\n",
      "Epoch 2771: train loss 4.96423864364624 val loss 4.965694427490234\n",
      "Epoch 2772: train loss 4.965694427490234 val loss 4.9655561447143555\n",
      "Epoch 2773: train loss 4.9655561447143555 val loss 4.964272975921631\n",
      "Epoch 2774: train loss 4.964272975921631 val loss 4.964504241943359\n",
      "Epoch 2775: train loss 4.964504241943359 val loss 4.965047836303711\n",
      "Epoch 2776: train loss 4.965047836303711 val loss 4.965091228485107\n",
      "Epoch 2777: train loss 4.965091228485107 val loss 4.964156150817871\n",
      "Epoch 2778: train loss 4.964156150817871 val loss 4.964017868041992\n",
      "Epoch 2779: train loss 4.964017868041992 val loss 4.963908672332764\n",
      "Epoch 2780: train loss 4.963908672332764 val loss 4.964205741882324\n",
      "Epoch 2781: train loss 4.964205741882324 val loss 4.964155673980713\n",
      "Epoch 2782: train loss 4.964155673980713 val loss 4.964051246643066\n",
      "Epoch 2783: train loss 4.964051246643066 val loss 4.9641218185424805\n",
      "Epoch 2784: train loss 4.9641218185424805 val loss 4.964118957519531\n",
      "Epoch 2785: train loss 4.964118957519531 val loss 4.963919639587402\n",
      "Epoch 2786: train loss 4.963919639587402 val loss 4.9639482498168945\n",
      "Epoch 2787: train loss 4.9639482498168945 val loss 4.963834285736084\n",
      "Epoch 2788: train loss 4.963834285736084 val loss 4.96390962600708\n",
      "Epoch 2789: train loss 4.96390962600708 val loss 4.964401721954346\n",
      "Epoch 2790: train loss 4.964401721954346 val loss 4.964249610900879\n",
      "Epoch 2791: train loss 4.964249610900879 val loss 4.964235305786133\n",
      "Epoch 2792: train loss 4.964235305786133 val loss 4.964406490325928\n",
      "Epoch 2793: train loss 4.964406490325928 val loss 4.964057445526123\n",
      "Epoch 2794: train loss 4.964057445526123 val loss 4.964632034301758\n",
      "Epoch 2795: train loss 4.964632034301758 val loss 4.96409797668457\n",
      "Epoch 2796: train loss 4.96409797668457 val loss 4.964021682739258\n",
      "Epoch 2797: train loss 4.964021682739258 val loss 4.964715957641602\n",
      "Epoch 2798: train loss 4.964715957641602 val loss 4.9638895988464355\n",
      "Epoch 2799: train loss 4.9638895988464355 val loss 4.963957786560059\n",
      "Epoch 2800: train loss 4.963957786560059 val loss 4.964145183563232\n",
      "Epoch 2801: train loss 4.964145183563232 val loss 4.964066505432129\n",
      "Epoch 2802: train loss 4.964066505432129 val loss 4.964020729064941\n",
      "Epoch 2803: train loss 4.964020729064941 val loss 4.963903903961182\n",
      "Epoch 2804: train loss 4.963903903961182 val loss 4.963873863220215\n",
      "Epoch 2805: train loss 4.963873863220215 val loss 4.963971138000488\n",
      "Epoch 2806: train loss 4.963971138000488 val loss 4.964552879333496\n",
      "Epoch 2807: train loss 4.964552879333496 val loss 4.963931083679199\n",
      "Epoch 2808: train loss 4.963931083679199 val loss 4.966004371643066\n",
      "Epoch 2809: train loss 4.966004371643066 val loss 4.9644246101379395\n",
      "Epoch 2810: train loss 4.9644246101379395 val loss 4.9662628173828125\n",
      "Epoch 2811: train loss 4.9662628173828125 val loss 4.966886043548584\n",
      "Epoch 2812: train loss 4.966886043548584 val loss 4.964491367340088\n",
      "Epoch 2813: train loss 4.964491367340088 val loss 4.967572212219238\n",
      "Epoch 2814: train loss 4.967572212219238 val loss 4.966644287109375\n",
      "Epoch 2815: train loss 4.966644287109375 val loss 4.964724540710449\n",
      "Epoch 2816: train loss 4.964724540710449 val loss 4.966667652130127\n",
      "Epoch 2817: train loss 4.966667652130127 val loss 4.964763641357422\n",
      "Epoch 2818: train loss 4.964763641357422 val loss 4.966409683227539\n",
      "Epoch 2819: train loss 4.966409683227539 val loss 4.96556282043457\n",
      "Epoch 2820: train loss 4.96556282043457 val loss 4.965533256530762\n",
      "Epoch 2821: train loss 4.965533256530762 val loss 4.966303825378418\n",
      "Epoch 2822: train loss 4.966303825378418 val loss 4.963827610015869\n",
      "Epoch 2823: train loss 4.963827610015869 val loss 4.966305732727051\n",
      "Epoch 2824: train loss 4.966305732727051 val loss 4.96408748626709\n",
      "Epoch 2825: train loss 4.96408748626709 val loss 4.964933395385742\n",
      "Epoch 2826: train loss 4.964933395385742 val loss 4.964171409606934\n",
      "Epoch 2827: train loss 4.964171409606934 val loss 4.965750694274902\n",
      "Epoch 2828: train loss 4.965750694274902 val loss 4.964388847351074\n",
      "Epoch 2829: train loss 4.964388847351074 val loss 4.965693950653076\n",
      "Epoch 2830: train loss 4.965693950653076 val loss 4.965709686279297\n",
      "Epoch 2831: train loss 4.965709686279297 val loss 4.9641008377075195\n",
      "Epoch 2832: train loss 4.9641008377075195 val loss 4.964682579040527\n",
      "Epoch 2833: train loss 4.964682579040527 val loss 4.965006351470947\n",
      "Epoch 2834: train loss 4.965006351470947 val loss 4.965253829956055\n",
      "Epoch 2835: train loss 4.965253829956055 val loss 4.964169025421143\n",
      "Epoch 2836: train loss 4.964169025421143 val loss 4.963972091674805\n",
      "Epoch 2837: train loss 4.963972091674805 val loss 4.963934421539307\n",
      "Epoch 2838: train loss 4.963934421539307 val loss 4.964000701904297\n",
      "Epoch 2839: train loss 4.964000701904297 val loss 4.963925838470459\n",
      "Epoch 2840: train loss 4.963925838470459 val loss 4.963817596435547\n",
      "Epoch 2841: train loss 4.963817596435547 val loss 4.963909149169922\n",
      "Epoch 2842: train loss 4.963909149169922 val loss 4.963927745819092\n",
      "Epoch 2843: train loss 4.963927745819092 val loss 4.964507579803467\n",
      "Epoch 2844: train loss 4.964507579803467 val loss 4.964537143707275\n",
      "Epoch 2845: train loss 4.964537143707275 val loss 4.9643425941467285\n",
      "Epoch 2846: train loss 4.9643425941467285 val loss 4.964651107788086\n",
      "Epoch 2847: train loss 4.964651107788086 val loss 4.963833808898926\n",
      "Epoch 2848: train loss 4.963833808898926 val loss 4.96428918838501\n",
      "Epoch 2849: train loss 4.96428918838501 val loss 4.963685512542725\n",
      "Epoch 2850: train loss 4.963685512542725 val loss 4.963881492614746\n",
      "Epoch 2851: train loss 4.963881492614746 val loss 4.964171409606934\n",
      "Epoch 2852: train loss 4.964171409606934 val loss 4.963691711425781\n",
      "Epoch 2853: train loss 4.963691711425781 val loss 4.963908672332764\n",
      "Epoch 2854: train loss 4.963908672332764 val loss 4.963935375213623\n",
      "Epoch 2855: train loss 4.963935375213623 val loss 4.964056968688965\n",
      "Epoch 2856: train loss 4.964056968688965 val loss 4.96381950378418\n",
      "Epoch 2857: train loss 4.96381950378418 val loss 4.964304447174072\n",
      "Epoch 2858: train loss 4.964304447174072 val loss 4.964103698730469\n",
      "Epoch 2859: train loss 4.964103698730469 val loss 4.964187145233154\n",
      "Epoch 2860: train loss 4.964187145233154 val loss 4.9646735191345215\n",
      "Epoch 2861: train loss 4.9646735191345215 val loss 4.963914394378662\n",
      "Epoch 2862: train loss 4.963914394378662 val loss 4.964091777801514\n",
      "Epoch 2863: train loss 4.964091777801514 val loss 4.963704586029053\n",
      "Epoch 2864: train loss 4.963704586029053 val loss 4.963997840881348\n",
      "Epoch 2865: train loss 4.963997840881348 val loss 4.963840961456299\n",
      "Epoch 2866: train loss 4.963840961456299 val loss 4.96401834487915\n",
      "Epoch 2867: train loss 4.96401834487915 val loss 4.963822364807129\n",
      "Epoch 2868: train loss 4.963822364807129 val loss 4.96429967880249\n",
      "Epoch 2869: train loss 4.96429967880249 val loss 4.9637532234191895\n",
      "Epoch 2870: train loss 4.9637532234191895 val loss 4.964824676513672\n",
      "Epoch 2871: train loss 4.964824676513672 val loss 4.963994026184082\n",
      "Epoch 2872: train loss 4.963994026184082 val loss 4.964555740356445\n",
      "Epoch 2873: train loss 4.964555740356445 val loss 4.964029312133789\n",
      "Epoch 2874: train loss 4.964029312133789 val loss 4.963727951049805\n",
      "Epoch 2875: train loss 4.963727951049805 val loss 4.963766574859619\n",
      "Epoch 2876: train loss 4.963766574859619 val loss 4.963809013366699\n",
      "Epoch 2877: train loss 4.963809013366699 val loss 4.963703632354736\n",
      "Epoch 2878: train loss 4.963703632354736 val loss 4.963655471801758\n",
      "Epoch 2879: train loss 4.963655471801758 val loss 4.9641289710998535\n",
      "Epoch 2880: train loss 4.9641289710998535 val loss 4.9645256996154785\n",
      "Epoch 2881: train loss 4.9645256996154785 val loss 4.964249610900879\n",
      "Epoch 2882: train loss 4.964249610900879 val loss 4.964907646179199\n",
      "Epoch 2883: train loss 4.964907646179199 val loss 4.963590145111084\n",
      "Epoch 2884: train loss 4.963590145111084 val loss 4.964122772216797\n",
      "Epoch 2885: train loss 4.964122772216797 val loss 4.964201927185059\n",
      "Epoch 2886: train loss 4.964201927185059 val loss 4.963718891143799\n",
      "Epoch 2887: train loss 4.963718891143799 val loss 4.964019775390625\n",
      "Epoch 2888: train loss 4.964019775390625 val loss 4.9637908935546875\n",
      "Epoch 2889: train loss 4.9637908935546875 val loss 4.963929176330566\n",
      "Epoch 2890: train loss 4.963929176330566 val loss 4.964925289154053\n",
      "Epoch 2891: train loss 4.964925289154053 val loss 4.964507579803467\n",
      "Epoch 2892: train loss 4.964507579803467 val loss 4.965183734893799\n",
      "Epoch 2893: train loss 4.965183734893799 val loss 4.963995933532715\n",
      "Epoch 2894: train loss 4.963995933532715 val loss 4.965577125549316\n",
      "Epoch 2895: train loss 4.965577125549316 val loss 4.965065002441406\n",
      "Epoch 2896: train loss 4.965065002441406 val loss 4.965038776397705\n",
      "Epoch 2897: train loss 4.965038776397705 val loss 4.964385032653809\n",
      "Epoch 2898: train loss 4.964385032653809 val loss 4.965770721435547\n",
      "Epoch 2899: train loss 4.965770721435547 val loss 4.965892791748047\n",
      "Epoch 2900: train loss 4.965892791748047 val loss 4.964088439941406\n",
      "Epoch 2901: train loss 4.964088439941406 val loss 4.965872764587402\n",
      "Epoch 2902: train loss 4.965872764587402 val loss 4.963792324066162\n",
      "Epoch 2903: train loss 4.963792324066162 val loss 4.965372085571289\n",
      "Epoch 2904: train loss 4.965372085571289 val loss 4.963953018188477\n",
      "Epoch 2905: train loss 4.963953018188477 val loss 4.965188026428223\n",
      "Epoch 2906: train loss 4.965188026428223 val loss 4.963662624359131\n",
      "Epoch 2907: train loss 4.963662624359131 val loss 4.964465618133545\n",
      "Epoch 2908: train loss 4.964465618133545 val loss 4.963584899902344\n",
      "Epoch 2909: train loss 4.963584899902344 val loss 4.964624881744385\n",
      "Epoch 2910: train loss 4.964624881744385 val loss 4.96400260925293\n",
      "Epoch 2911: train loss 4.96400260925293 val loss 4.963827133178711\n",
      "Epoch 2912: train loss 4.963827133178711 val loss 4.964515686035156\n",
      "Epoch 2913: train loss 4.964515686035156 val loss 4.963772773742676\n",
      "Epoch 2914: train loss 4.963772773742676 val loss 4.964669227600098\n",
      "Epoch 2915: train loss 4.964669227600098 val loss 4.963691711425781\n",
      "Epoch 2916: train loss 4.963691711425781 val loss 4.963905334472656\n",
      "Epoch 2917: train loss 4.963905334472656 val loss 4.964768409729004\n",
      "Epoch 2918: train loss 4.964768409729004 val loss 4.964313983917236\n",
      "Epoch 2919: train loss 4.964313983917236 val loss 4.965451717376709\n",
      "Epoch 2920: train loss 4.965451717376709 val loss 4.964683532714844\n",
      "Epoch 2921: train loss 4.964683532714844 val loss 4.965599060058594\n",
      "Epoch 2922: train loss 4.965599060058594 val loss 4.9658284187316895\n",
      "Epoch 2923: train loss 4.9658284187316895 val loss 4.963954448699951\n",
      "Epoch 2924: train loss 4.963954448699951 val loss 4.966341972351074\n",
      "Epoch 2925: train loss 4.966341972351074 val loss 4.963991165161133\n",
      "Epoch 2926: train loss 4.963991165161133 val loss 4.965353488922119\n",
      "Epoch 2927: train loss 4.965353488922119 val loss 4.964686393737793\n",
      "Epoch 2928: train loss 4.964686393737793 val loss 4.964719772338867\n",
      "Epoch 2929: train loss 4.964719772338867 val loss 4.963724613189697\n",
      "Epoch 2930: train loss 4.963724613189697 val loss 4.965721130371094\n",
      "Epoch 2931: train loss 4.965721130371094 val loss 4.965234756469727\n",
      "Epoch 2932: train loss 4.965234756469727 val loss 4.964638710021973\n",
      "Epoch 2933: train loss 4.964638710021973 val loss 4.964627265930176\n",
      "Epoch 2934: train loss 4.964627265930176 val loss 4.965423107147217\n",
      "Epoch 2935: train loss 4.965423107147217 val loss 4.9656243324279785\n",
      "Epoch 2936: train loss 4.9656243324279785 val loss 4.963978290557861\n",
      "Epoch 2937: train loss 4.963978290557861 val loss 4.966618061065674\n",
      "Epoch 2938: train loss 4.966618061065674 val loss 4.963910102844238\n",
      "Epoch 2939: train loss 4.963910102844238 val loss 4.965519905090332\n",
      "Epoch 2940: train loss 4.965519905090332 val loss 4.964973449707031\n",
      "Epoch 2941: train loss 4.964973449707031 val loss 4.9643659591674805\n",
      "Epoch 2942: train loss 4.9643659591674805 val loss 4.963926315307617\n",
      "Epoch 2943: train loss 4.963926315307617 val loss 4.965408802032471\n",
      "Epoch 2944: train loss 4.965408802032471 val loss 4.964925765991211\n",
      "Epoch 2945: train loss 4.964925765991211 val loss 4.964348316192627\n",
      "Epoch 2946: train loss 4.964348316192627 val loss 4.9642791748046875\n",
      "Epoch 2947: train loss 4.9642791748046875 val loss 4.96453332901001\n",
      "Epoch 2948: train loss 4.96453332901001 val loss 4.964290142059326\n",
      "Epoch 2949: train loss 4.964290142059326 val loss 4.9639129638671875\n",
      "Epoch 2950: train loss 4.9639129638671875 val loss 4.965169906616211\n",
      "Epoch 2951: train loss 4.965169906616211 val loss 4.963923931121826\n",
      "Epoch 2952: train loss 4.963923931121826 val loss 4.9645256996154785\n",
      "Epoch 2953: train loss 4.9645256996154785 val loss 4.963774681091309\n",
      "Epoch 2954: train loss 4.963774681091309 val loss 4.964911937713623\n",
      "Epoch 2955: train loss 4.964911937713623 val loss 4.964062690734863\n",
      "Epoch 2956: train loss 4.964062690734863 val loss 4.964231491088867\n",
      "Epoch 2957: train loss 4.964231491088867 val loss 4.9638671875\n",
      "Epoch 2958: train loss 4.9638671875 val loss 4.963682174682617\n",
      "Epoch 2959: train loss 4.963682174682617 val loss 4.963818550109863\n",
      "Epoch 2960: train loss 4.963818550109863 val loss 4.9637250900268555\n",
      "Epoch 2961: train loss 4.9637250900268555 val loss 4.9643635749816895\n",
      "Epoch 2962: train loss 4.9643635749816895 val loss 4.963998794555664\n",
      "Epoch 2963: train loss 4.963998794555664 val loss 4.963864326477051\n",
      "Epoch 2964: train loss 4.963864326477051 val loss 4.964587688446045\n",
      "Epoch 2965: train loss 4.964587688446045 val loss 4.96358060836792\n",
      "Epoch 2966: train loss 4.96358060836792 val loss 4.9640889167785645\n",
      "Epoch 2967: train loss 4.9640889167785645 val loss 4.964015960693359\n",
      "Epoch 2968: train loss 4.964015960693359 val loss 4.963598728179932\n",
      "Epoch 2969: train loss 4.963598728179932 val loss 4.964041233062744\n",
      "Epoch 2970: train loss 4.964041233062744 val loss 4.963929176330566\n",
      "Epoch 2971: train loss 4.963929176330566 val loss 4.963720321655273\n",
      "Epoch 2972: train loss 4.963720321655273 val loss 4.963995933532715\n",
      "Epoch 2973: train loss 4.963995933532715 val loss 4.964040756225586\n",
      "Epoch 2974: train loss 4.964040756225586 val loss 4.963730335235596\n",
      "Epoch 2975: train loss 4.963730335235596 val loss 4.964264869689941\n",
      "Epoch 2976: train loss 4.964264869689941 val loss 4.963906764984131\n",
      "Epoch 2977: train loss 4.963906764984131 val loss 4.963906288146973\n",
      "Epoch 2978: train loss 4.963906288146973 val loss 4.965377330780029\n",
      "Epoch 2979: train loss 4.965377330780029 val loss 4.9649505615234375\n",
      "Epoch 2980: train loss 4.9649505615234375 val loss 4.964778423309326\n",
      "Epoch 2981: train loss 4.964778423309326 val loss 4.964747428894043\n",
      "Epoch 2982: train loss 4.964747428894043 val loss 4.964637279510498\n",
      "Epoch 2983: train loss 4.964637279510498 val loss 4.964723110198975\n",
      "Epoch 2984: train loss 4.964723110198975 val loss 4.964470386505127\n",
      "Epoch 2985: train loss 4.964470386505127 val loss 4.964012145996094\n",
      "Epoch 2986: train loss 4.964012145996094 val loss 4.9650702476501465\n",
      "Epoch 2987: train loss 4.9650702476501465 val loss 4.964540481567383\n",
      "Epoch 2988: train loss 4.964540481567383 val loss 4.964742183685303\n",
      "Epoch 2989: train loss 4.964742183685303 val loss 4.9640116691589355\n",
      "Epoch 2990: train loss 4.9640116691589355 val loss 4.964795112609863\n",
      "Epoch 2991: train loss 4.964795112609863 val loss 4.964204788208008\n",
      "Epoch 2992: train loss 4.964204788208008 val loss 4.964381694793701\n",
      "Epoch 2993: train loss 4.964381694793701 val loss 4.965078353881836\n",
      "Epoch 2994: train loss 4.965078353881836 val loss 4.9649763107299805\n",
      "Epoch 2995: train loss 4.9649763107299805 val loss 4.96551513671875\n",
      "Epoch 2996: train loss 4.96551513671875 val loss 4.963888168334961\n",
      "Epoch 2997: train loss 4.963888168334961 val loss 4.96634578704834\n",
      "Epoch 2998: train loss 4.96634578704834 val loss 4.964001178741455\n",
      "Epoch 2999: train loss 4.964001178741455 val loss 4.9656476974487305\n",
      "Epoch 3000: train loss 4.9656476974487305 val loss 4.9651970863342285\n",
      "Epoch 3001: train loss 4.9651970863342285 val loss 4.964468955993652\n",
      "Epoch 3002: train loss 4.964468955993652 val loss 4.964430809020996\n",
      "Epoch 3003: train loss 4.964430809020996 val loss 4.964438438415527\n",
      "Epoch 3004: train loss 4.964438438415527 val loss 4.964737892150879\n",
      "Epoch 3005: train loss 4.964737892150879 val loss 4.963996410369873\n",
      "Epoch 3006: train loss 4.963996410369873 val loss 4.9648332595825195\n",
      "Epoch 3007: train loss 4.9648332595825195 val loss 4.963751316070557\n",
      "Epoch 3008: train loss 4.963751316070557 val loss 4.96519660949707\n",
      "Epoch 3009: train loss 4.96519660949707 val loss 4.96391487121582\n",
      "Epoch 3010: train loss 4.96391487121582 val loss 4.964926242828369\n",
      "Epoch 3011: train loss 4.964926242828369 val loss 4.964328765869141\n",
      "Epoch 3012: train loss 4.964328765869141 val loss 4.9650983810424805\n",
      "Epoch 3013: train loss 4.9650983810424805 val loss 4.964764595031738\n",
      "Epoch 3014: train loss 4.964764595031738 val loss 4.964740753173828\n",
      "Epoch 3015: train loss 4.964740753173828 val loss 4.964111328125\n",
      "Epoch 3016: train loss 4.964111328125 val loss 4.963981628417969\n",
      "Epoch 3017: train loss 4.963981628417969 val loss 4.964267253875732\n",
      "Epoch 3018: train loss 4.964267253875732 val loss 4.9636030197143555\n",
      "Epoch 3019: train loss 4.9636030197143555 val loss 4.964423656463623\n",
      "Epoch 3020: train loss 4.964423656463623 val loss 4.963986396789551\n",
      "Epoch 3021: train loss 4.963986396789551 val loss 4.964543342590332\n",
      "Epoch 3022: train loss 4.964543342590332 val loss 4.96406888961792\n",
      "Epoch 3023: train loss 4.96406888961792 val loss 4.963868141174316\n",
      "Epoch 3024: train loss 4.963868141174316 val loss 4.964633464813232\n",
      "Epoch 3025: train loss 4.964633464813232 val loss 4.964003086090088\n",
      "Epoch 3026: train loss 4.964003086090088 val loss 4.964334011077881\n",
      "Epoch 3027: train loss 4.964334011077881 val loss 4.96407413482666\n",
      "Epoch 3028: train loss 4.96407413482666 val loss 4.964162349700928\n",
      "Epoch 3029: train loss 4.964162349700928 val loss 4.9639363288879395\n",
      "Epoch 3030: train loss 4.9639363288879395 val loss 4.964123249053955\n",
      "Epoch 3031: train loss 4.964123249053955 val loss 4.963723659515381\n",
      "Epoch 3032: train loss 4.963723659515381 val loss 4.963883876800537\n",
      "Epoch 3033: train loss 4.963883876800537 val loss 4.964048862457275\n",
      "Epoch 3034: train loss 4.964048862457275 val loss 4.9638166427612305\n",
      "Epoch 3035: train loss 4.9638166427612305 val loss 4.9636921882629395\n",
      "Epoch 3036: train loss 4.9636921882629395 val loss 4.963761329650879\n",
      "Epoch 3037: train loss 4.963761329650879 val loss 4.96400260925293\n",
      "Epoch 3038: train loss 4.96400260925293 val loss 4.964067459106445\n",
      "Epoch 3039: train loss 4.964067459106445 val loss 4.963958263397217\n",
      "Epoch 3040: train loss 4.963958263397217 val loss 4.963868141174316\n",
      "Epoch 3041: train loss 4.963868141174316 val loss 4.963521957397461\n",
      "Epoch 3042: train loss 4.963521957397461 val loss 4.9638352394104\n",
      "Epoch 3043: train loss 4.9638352394104 val loss 4.96358585357666\n",
      "Epoch 3044: train loss 4.96358585357666 val loss 4.963506698608398\n",
      "Epoch 3045: train loss 4.963506698608398 val loss 4.963720321655273\n",
      "Epoch 3046: train loss 4.963720321655273 val loss 4.964357852935791\n",
      "Epoch 3047: train loss 4.964357852935791 val loss 4.963688373565674\n",
      "Epoch 3048: train loss 4.963688373565674 val loss 4.964850425720215\n",
      "Epoch 3049: train loss 4.964850425720215 val loss 4.963673114776611\n",
      "Epoch 3050: train loss 4.963673114776611 val loss 4.964870452880859\n",
      "Epoch 3051: train loss 4.964870452880859 val loss 4.963683128356934\n",
      "Epoch 3052: train loss 4.963683128356934 val loss 4.965970993041992\n",
      "Epoch 3053: train loss 4.965970993041992 val loss 4.964553356170654\n",
      "Epoch 3054: train loss 4.964553356170654 val loss 4.966376304626465\n",
      "Epoch 3055: train loss 4.966376304626465 val loss 4.967565059661865\n",
      "Epoch 3056: train loss 4.967565059661865 val loss 4.964970111846924\n",
      "Epoch 3057: train loss 4.964970111846924 val loss 4.966248989105225\n",
      "Epoch 3058: train loss 4.966248989105225 val loss 4.966107368469238\n",
      "Epoch 3059: train loss 4.966107368469238 val loss 4.964719772338867\n",
      "Epoch 3060: train loss 4.964719772338867 val loss 4.966190338134766\n",
      "Epoch 3061: train loss 4.966190338134766 val loss 4.963980674743652\n",
      "Epoch 3062: train loss 4.963980674743652 val loss 4.967268466949463\n",
      "Epoch 3063: train loss 4.967268466949463 val loss 4.966802597045898\n",
      "Epoch 3064: train loss 4.966802597045898 val loss 4.964415073394775\n",
      "Epoch 3065: train loss 4.964415073394775 val loss 4.965750217437744\n",
      "Epoch 3066: train loss 4.965750217437744 val loss 4.964138984680176\n",
      "Epoch 3067: train loss 4.964138984680176 val loss 4.96488094329834\n",
      "Epoch 3068: train loss 4.96488094329834 val loss 4.963841438293457\n",
      "Epoch 3069: train loss 4.963841438293457 val loss 4.964517116546631\n",
      "Epoch 3070: train loss 4.964517116546631 val loss 4.964125633239746\n",
      "Epoch 3071: train loss 4.964125633239746 val loss 4.965450763702393\n",
      "Epoch 3072: train loss 4.965450763702393 val loss 4.9647626876831055\n",
      "Epoch 3073: train loss 4.9647626876831055 val loss 4.964917182922363\n",
      "Epoch 3074: train loss 4.964917182922363 val loss 4.965245246887207\n",
      "Epoch 3075: train loss 4.965245246887207 val loss 4.963681221008301\n",
      "Epoch 3076: train loss 4.963681221008301 val loss 4.963709354400635\n",
      "Epoch 3077: train loss 4.963709354400635 val loss 4.963822841644287\n",
      "Epoch 3078: train loss 4.963822841644287 val loss 4.963865280151367\n",
      "Epoch 3079: train loss 4.963865280151367 val loss 4.963827610015869\n",
      "Epoch 3080: train loss 4.963827610015869 val loss 4.963453769683838\n",
      "Epoch 3081: train loss 4.963453769683838 val loss 4.963525772094727\n",
      "Epoch 3082: train loss 4.963525772094727 val loss 4.963586330413818\n",
      "Epoch 3083: train loss 4.963586330413818 val loss 4.964117050170898\n",
      "Epoch 3084: train loss 4.964117050170898 val loss 4.963716506958008\n",
      "Epoch 3085: train loss 4.963716506958008 val loss 4.96422004699707\n",
      "Epoch 3086: train loss 4.96422004699707 val loss 4.963957786560059\n",
      "Epoch 3087: train loss 4.963957786560059 val loss 4.964720726013184\n",
      "Epoch 3088: train loss 4.964720726013184 val loss 4.963720321655273\n",
      "Epoch 3089: train loss 4.963720321655273 val loss 4.9654693603515625\n",
      "Epoch 3090: train loss 4.9654693603515625 val loss 4.963523864746094\n",
      "Epoch 3091: train loss 4.963523864746094 val loss 4.9643025398254395\n",
      "Epoch 3092: train loss 4.9643025398254395 val loss 4.963486671447754\n",
      "Epoch 3093: train loss 4.963486671447754 val loss 4.964982509613037\n",
      "Epoch 3094: train loss 4.964982509613037 val loss 4.963457107543945\n",
      "Epoch 3095: train loss 4.963457107543945 val loss 4.965850353240967\n",
      "Epoch 3096: train loss 4.965850353240967 val loss 4.965435028076172\n",
      "Epoch 3097: train loss 4.965435028076172 val loss 4.964000225067139\n",
      "Epoch 3098: train loss 4.964000225067139 val loss 4.964599609375\n",
      "Epoch 3099: train loss 4.964599609375 val loss 4.964370250701904\n",
      "Epoch 3100: train loss 4.964370250701904 val loss 4.965024948120117\n",
      "Epoch 3101: train loss 4.965024948120117 val loss 4.963799953460693\n",
      "Epoch 3102: train loss 4.963799953460693 val loss 4.964859962463379\n",
      "Epoch 3103: train loss 4.964859962463379 val loss 4.963413238525391\n",
      "Epoch 3104: train loss 4.963413238525391 val loss 4.964077949523926\n",
      "Epoch 3105: train loss 4.964077949523926 val loss 4.963773727416992\n",
      "Epoch 3106: train loss 4.963773727416992 val loss 4.963494300842285\n",
      "Epoch 3107: train loss 4.963494300842285 val loss 4.963476181030273\n",
      "Epoch 3108: train loss 4.963476181030273 val loss 4.963371276855469\n",
      "Epoch 3109: train loss 4.963371276855469 val loss 4.963776588439941\n",
      "Epoch 3110: train loss 4.963776588439941 val loss 4.963807106018066\n",
      "Epoch 3111: train loss 4.963807106018066 val loss 4.963561534881592\n",
      "Epoch 3112: train loss 4.963561534881592 val loss 4.963611602783203\n",
      "Epoch 3113: train loss 4.963611602783203 val loss 4.96340274810791\n",
      "Epoch 3114: train loss 4.96340274810791 val loss 4.963654518127441\n",
      "Epoch 3115: train loss 4.963654518127441 val loss 4.964025020599365\n",
      "Epoch 3116: train loss 4.964025020599365 val loss 4.963830471038818\n",
      "Epoch 3117: train loss 4.963830471038818 val loss 4.963517189025879\n",
      "Epoch 3118: train loss 4.963517189025879 val loss 4.963581562042236\n",
      "Epoch 3119: train loss 4.963581562042236 val loss 4.963521480560303\n",
      "Epoch 3120: train loss 4.963521480560303 val loss 4.963613033294678\n",
      "Epoch 3121: train loss 4.963613033294678 val loss 4.963499069213867\n",
      "Epoch 3122: train loss 4.963499069213867 val loss 4.9637451171875\n",
      "Epoch 3123: train loss 4.9637451171875 val loss 4.963530540466309\n",
      "Epoch 3124: train loss 4.963530540466309 val loss 4.963497161865234\n",
      "Epoch 3125: train loss 4.963497161865234 val loss 4.9633893966674805\n",
      "Epoch 3126: train loss 4.9633893966674805 val loss 4.963866233825684\n",
      "Epoch 3127: train loss 4.963866233825684 val loss 4.9638166427612305\n",
      "Epoch 3128: train loss 4.9638166427612305 val loss 4.9636549949646\n",
      "Epoch 3129: train loss 4.9636549949646 val loss 4.964442253112793\n",
      "Epoch 3130: train loss 4.964442253112793 val loss 4.963497161865234\n",
      "Epoch 3131: train loss 4.963497161865234 val loss 4.963387489318848\n",
      "Epoch 3132: train loss 4.963387489318848 val loss 4.963825225830078\n",
      "Epoch 3133: train loss 4.963825225830078 val loss 4.963934898376465\n",
      "Epoch 3134: train loss 4.963934898376465 val loss 4.963658809661865\n",
      "Epoch 3135: train loss 4.963658809661865 val loss 4.9639081954956055\n",
      "Epoch 3136: train loss 4.9639081954956055 val loss 4.963600158691406\n",
      "Epoch 3137: train loss 4.963600158691406 val loss 4.963457107543945\n",
      "Epoch 3138: train loss 4.963457107543945 val loss 4.963576793670654\n",
      "Epoch 3139: train loss 4.963576793670654 val loss 4.963356971740723\n",
      "Epoch 3140: train loss 4.963356971740723 val loss 4.963770866394043\n",
      "Epoch 3141: train loss 4.963770866394043 val loss 4.964179992675781\n",
      "Epoch 3142: train loss 4.964179992675781 val loss 4.963596343994141\n",
      "Epoch 3143: train loss 4.963596343994141 val loss 4.964404582977295\n",
      "Epoch 3144: train loss 4.964404582977295 val loss 4.963478088378906\n",
      "Epoch 3145: train loss 4.963478088378906 val loss 4.963544845581055\n",
      "Epoch 3146: train loss 4.963544845581055 val loss 4.963454246520996\n",
      "Epoch 3147: train loss 4.963454246520996 val loss 4.963461875915527\n",
      "Epoch 3148: train loss 4.963461875915527 val loss 4.964844226837158\n",
      "Epoch 3149: train loss 4.964844226837158 val loss 4.9639787673950195\n",
      "Epoch 3150: train loss 4.9639787673950195 val loss 4.965707302093506\n",
      "Epoch 3151: train loss 4.965707302093506 val loss 4.964376449584961\n",
      "Epoch 3152: train loss 4.964376449584961 val loss 4.965476989746094\n",
      "Epoch 3153: train loss 4.965476989746094 val loss 4.966150283813477\n",
      "Epoch 3154: train loss 4.966150283813477 val loss 4.963749885559082\n",
      "Epoch 3155: train loss 4.963749885559082 val loss 4.966726303100586\n",
      "Epoch 3156: train loss 4.966726303100586 val loss 4.965052127838135\n",
      "Epoch 3157: train loss 4.965052127838135 val loss 4.965785026550293\n",
      "Epoch 3158: train loss 4.965785026550293 val loss 4.967019557952881\n",
      "Epoch 3159: train loss 4.967019557952881 val loss 4.964500904083252\n",
      "Epoch 3160: train loss 4.964500904083252 val loss 4.9673686027526855\n",
      "Epoch 3161: train loss 4.9673686027526855 val loss 4.967916488647461\n",
      "Epoch 3162: train loss 4.967916488647461 val loss 4.963825225830078\n",
      "Epoch 3163: train loss 4.963825225830078 val loss 4.966760635375977\n",
      "Epoch 3164: train loss 4.966760635375977 val loss 4.966814994812012\n",
      "Epoch 3165: train loss 4.966814994812012 val loss 4.963613986968994\n",
      "Epoch 3166: train loss 4.963613986968994 val loss 4.966890335083008\n",
      "Epoch 3167: train loss 4.966890335083008 val loss 4.965773105621338\n",
      "Epoch 3168: train loss 4.965773105621338 val loss 4.964778900146484\n",
      "Epoch 3169: train loss 4.964778900146484 val loss 4.96631383895874\n",
      "Epoch 3170: train loss 4.96631383895874 val loss 4.964550018310547\n",
      "Epoch 3171: train loss 4.964550018310547 val loss 4.965906143188477\n",
      "Epoch 3172: train loss 4.965906143188477 val loss 4.966436386108398\n",
      "Epoch 3173: train loss 4.966436386108398 val loss 4.96382474899292\n",
      "Epoch 3174: train loss 4.96382474899292 val loss 4.9646477699279785\n",
      "Epoch 3175: train loss 4.9646477699279785 val loss 4.963696479797363\n",
      "Epoch 3176: train loss 4.963696479797363 val loss 4.965514659881592\n",
      "Epoch 3177: train loss 4.965514659881592 val loss 4.964271068572998\n",
      "Epoch 3178: train loss 4.964271068572998 val loss 4.965298175811768\n",
      "Epoch 3179: train loss 4.965298175811768 val loss 4.965479850769043\n",
      "Epoch 3180: train loss 4.965479850769043 val loss 4.96360969543457\n",
      "Epoch 3181: train loss 4.96360969543457 val loss 4.966595649719238\n",
      "Epoch 3182: train loss 4.966595649719238 val loss 4.964158535003662\n",
      "Epoch 3183: train loss 4.964158535003662 val loss 4.966578483581543\n",
      "Epoch 3184: train loss 4.966578483581543 val loss 4.967916488647461\n",
      "Epoch 3185: train loss 4.967916488647461 val loss 4.9655327796936035\n",
      "Epoch 3186: train loss 4.9655327796936035 val loss 4.965487480163574\n",
      "Epoch 3187: train loss 4.965487480163574 val loss 4.966650009155273\n",
      "Epoch 3188: train loss 4.966650009155273 val loss 4.964271545410156\n",
      "Epoch 3189: train loss 4.964271545410156 val loss 4.9658942222595215\n",
      "Epoch 3190: train loss 4.9658942222595215 val loss 4.965205192565918\n",
      "Epoch 3191: train loss 4.965205192565918 val loss 4.9637908935546875\n",
      "Epoch 3192: train loss 4.9637908935546875 val loss 4.9662604331970215\n",
      "Epoch 3193: train loss 4.9662604331970215 val loss 4.963683128356934\n",
      "Epoch 3194: train loss 4.963683128356934 val loss 4.967245578765869\n",
      "Epoch 3195: train loss 4.967245578765869 val loss 4.968257427215576\n",
      "Epoch 3196: train loss 4.968257427215576 val loss 4.965599536895752\n",
      "Epoch 3197: train loss 4.965599536895752 val loss 4.966536998748779\n",
      "Epoch 3198: train loss 4.966536998748779 val loss 4.967625617980957\n",
      "Epoch 3199: train loss 4.967625617980957 val loss 4.963993072509766\n",
      "Epoch 3200: train loss 4.963993072509766 val loss 4.966470718383789\n",
      "Epoch 3201: train loss 4.966470718383789 val loss 4.966890335083008\n",
      "Epoch 3202: train loss 4.966890335083008 val loss 4.964325904846191\n",
      "Epoch 3203: train loss 4.964325904846191 val loss 4.966468811035156\n",
      "Epoch 3204: train loss 4.966468811035156 val loss 4.966609954833984\n",
      "Epoch 3205: train loss 4.966609954833984 val loss 4.9643754959106445\n",
      "Epoch 3206: train loss 4.9643754959106445 val loss 4.9658918380737305\n",
      "Epoch 3207: train loss 4.9658918380737305 val loss 4.9648332595825195\n",
      "Epoch 3208: train loss 4.9648332595825195 val loss 4.96512508392334\n",
      "Epoch 3209: train loss 4.96512508392334 val loss 4.964594841003418\n",
      "Epoch 3210: train loss 4.964594841003418 val loss 4.964642524719238\n",
      "Epoch 3211: train loss 4.964642524719238 val loss 4.9647345542907715\n",
      "Epoch 3212: train loss 4.9647345542907715 val loss 4.963932991027832\n",
      "Epoch 3213: train loss 4.963932991027832 val loss 4.964515686035156\n",
      "Epoch 3214: train loss 4.964515686035156 val loss 4.964277267456055\n",
      "Epoch 3215: train loss 4.964277267456055 val loss 4.964527130126953\n",
      "Epoch 3216: train loss 4.964527130126953 val loss 4.963841438293457\n",
      "Epoch 3217: train loss 4.963841438293457 val loss 4.963450908660889\n",
      "Epoch 3218: train loss 4.963450908660889 val loss 4.963507175445557\n",
      "Epoch 3219: train loss 4.963507175445557 val loss 4.963418483734131\n",
      "Epoch 3220: train loss 4.963418483734131 val loss 4.963536262512207\n",
      "Epoch 3221: train loss 4.963536262512207 val loss 4.9633684158325195\n",
      "Epoch 3222: train loss 4.9633684158325195 val loss 4.963336944580078\n",
      "Epoch 3223: train loss 4.963336944580078 val loss 4.963285446166992\n",
      "Epoch 3224: train loss 4.963285446166992 val loss 4.964158058166504\n",
      "Epoch 3225: train loss 4.964158058166504 val loss 4.963379859924316\n",
      "Epoch 3226: train loss 4.963379859924316 val loss 4.963713645935059\n",
      "Epoch 3227: train loss 4.963713645935059 val loss 4.963662624359131\n",
      "Epoch 3228: train loss 4.963662624359131 val loss 4.963395118713379\n",
      "Epoch 3229: train loss 4.963395118713379 val loss 4.963446617126465\n",
      "Epoch 3230: train loss 4.963446617126465 val loss 4.964089393615723\n",
      "Epoch 3231: train loss 4.964089393615723 val loss 4.963305950164795\n",
      "Epoch 3232: train loss 4.963305950164795 val loss 4.963832378387451\n",
      "Epoch 3233: train loss 4.963832378387451 val loss 4.963771343231201\n",
      "Epoch 3234: train loss 4.963771343231201 val loss 4.963297367095947\n",
      "Epoch 3235: train loss 4.963297367095947 val loss 4.963325500488281\n",
      "Epoch 3236: train loss 4.963325500488281 val loss 4.96339225769043\n",
      "Epoch 3237: train loss 4.96339225769043 val loss 4.963418006896973\n",
      "Epoch 3238: train loss 4.963418006896973 val loss 4.963476657867432\n",
      "Epoch 3239: train loss 4.963476657867432 val loss 4.9633870124816895\n",
      "Epoch 3240: train loss 4.9633870124816895 val loss 4.964047431945801\n",
      "Epoch 3241: train loss 4.964047431945801 val loss 4.9632978439331055\n",
      "Epoch 3242: train loss 4.9632978439331055 val loss 4.963163375854492\n",
      "Epoch 3243: train loss 4.963163375854492 val loss 4.964420318603516\n",
      "Epoch 3244: train loss 4.964420318603516 val loss 4.963327884674072\n",
      "Epoch 3245: train loss 4.963327884674072 val loss 4.963896751403809\n",
      "Epoch 3246: train loss 4.963896751403809 val loss 4.963323593139648\n",
      "Epoch 3247: train loss 4.963323593139648 val loss 4.964233875274658\n",
      "Epoch 3248: train loss 4.964233875274658 val loss 4.963427543640137\n",
      "Epoch 3249: train loss 4.963427543640137 val loss 4.9632344245910645\n",
      "Epoch 3250: train loss 4.9632344245910645 val loss 4.964204788208008\n",
      "Epoch 3251: train loss 4.964204788208008 val loss 4.963190078735352\n",
      "Epoch 3252: train loss 4.963190078735352 val loss 4.963831901550293\n",
      "Epoch 3253: train loss 4.963831901550293 val loss 4.963396072387695\n",
      "Epoch 3254: train loss 4.963396072387695 val loss 4.9645586013793945\n",
      "Epoch 3255: train loss 4.9645586013793945 val loss 4.963803291320801\n",
      "Epoch 3256: train loss 4.963803291320801 val loss 4.963897228240967\n",
      "Epoch 3257: train loss 4.963897228240967 val loss 4.963502407073975\n",
      "Epoch 3258: train loss 4.963502407073975 val loss 4.963846683502197\n",
      "Epoch 3259: train loss 4.963846683502197 val loss 4.964172840118408\n",
      "Epoch 3260: train loss 4.964172840118408 val loss 4.963850975036621\n",
      "Epoch 3261: train loss 4.963850975036621 val loss 4.964832782745361\n",
      "Epoch 3262: train loss 4.964832782745361 val loss 4.963894844055176\n",
      "Epoch 3263: train loss 4.963894844055176 val loss 4.964954376220703\n",
      "Epoch 3264: train loss 4.964954376220703 val loss 4.964835166931152\n",
      "Epoch 3265: train loss 4.964835166931152 val loss 4.963751792907715\n",
      "Epoch 3266: train loss 4.963751792907715 val loss 4.965362071990967\n",
      "Epoch 3267: train loss 4.965362071990967 val loss 4.963478088378906\n",
      "Epoch 3268: train loss 4.963478088378906 val loss 4.9656662940979\n",
      "Epoch 3269: train loss 4.9656662940979 val loss 4.965183258056641\n",
      "Epoch 3270: train loss 4.965183258056641 val loss 4.963499069213867\n",
      "Epoch 3271: train loss 4.963499069213867 val loss 4.964262008666992\n",
      "Epoch 3272: train loss 4.964262008666992 val loss 4.963756084442139\n",
      "Epoch 3273: train loss 4.963756084442139 val loss 4.963602066040039\n",
      "Epoch 3274: train loss 4.963602066040039 val loss 4.964127540588379\n",
      "Epoch 3275: train loss 4.964127540588379 val loss 4.96339750289917\n",
      "Epoch 3276: train loss 4.96339750289917 val loss 4.963324546813965\n",
      "Epoch 3277: train loss 4.963324546813965 val loss 4.963298320770264\n",
      "Epoch 3278: train loss 4.963298320770264 val loss 4.963199138641357\n",
      "Epoch 3279: train loss 4.963199138641357 val loss 4.963115215301514\n",
      "Epoch 3280: train loss 4.963115215301514 val loss 4.96304988861084\n",
      "Epoch 3281: train loss 4.96304988861084 val loss 4.963191986083984\n",
      "Epoch 3282: train loss 4.963191986083984 val loss 4.963992118835449\n",
      "Epoch 3283: train loss 4.963992118835449 val loss 4.963507175445557\n",
      "Epoch 3284: train loss 4.963507175445557 val loss 4.963441848754883\n",
      "Epoch 3285: train loss 4.963441848754883 val loss 4.963990688323975\n",
      "Epoch 3286: train loss 4.963990688323975 val loss 4.963461399078369\n",
      "Epoch 3287: train loss 4.963461399078369 val loss 4.9636149406433105\n",
      "Epoch 3288: train loss 4.9636149406433105 val loss 4.963791847229004\n",
      "Epoch 3289: train loss 4.963791847229004 val loss 4.963117599487305\n",
      "Epoch 3290: train loss 4.963117599487305 val loss 4.963848114013672\n",
      "Epoch 3291: train loss 4.963848114013672 val loss 4.963229656219482\n",
      "Epoch 3292: train loss 4.963229656219482 val loss 4.964069366455078\n",
      "Epoch 3293: train loss 4.964069366455078 val loss 4.963608741760254\n",
      "Epoch 3294: train loss 4.963608741760254 val loss 4.963805198669434\n",
      "Epoch 3295: train loss 4.963805198669434 val loss 4.9633684158325195\n",
      "Epoch 3296: train loss 4.9633684158325195 val loss 4.963791847229004\n",
      "Epoch 3297: train loss 4.963791847229004 val loss 4.963844299316406\n",
      "Epoch 3298: train loss 4.963844299316406 val loss 4.963569641113281\n",
      "Epoch 3299: train loss 4.963569641113281 val loss 4.964404582977295\n",
      "Epoch 3300: train loss 4.964404582977295 val loss 4.9631242752075195\n",
      "Epoch 3301: train loss 4.9631242752075195 val loss 4.964473724365234\n",
      "Epoch 3302: train loss 4.964473724365234 val loss 4.963670253753662\n",
      "Epoch 3303: train loss 4.963670253753662 val loss 4.965476036071777\n",
      "Epoch 3304: train loss 4.965476036071777 val loss 4.964675426483154\n",
      "Epoch 3305: train loss 4.964675426483154 val loss 4.964747905731201\n",
      "Epoch 3306: train loss 4.964747905731201 val loss 4.965121269226074\n",
      "Epoch 3307: train loss 4.965121269226074 val loss 4.963377475738525\n",
      "Epoch 3308: train loss 4.963377475738525 val loss 4.964787483215332\n",
      "Epoch 3309: train loss 4.964787483215332 val loss 4.9635491371154785\n",
      "Epoch 3310: train loss 4.9635491371154785 val loss 4.963619709014893\n",
      "Epoch 3311: train loss 4.963619709014893 val loss 4.96356201171875\n",
      "Epoch 3312: train loss 4.96356201171875 val loss 4.963315963745117\n",
      "Epoch 3313: train loss 4.963315963745117 val loss 4.9633283615112305\n",
      "Epoch 3314: train loss 4.9633283615112305 val loss 4.963589191436768\n",
      "Epoch 3315: train loss 4.963589191436768 val loss 4.963295936584473\n",
      "Epoch 3316: train loss 4.963295936584473 val loss 4.963269233703613\n",
      "Epoch 3317: train loss 4.963269233703613 val loss 4.963807582855225\n",
      "Epoch 3318: train loss 4.963807582855225 val loss 4.963499546051025\n",
      "Epoch 3319: train loss 4.963499546051025 val loss 4.963974952697754\n",
      "Epoch 3320: train loss 4.963974952697754 val loss 4.963332653045654\n",
      "Epoch 3321: train loss 4.963332653045654 val loss 4.964041233062744\n",
      "Epoch 3322: train loss 4.964041233062744 val loss 4.964227199554443\n",
      "Epoch 3323: train loss 4.964227199554443 val loss 4.964334487915039\n",
      "Epoch 3324: train loss 4.964334487915039 val loss 4.963919162750244\n",
      "Epoch 3325: train loss 4.963919162750244 val loss 4.963735103607178\n",
      "Epoch 3326: train loss 4.963735103607178 val loss 4.963442325592041\n",
      "Epoch 3327: train loss 4.963442325592041 val loss 4.9634199142456055\n",
      "Epoch 3328: train loss 4.9634199142456055 val loss 4.963567733764648\n",
      "Epoch 3329: train loss 4.963567733764648 val loss 4.963561058044434\n",
      "Epoch 3330: train loss 4.963561058044434 val loss 4.963240146636963\n",
      "Epoch 3331: train loss 4.963240146636963 val loss 4.963037490844727\n",
      "Epoch 3332: train loss 4.963037490844727 val loss 4.963710784912109\n",
      "Epoch 3333: train loss 4.963710784912109 val loss 4.96368408203125\n",
      "Epoch 3334: train loss 4.96368408203125 val loss 4.963502883911133\n",
      "Epoch 3335: train loss 4.963502883911133 val loss 4.963775634765625\n",
      "Epoch 3336: train loss 4.963775634765625 val loss 4.963114261627197\n",
      "Epoch 3337: train loss 4.963114261627197 val loss 4.963111400604248\n",
      "Epoch 3338: train loss 4.963111400604248 val loss 4.963460445404053\n",
      "Epoch 3339: train loss 4.963460445404053 val loss 4.963434219360352\n",
      "Epoch 3340: train loss 4.963434219360352 val loss 4.963249206542969\n",
      "Epoch 3341: train loss 4.963249206542969 val loss 4.963022232055664\n",
      "Epoch 3342: train loss 4.963022232055664 val loss 4.963319778442383\n",
      "Epoch 3343: train loss 4.963319778442383 val loss 4.963441848754883\n",
      "Epoch 3344: train loss 4.963441848754883 val loss 4.963431358337402\n",
      "Epoch 3345: train loss 4.963431358337402 val loss 4.963425636291504\n",
      "Epoch 3346: train loss 4.963425636291504 val loss 4.963186264038086\n",
      "Epoch 3347: train loss 4.963186264038086 val loss 4.963198184967041\n",
      "Epoch 3348: train loss 4.963198184967041 val loss 4.963282108306885\n",
      "Epoch 3349: train loss 4.963282108306885 val loss 4.963399887084961\n",
      "Epoch 3350: train loss 4.963399887084961 val loss 4.963068962097168\n",
      "Epoch 3351: train loss 4.963068962097168 val loss 4.962924480438232\n",
      "Epoch 3352: train loss 4.962924480438232 val loss 4.963881015777588\n",
      "Epoch 3353: train loss 4.963881015777588 val loss 4.963110446929932\n",
      "Epoch 3354: train loss 4.963110446929932 val loss 4.9629716873168945\n",
      "Epoch 3355: train loss 4.9629716873168945 val loss 4.963061809539795\n",
      "Epoch 3356: train loss 4.963061809539795 val loss 4.963082790374756\n",
      "Epoch 3357: train loss 4.963082790374756 val loss 4.963059425354004\n",
      "Epoch 3358: train loss 4.963059425354004 val loss 4.963022232055664\n",
      "Epoch 3359: train loss 4.963022232055664 val loss 4.9634552001953125\n",
      "Epoch 3360: train loss 4.9634552001953125 val loss 4.96370267868042\n",
      "Epoch 3361: train loss 4.96370267868042 val loss 4.963034152984619\n",
      "Epoch 3362: train loss 4.963034152984619 val loss 4.964800834655762\n",
      "Epoch 3363: train loss 4.964800834655762 val loss 4.962922096252441\n",
      "Epoch 3364: train loss 4.962922096252441 val loss 4.964249134063721\n",
      "Epoch 3365: train loss 4.964249134063721 val loss 4.963021278381348\n",
      "Epoch 3366: train loss 4.963021278381348 val loss 4.965567111968994\n",
      "Epoch 3367: train loss 4.965567111968994 val loss 4.96410608291626\n",
      "Epoch 3368: train loss 4.96410608291626 val loss 4.965975761413574\n",
      "Epoch 3369: train loss 4.965975761413574 val loss 4.9671807289123535\n",
      "Epoch 3370: train loss 4.9671807289123535 val loss 4.964689254760742\n",
      "Epoch 3371: train loss 4.964689254760742 val loss 4.966175079345703\n",
      "Epoch 3372: train loss 4.966175079345703 val loss 4.967559814453125\n",
      "Epoch 3373: train loss 4.967559814453125 val loss 4.963364601135254\n",
      "Epoch 3374: train loss 4.963364601135254 val loss 4.965834617614746\n",
      "Epoch 3375: train loss 4.965834617614746 val loss 4.9659929275512695\n",
      "Epoch 3376: train loss 4.9659929275512695 val loss 4.963338375091553\n",
      "Epoch 3377: train loss 4.963338375091553 val loss 4.967623710632324\n",
      "Epoch 3378: train loss 4.967623710632324 val loss 4.967052459716797\n",
      "Epoch 3379: train loss 4.967052459716797 val loss 4.963774681091309\n",
      "Epoch 3380: train loss 4.963774681091309 val loss 4.965961456298828\n",
      "Epoch 3381: train loss 4.965961456298828 val loss 4.964857578277588\n",
      "Epoch 3382: train loss 4.964857578277588 val loss 4.964048862457275\n",
      "Epoch 3383: train loss 4.964048862457275 val loss 4.964191436767578\n",
      "Epoch 3384: train loss 4.964191436767578 val loss 4.964099884033203\n",
      "Epoch 3385: train loss 4.964099884033203 val loss 4.964218616485596\n",
      "Epoch 3386: train loss 4.964218616485596 val loss 4.963482856750488\n",
      "Epoch 3387: train loss 4.963482856750488 val loss 4.963391304016113\n",
      "Epoch 3388: train loss 4.963391304016113 val loss 4.963778495788574\n",
      "Epoch 3389: train loss 4.963778495788574 val loss 4.963374137878418\n",
      "Epoch 3390: train loss 4.963374137878418 val loss 4.96415376663208\n",
      "Epoch 3391: train loss 4.96415376663208 val loss 4.963456153869629\n",
      "Epoch 3392: train loss 4.963456153869629 val loss 4.964178085327148\n",
      "Epoch 3393: train loss 4.964178085327148 val loss 4.963305473327637\n",
      "Epoch 3394: train loss 4.963305473327637 val loss 4.9639201164245605\n",
      "Epoch 3395: train loss 4.9639201164245605 val loss 4.963177680969238\n",
      "Epoch 3396: train loss 4.963177680969238 val loss 4.963831901550293\n",
      "Epoch 3397: train loss 4.963831901550293 val loss 4.963376522064209\n",
      "Epoch 3398: train loss 4.963376522064209 val loss 4.963709831237793\n",
      "Epoch 3399: train loss 4.963709831237793 val loss 4.963325500488281\n",
      "Epoch 3400: train loss 4.963325500488281 val loss 4.962947845458984\n",
      "Epoch 3401: train loss 4.962947845458984 val loss 4.963078498840332\n",
      "Epoch 3402: train loss 4.963078498840332 val loss 4.963205337524414\n",
      "Epoch 3403: train loss 4.963205337524414 val loss 4.963510036468506\n",
      "Epoch 3404: train loss 4.963510036468506 val loss 4.963309288024902\n",
      "Epoch 3405: train loss 4.963309288024902 val loss 4.962880611419678\n",
      "Epoch 3406: train loss 4.962880611419678 val loss 4.963828086853027\n",
      "Epoch 3407: train loss 4.963828086853027 val loss 4.963252067565918\n",
      "Epoch 3408: train loss 4.963252067565918 val loss 4.963099479675293\n",
      "Epoch 3409: train loss 4.963099479675293 val loss 4.963554382324219\n",
      "Epoch 3410: train loss 4.963554382324219 val loss 4.963220119476318\n",
      "Epoch 3411: train loss 4.963220119476318 val loss 4.963338851928711\n",
      "Epoch 3412: train loss 4.963338851928711 val loss 4.963493347167969\n",
      "Epoch 3413: train loss 4.963493347167969 val loss 4.963410377502441\n",
      "Epoch 3414: train loss 4.963410377502441 val loss 4.963235378265381\n",
      "Epoch 3415: train loss 4.963235378265381 val loss 4.963418483734131\n",
      "Epoch 3416: train loss 4.963418483734131 val loss 4.962925910949707\n",
      "Epoch 3417: train loss 4.962925910949707 val loss 4.962924957275391\n",
      "Epoch 3418: train loss 4.962924957275391 val loss 4.963042259216309\n",
      "Epoch 3419: train loss 4.963042259216309 val loss 4.964109897613525\n",
      "Epoch 3420: train loss 4.964109897613525 val loss 4.9629998207092285\n",
      "Epoch 3421: train loss 4.9629998207092285 val loss 4.963450908660889\n",
      "Epoch 3422: train loss 4.963450908660889 val loss 4.962862491607666\n",
      "Epoch 3423: train loss 4.962862491607666 val loss 4.9630866050720215\n",
      "Epoch 3424: train loss 4.9630866050720215 val loss 4.963041305541992\n",
      "Epoch 3425: train loss 4.963041305541992 val loss 4.963119029998779\n",
      "Epoch 3426: train loss 4.963119029998779 val loss 4.963016033172607\n",
      "Epoch 3427: train loss 4.963016033172607 val loss 4.962974548339844\n",
      "Epoch 3428: train loss 4.962974548339844 val loss 4.962939262390137\n",
      "Epoch 3429: train loss 4.962939262390137 val loss 4.962868690490723\n",
      "Epoch 3430: train loss 4.962868690490723 val loss 4.962801933288574\n",
      "Epoch 3431: train loss 4.962801933288574 val loss 4.963164806365967\n",
      "Epoch 3432: train loss 4.963164806365967 val loss 4.9637274742126465\n",
      "Epoch 3433: train loss 4.9637274742126465 val loss 4.963682651519775\n",
      "Epoch 3434: train loss 4.963682651519775 val loss 4.9631452560424805\n",
      "Epoch 3435: train loss 4.9631452560424805 val loss 4.963139533996582\n",
      "Epoch 3436: train loss 4.963139533996582 val loss 4.962954044342041\n",
      "Epoch 3437: train loss 4.962954044342041 val loss 4.962724208831787\n",
      "Epoch 3438: train loss 4.962724208831787 val loss 4.962923049926758\n",
      "Epoch 3439: train loss 4.962923049926758 val loss 4.963404178619385\n",
      "Epoch 3440: train loss 4.963404178619385 val loss 4.963702201843262\n",
      "Epoch 3441: train loss 4.963702201843262 val loss 4.9628190994262695\n",
      "Epoch 3442: train loss 4.9628190994262695 val loss 4.963012218475342\n",
      "Epoch 3443: train loss 4.963012218475342 val loss 4.964064121246338\n",
      "Epoch 3444: train loss 4.964064121246338 val loss 4.963273048400879\n",
      "Epoch 3445: train loss 4.963273048400879 val loss 4.963379383087158\n",
      "Epoch 3446: train loss 4.963379383087158 val loss 4.963081359863281\n",
      "Epoch 3447: train loss 4.963081359863281 val loss 4.963320732116699\n",
      "Epoch 3448: train loss 4.963320732116699 val loss 4.963422775268555\n",
      "Epoch 3449: train loss 4.963422775268555 val loss 4.96329402923584\n",
      "Epoch 3450: train loss 4.96329402923584 val loss 4.963200092315674\n",
      "Epoch 3451: train loss 4.963200092315674 val loss 4.963048458099365\n",
      "Epoch 3452: train loss 4.963048458099365 val loss 4.963046550750732\n",
      "Epoch 3453: train loss 4.963046550750732 val loss 4.963314056396484\n",
      "Epoch 3454: train loss 4.963314056396484 val loss 4.963092803955078\n",
      "Epoch 3455: train loss 4.963092803955078 val loss 4.963801383972168\n",
      "Epoch 3456: train loss 4.963801383972168 val loss 4.963160514831543\n",
      "Epoch 3457: train loss 4.963160514831543 val loss 4.963231086730957\n",
      "Epoch 3458: train loss 4.963231086730957 val loss 4.96353816986084\n",
      "Epoch 3459: train loss 4.96353816986084 val loss 4.962821960449219\n",
      "Epoch 3460: train loss 4.962821960449219 val loss 4.963003158569336\n",
      "Epoch 3461: train loss 4.963003158569336 val loss 4.963741302490234\n",
      "Epoch 3462: train loss 4.963741302490234 val loss 4.963144302368164\n",
      "Epoch 3463: train loss 4.963144302368164 val loss 4.962778091430664\n",
      "Epoch 3464: train loss 4.962778091430664 val loss 4.963044166564941\n",
      "Epoch 3465: train loss 4.963044166564941 val loss 4.963535308837891\n",
      "Epoch 3466: train loss 4.963535308837891 val loss 4.962809085845947\n",
      "Epoch 3467: train loss 4.962809085845947 val loss 4.964542388916016\n",
      "Epoch 3468: train loss 4.964542388916016 val loss 4.962950229644775\n",
      "Epoch 3469: train loss 4.962950229644775 val loss 4.964565277099609\n",
      "Epoch 3470: train loss 4.964565277099609 val loss 4.963480472564697\n",
      "Epoch 3471: train loss 4.963480472564697 val loss 4.965706825256348\n",
      "Epoch 3472: train loss 4.965706825256348 val loss 4.965581893920898\n",
      "Epoch 3473: train loss 4.965581893920898 val loss 4.963342666625977\n",
      "Epoch 3474: train loss 4.963342666625977 val loss 4.964148998260498\n",
      "Epoch 3475: train loss 4.964148998260498 val loss 4.9630045890808105\n",
      "Epoch 3476: train loss 4.9630045890808105 val loss 4.962827682495117\n",
      "Epoch 3477: train loss 4.962827682495117 val loss 4.963321685791016\n",
      "Epoch 3478: train loss 4.963321685791016 val loss 4.962796211242676\n",
      "Epoch 3479: train loss 4.962796211242676 val loss 4.962824821472168\n",
      "Epoch 3480: train loss 4.962824821472168 val loss 4.962722301483154\n",
      "Epoch 3481: train loss 4.962722301483154 val loss 4.962808609008789\n",
      "Epoch 3482: train loss 4.962808609008789 val loss 4.962737560272217\n",
      "Epoch 3483: train loss 4.962737560272217 val loss 4.962880611419678\n",
      "Epoch 3484: train loss 4.962880611419678 val loss 4.963855266571045\n",
      "Epoch 3485: train loss 4.963855266571045 val loss 4.962690830230713\n",
      "Epoch 3486: train loss 4.962690830230713 val loss 4.96269416809082\n",
      "Epoch 3487: train loss 4.96269416809082 val loss 4.962836265563965\n",
      "Epoch 3488: train loss 4.962836265563965 val loss 4.963764190673828\n",
      "Epoch 3489: train loss 4.963764190673828 val loss 4.962843418121338\n",
      "Epoch 3490: train loss 4.962843418121338 val loss 4.96541690826416\n",
      "Epoch 3491: train loss 4.96541690826416 val loss 4.964325428009033\n",
      "Epoch 3492: train loss 4.964325428009033 val loss 4.964879512786865\n",
      "Epoch 3493: train loss 4.964879512786865 val loss 4.965510368347168\n",
      "Epoch 3494: train loss 4.965510368347168 val loss 4.962900638580322\n",
      "Epoch 3495: train loss 4.962900638580322 val loss 4.966036796569824\n",
      "Epoch 3496: train loss 4.966036796569824 val loss 4.964344024658203\n",
      "Epoch 3497: train loss 4.964344024658203 val loss 4.965426445007324\n",
      "Epoch 3498: train loss 4.965426445007324 val loss 4.966718673706055\n",
      "Epoch 3499: train loss 4.966718673706055 val loss 4.964317321777344\n",
      "Epoch 3500: train loss 4.964317321777344 val loss 4.965992450714111\n",
      "Epoch 3501: train loss 4.965992450714111 val loss 4.966436386108398\n",
      "Epoch 3502: train loss 4.966436386108398 val loss 4.963113307952881\n",
      "Epoch 3503: train loss 4.963113307952881 val loss 4.965381145477295\n",
      "Epoch 3504: train loss 4.965381145477295 val loss 4.964271545410156\n",
      "Epoch 3505: train loss 4.964271545410156 val loss 4.964104175567627\n",
      "Epoch 3506: train loss 4.964104175567627 val loss 4.9635772705078125\n",
      "Epoch 3507: train loss 4.9635772705078125 val loss 4.965160846710205\n",
      "Epoch 3508: train loss 4.965160846710205 val loss 4.965727806091309\n",
      "Epoch 3509: train loss 4.965727806091309 val loss 4.96290397644043\n",
      "Epoch 3510: train loss 4.96290397644043 val loss 4.967349529266357\n",
      "Epoch 3511: train loss 4.967349529266357 val loss 4.9671311378479\n",
      "Epoch 3512: train loss 4.9671311378479 val loss 4.962894916534424\n",
      "Epoch 3513: train loss 4.962894916534424 val loss 4.96474552154541\n",
      "Epoch 3514: train loss 4.96474552154541 val loss 4.9632110595703125\n",
      "Epoch 3515: train loss 4.9632110595703125 val loss 4.965798377990723\n",
      "Epoch 3516: train loss 4.965798377990723 val loss 4.964851379394531\n",
      "Epoch 3517: train loss 4.964851379394531 val loss 4.964289665222168\n",
      "Epoch 3518: train loss 4.964289665222168 val loss 4.96511173248291\n",
      "Epoch 3519: train loss 4.96511173248291 val loss 4.962886810302734\n",
      "Epoch 3520: train loss 4.962886810302734 val loss 4.965912342071533\n",
      "Epoch 3521: train loss 4.965912342071533 val loss 4.9643874168396\n",
      "Epoch 3522: train loss 4.9643874168396 val loss 4.965084552764893\n",
      "Epoch 3523: train loss 4.965084552764893 val loss 4.9663262367248535\n",
      "Epoch 3524: train loss 4.9663262367248535 val loss 4.963927745819092\n",
      "Epoch 3525: train loss 4.963927745819092 val loss 4.966249942779541\n",
      "Epoch 3526: train loss 4.966249942779541 val loss 4.966742515563965\n",
      "Epoch 3527: train loss 4.966742515563965 val loss 4.962737560272217\n",
      "Epoch 3528: train loss 4.962737560272217 val loss 4.964888572692871\n",
      "Epoch 3529: train loss 4.964888572692871 val loss 4.963757514953613\n",
      "Epoch 3530: train loss 4.963757514953613 val loss 4.9650044441223145\n",
      "Epoch 3531: train loss 4.9650044441223145 val loss 4.964960098266602\n",
      "Epoch 3532: train loss 4.964960098266602 val loss 4.963456153869629\n",
      "Epoch 3533: train loss 4.963456153869629 val loss 4.9642252922058105\n",
      "Epoch 3534: train loss 4.9642252922058105 val loss 4.962774276733398\n",
      "Epoch 3535: train loss 4.962774276733398 val loss 4.962858200073242\n",
      "Epoch 3536: train loss 4.962858200073242 val loss 4.964025497436523\n",
      "Epoch 3537: train loss 4.964025497436523 val loss 4.963013648986816\n",
      "Epoch 3538: train loss 4.963013648986816 val loss 4.965009689331055\n",
      "Epoch 3539: train loss 4.965009689331055 val loss 4.964226722717285\n",
      "Epoch 3540: train loss 4.964226722717285 val loss 4.964498519897461\n",
      "Epoch 3541: train loss 4.964498519897461 val loss 4.965217590332031\n",
      "Epoch 3542: train loss 4.965217590332031 val loss 4.963230609893799\n",
      "Epoch 3543: train loss 4.963230609893799 val loss 4.965050220489502\n",
      "Epoch 3544: train loss 4.965050220489502 val loss 4.96328067779541\n",
      "Epoch 3545: train loss 4.96328067779541 val loss 4.964134693145752\n",
      "Epoch 3546: train loss 4.964134693145752 val loss 4.963590621948242\n",
      "Epoch 3547: train loss 4.963590621948242 val loss 4.964540004730225\n",
      "Epoch 3548: train loss 4.964540004730225 val loss 4.964381694793701\n",
      "Epoch 3549: train loss 4.964381694793701 val loss 4.96380090713501\n",
      "Epoch 3550: train loss 4.96380090713501 val loss 4.964037895202637\n",
      "Epoch 3551: train loss 4.964037895202637 val loss 4.96323823928833\n",
      "Epoch 3552: train loss 4.96323823928833 val loss 4.962790012359619\n",
      "Epoch 3553: train loss 4.962790012359619 val loss 4.9635467529296875\n",
      "Epoch 3554: train loss 4.9635467529296875 val loss 4.962766170501709\n",
      "Epoch 3555: train loss 4.962766170501709 val loss 4.962906837463379\n",
      "Epoch 3556: train loss 4.962906837463379 val loss 4.962717533111572\n",
      "Epoch 3557: train loss 4.962717533111572 val loss 4.962704658508301\n",
      "Epoch 3558: train loss 4.962704658508301 val loss 4.962625503540039\n",
      "Epoch 3559: train loss 4.962625503540039 val loss 4.962911605834961\n",
      "Epoch 3560: train loss 4.962911605834961 val loss 4.963033676147461\n",
      "Epoch 3561: train loss 4.963033676147461 val loss 4.962578773498535\n",
      "Epoch 3562: train loss 4.962578773498535 val loss 4.962629318237305\n",
      "Epoch 3563: train loss 4.962629318237305 val loss 4.96424674987793\n",
      "Epoch 3564: train loss 4.96424674987793 val loss 4.96329927444458\n",
      "Epoch 3565: train loss 4.96329927444458 val loss 4.964730739593506\n",
      "Epoch 3566: train loss 4.964730739593506 val loss 4.964099407196045\n",
      "Epoch 3567: train loss 4.964099407196045 val loss 4.964456081390381\n",
      "Epoch 3568: train loss 4.964456081390381 val loss 4.965091705322266\n",
      "Epoch 3569: train loss 4.965091705322266 val loss 4.9627275466918945\n",
      "Epoch 3570: train loss 4.9627275466918945 val loss 4.9658203125\n",
      "Epoch 3571: train loss 4.9658203125 val loss 4.965008735656738\n",
      "Epoch 3572: train loss 4.965008735656738 val loss 4.963809013366699\n",
      "Epoch 3573: train loss 4.963809013366699 val loss 4.964555263519287\n",
      "Epoch 3574: train loss 4.964555263519287 val loss 4.9625163078308105\n",
      "Epoch 3575: train loss 4.9625163078308105 val loss 4.963997840881348\n",
      "Epoch 3576: train loss 4.963997840881348 val loss 4.962675094604492\n",
      "Epoch 3577: train loss 4.962675094604492 val loss 4.96284818649292\n",
      "Epoch 3578: train loss 4.96284818649292 val loss 4.96324348449707\n",
      "Epoch 3579: train loss 4.96324348449707 val loss 4.9625420570373535\n",
      "Epoch 3580: train loss 4.9625420570373535 val loss 4.962961196899414\n",
      "Epoch 3581: train loss 4.962961196899414 val loss 4.962596416473389\n",
      "Epoch 3582: train loss 4.962596416473389 val loss 4.96302604675293\n",
      "Epoch 3583: train loss 4.96302604675293 val loss 4.962986946105957\n",
      "Epoch 3584: train loss 4.962986946105957 val loss 4.96256685256958\n",
      "Epoch 3585: train loss 4.96256685256958 val loss 4.962625026702881\n",
      "Epoch 3586: train loss 4.962625026702881 val loss 4.962869167327881\n",
      "Epoch 3587: train loss 4.962869167327881 val loss 4.96246337890625\n",
      "Epoch 3588: train loss 4.96246337890625 val loss 4.96245813369751\n",
      "Epoch 3589: train loss 4.96245813369751 val loss 4.96240234375\n",
      "Epoch 3590: train loss 4.96240234375 val loss 4.962344169616699\n",
      "Epoch 3591: train loss 4.962344169616699 val loss 4.962400436401367\n",
      "Epoch 3592: train loss 4.962400436401367 val loss 4.963736534118652\n",
      "Epoch 3593: train loss 4.963736534118652 val loss 4.962460517883301\n",
      "Epoch 3594: train loss 4.962460517883301 val loss 4.965022087097168\n",
      "Epoch 3595: train loss 4.965022087097168 val loss 4.96362829208374\n",
      "Epoch 3596: train loss 4.96362829208374 val loss 4.964784145355225\n",
      "Epoch 3597: train loss 4.964784145355225 val loss 4.965428352355957\n",
      "Epoch 3598: train loss 4.965428352355957 val loss 4.9628143310546875\n",
      "Epoch 3599: train loss 4.9628143310546875 val loss 4.965937614440918\n",
      "Epoch 3600: train loss 4.965937614440918 val loss 4.965745449066162\n",
      "Epoch 3601: train loss 4.965745449066162 val loss 4.962868690490723\n",
      "Epoch 3602: train loss 4.962868690490723 val loss 4.964504241943359\n",
      "Epoch 3603: train loss 4.964504241943359 val loss 4.962882041931152\n",
      "Epoch 3604: train loss 4.962882041931152 val loss 4.965719223022461\n",
      "Epoch 3605: train loss 4.965719223022461 val loss 4.965610027313232\n",
      "Epoch 3606: train loss 4.965610027313232 val loss 4.962713241577148\n",
      "Epoch 3607: train loss 4.962713241577148 val loss 4.963435173034668\n",
      "Epoch 3608: train loss 4.963435173034668 val loss 4.963022232055664\n",
      "Epoch 3609: train loss 4.963022232055664 val loss 4.962610244750977\n",
      "Epoch 3610: train loss 4.962610244750977 val loss 4.963981628417969\n",
      "Epoch 3611: train loss 4.963981628417969 val loss 4.963084697723389\n",
      "Epoch 3612: train loss 4.963084697723389 val loss 4.964010238647461\n",
      "Epoch 3613: train loss 4.964010238647461 val loss 4.962867736816406\n",
      "Epoch 3614: train loss 4.962867736816406 val loss 4.965179920196533\n",
      "Epoch 3615: train loss 4.965179920196533 val loss 4.965701580047607\n",
      "Epoch 3616: train loss 4.965701580047607 val loss 4.963142395019531\n",
      "Epoch 3617: train loss 4.963142395019531 val loss 4.96567964553833\n",
      "Epoch 3618: train loss 4.96567964553833 val loss 4.964799404144287\n",
      "Epoch 3619: train loss 4.964799404144287 val loss 4.963606357574463\n",
      "Epoch 3620: train loss 4.963606357574463 val loss 4.964865684509277\n",
      "Epoch 3621: train loss 4.964865684509277 val loss 4.962730407714844\n",
      "Epoch 3622: train loss 4.962730407714844 val loss 4.96497106552124\n",
      "Epoch 3623: train loss 4.96497106552124 val loss 4.963638782501221\n",
      "Epoch 3624: train loss 4.963638782501221 val loss 4.964649200439453\n",
      "Epoch 3625: train loss 4.964649200439453 val loss 4.965324878692627\n",
      "Epoch 3626: train loss 4.965324878692627 val loss 4.962674617767334\n",
      "Epoch 3627: train loss 4.962674617767334 val loss 4.966339588165283\n",
      "Epoch 3628: train loss 4.966339588165283 val loss 4.9658918380737305\n",
      "Epoch 3629: train loss 4.9658918380737305 val loss 4.9632182121276855\n",
      "Epoch 3630: train loss 4.9632182121276855 val loss 4.9648332595825195\n",
      "Epoch 3631: train loss 4.9648332595825195 val loss 4.963574409484863\n",
      "Epoch 3632: train loss 4.963574409484863 val loss 4.964637756347656\n",
      "Epoch 3633: train loss 4.964637756347656 val loss 4.965435981750488\n",
      "Epoch 3634: train loss 4.965435981750488 val loss 4.962618827819824\n",
      "Epoch 3635: train loss 4.962618827819824 val loss 4.9642767906188965\n",
      "Epoch 3636: train loss 4.9642767906188965 val loss 4.962732315063477\n",
      "Epoch 3637: train loss 4.962732315063477 val loss 4.9649457931518555\n",
      "Epoch 3638: train loss 4.9649457931518555 val loss 4.963611602783203\n",
      "Epoch 3639: train loss 4.963611602783203 val loss 4.96400785446167\n",
      "Epoch 3640: train loss 4.96400785446167 val loss 4.964232444763184\n",
      "Epoch 3641: train loss 4.964232444763184 val loss 4.962759494781494\n",
      "Epoch 3642: train loss 4.962759494781494 val loss 4.96501350402832\n",
      "Epoch 3643: train loss 4.96501350402832 val loss 4.962782859802246\n",
      "Epoch 3644: train loss 4.962782859802246 val loss 4.963754653930664\n",
      "Epoch 3645: train loss 4.963754653930664 val loss 4.963134288787842\n",
      "Epoch 3646: train loss 4.963134288787842 val loss 4.964348793029785\n",
      "Epoch 3647: train loss 4.964348793029785 val loss 4.963825225830078\n",
      "Epoch 3648: train loss 4.963825225830078 val loss 4.964077949523926\n",
      "Epoch 3649: train loss 4.964077949523926 val loss 4.964636325836182\n",
      "Epoch 3650: train loss 4.964636325836182 val loss 4.96253776550293\n",
      "Epoch 3651: train loss 4.96253776550293 val loss 4.963212490081787\n",
      "Epoch 3652: train loss 4.963212490081787 val loss 4.9624223709106445\n",
      "Epoch 3653: train loss 4.9624223709106445 val loss 4.962707996368408\n",
      "Epoch 3654: train loss 4.962707996368408 val loss 4.962695121765137\n",
      "Epoch 3655: train loss 4.962695121765137 val loss 4.963271617889404\n",
      "Epoch 3656: train loss 4.963271617889404 val loss 4.963493347167969\n",
      "Epoch 3657: train loss 4.963493347167969 val loss 4.963497161865234\n",
      "Epoch 3658: train loss 4.963497161865234 val loss 4.963130950927734\n",
      "Epoch 3659: train loss 4.963130950927734 val loss 4.962759494781494\n",
      "Epoch 3660: train loss 4.962759494781494 val loss 4.963743209838867\n",
      "Epoch 3661: train loss 4.963743209838867 val loss 4.963201522827148\n",
      "Epoch 3662: train loss 4.963201522827148 val loss 4.964157581329346\n",
      "Epoch 3663: train loss 4.964157581329346 val loss 4.96405029296875\n",
      "Epoch 3664: train loss 4.96405029296875 val loss 4.9634199142456055\n",
      "Epoch 3665: train loss 4.9634199142456055 val loss 4.964137077331543\n",
      "Epoch 3666: train loss 4.964137077331543 val loss 4.962601661682129\n",
      "Epoch 3667: train loss 4.962601661682129 val loss 4.963860511779785\n",
      "Epoch 3668: train loss 4.963860511779785 val loss 4.9623823165893555\n",
      "Epoch 3669: train loss 4.9623823165893555 val loss 4.9641594886779785\n",
      "Epoch 3670: train loss 4.9641594886779785 val loss 4.963055610656738\n",
      "Epoch 3671: train loss 4.963055610656738 val loss 4.9643049240112305\n",
      "Epoch 3672: train loss 4.9643049240112305 val loss 4.963373184204102\n",
      "Epoch 3673: train loss 4.963373184204102 val loss 4.964051246643066\n",
      "Epoch 3674: train loss 4.964051246643066 val loss 4.964108943939209\n",
      "Epoch 3675: train loss 4.964108943939209 val loss 4.962927341461182\n",
      "Epoch 3676: train loss 4.962927341461182 val loss 4.96367883682251\n",
      "Epoch 3677: train loss 4.96367883682251 val loss 4.962589263916016\n",
      "Epoch 3678: train loss 4.962589263916016 val loss 4.962821006774902\n",
      "Epoch 3679: train loss 4.962821006774902 val loss 4.963081359863281\n",
      "Epoch 3680: train loss 4.963081359863281 val loss 4.962453365325928\n",
      "Epoch 3681: train loss 4.962453365325928 val loss 4.962736129760742\n",
      "Epoch 3682: train loss 4.962736129760742 val loss 4.9624223709106445\n",
      "Epoch 3683: train loss 4.9624223709106445 val loss 4.962316989898682\n",
      "Epoch 3684: train loss 4.962316989898682 val loss 4.9623212814331055\n",
      "Epoch 3685: train loss 4.9623212814331055 val loss 4.962899208068848\n",
      "Epoch 3686: train loss 4.962899208068848 val loss 4.962518692016602\n",
      "Epoch 3687: train loss 4.962518692016602 val loss 4.962686538696289\n",
      "Epoch 3688: train loss 4.962686538696289 val loss 4.962450981140137\n",
      "Epoch 3689: train loss 4.962450981140137 val loss 4.962489128112793\n",
      "Epoch 3690: train loss 4.962489128112793 val loss 4.962400436401367\n",
      "Epoch 3691: train loss 4.962400436401367 val loss 4.96238374710083\n",
      "Epoch 3692: train loss 4.96238374710083 val loss 4.962400436401367\n",
      "Epoch 3693: train loss 4.962400436401367 val loss 4.9625654220581055\n",
      "Epoch 3694: train loss 4.9625654220581055 val loss 4.962454319000244\n",
      "Epoch 3695: train loss 4.962454319000244 val loss 4.962294101715088\n",
      "Epoch 3696: train loss 4.962294101715088 val loss 4.962285995483398\n",
      "Epoch 3697: train loss 4.962285995483398 val loss 4.962197780609131\n",
      "Epoch 3698: train loss 4.962197780609131 val loss 4.962524890899658\n",
      "Epoch 3699: train loss 4.962524890899658 val loss 4.9626145362854\n",
      "Epoch 3700: train loss 4.9626145362854 val loss 4.962388515472412\n",
      "Epoch 3701: train loss 4.962388515472412 val loss 4.963350772857666\n",
      "Epoch 3702: train loss 4.963350772857666 val loss 4.962038040161133\n",
      "Epoch 3703: train loss 4.962038040161133 val loss 4.962612152099609\n",
      "Epoch 3704: train loss 4.962612152099609 val loss 4.962557315826416\n",
      "Epoch 3705: train loss 4.962557315826416 val loss 4.962248802185059\n",
      "Epoch 3706: train loss 4.962248802185059 val loss 4.962262153625488\n",
      "Epoch 3707: train loss 4.962262153625488 val loss 4.962337493896484\n",
      "Epoch 3708: train loss 4.962337493896484 val loss 4.962682723999023\n",
      "Epoch 3709: train loss 4.962682723999023 val loss 4.962002277374268\n",
      "Epoch 3710: train loss 4.962002277374268 val loss 4.962165832519531\n",
      "Epoch 3711: train loss 4.962165832519531 val loss 4.9622602462768555\n",
      "Epoch 3712: train loss 4.9622602462768555 val loss 4.9623894691467285\n",
      "Epoch 3713: train loss 4.9623894691467285 val loss 4.963179588317871\n",
      "Epoch 3714: train loss 4.963179588317871 val loss 4.962409019470215\n",
      "Epoch 3715: train loss 4.962409019470215 val loss 4.9631781578063965\n",
      "Epoch 3716: train loss 4.9631781578063965 val loss 4.962227821350098\n",
      "Epoch 3717: train loss 4.962227821350098 val loss 4.962547302246094\n",
      "Epoch 3718: train loss 4.962547302246094 val loss 4.962807655334473\n",
      "Epoch 3719: train loss 4.962807655334473 val loss 4.962193489074707\n",
      "Epoch 3720: train loss 4.962193489074707 val loss 4.96232271194458\n",
      "Epoch 3721: train loss 4.96232271194458 val loss 4.962619304656982\n",
      "Epoch 3722: train loss 4.962619304656982 val loss 4.962367057800293\n",
      "Epoch 3723: train loss 4.962367057800293 val loss 4.962270736694336\n",
      "Epoch 3724: train loss 4.962270736694336 val loss 4.962206840515137\n",
      "Epoch 3725: train loss 4.962206840515137 val loss 4.962174415588379\n",
      "Epoch 3726: train loss 4.962174415588379 val loss 4.962274074554443\n",
      "Epoch 3727: train loss 4.962274074554443 val loss 4.963567733764648\n",
      "Epoch 3728: train loss 4.963567733764648 val loss 4.962182521820068\n",
      "Epoch 3729: train loss 4.962182521820068 val loss 4.964049816131592\n",
      "Epoch 3730: train loss 4.964049816131592 val loss 4.963474273681641\n",
      "Epoch 3731: train loss 4.963474273681641 val loss 4.963393688201904\n",
      "Epoch 3732: train loss 4.963393688201904 val loss 4.962833404541016\n",
      "Epoch 3733: train loss 4.962833404541016 val loss 4.9644012451171875\n",
      "Epoch 3734: train loss 4.9644012451171875 val loss 4.964953422546387\n",
      "Epoch 3735: train loss 4.964953422546387 val loss 4.96226692199707\n",
      "Epoch 3736: train loss 4.96226692199707 val loss 4.966211795806885\n",
      "Epoch 3737: train loss 4.966211795806885 val loss 4.965958118438721\n",
      "Epoch 3738: train loss 4.965958118438721 val loss 4.9623870849609375\n",
      "Epoch 3739: train loss 4.9623870849609375 val loss 4.9650163650512695\n",
      "Epoch 3740: train loss 4.9650163650512695 val loss 4.964308738708496\n",
      "Epoch 3741: train loss 4.964308738708496 val loss 4.962534427642822\n",
      "Epoch 3742: train loss 4.962534427642822 val loss 4.962766647338867\n",
      "Epoch 3743: train loss 4.962766647338867 val loss 4.96321439743042\n",
      "Epoch 3744: train loss 4.96321439743042 val loss 4.962689399719238\n",
      "Epoch 3745: train loss 4.962689399719238 val loss 4.963566780090332\n",
      "Epoch 3746: train loss 4.963566780090332 val loss 4.962574481964111\n",
      "Epoch 3747: train loss 4.962574481964111 val loss 4.964300155639648\n",
      "Epoch 3748: train loss 4.964300155639648 val loss 4.964670181274414\n",
      "Epoch 3749: train loss 4.964670181274414 val loss 4.962131977081299\n",
      "Epoch 3750: train loss 4.962131977081299 val loss 4.964700698852539\n",
      "Epoch 3751: train loss 4.964700698852539 val loss 4.9632768630981445\n",
      "Epoch 3752: train loss 4.9632768630981445 val loss 4.964055061340332\n",
      "Epoch 3753: train loss 4.964055061340332 val loss 4.964779853820801\n",
      "Epoch 3754: train loss 4.964779853820801 val loss 4.962656021118164\n",
      "Epoch 3755: train loss 4.962656021118164 val loss 4.965053558349609\n",
      "Epoch 3756: train loss 4.965053558349609 val loss 4.964150428771973\n",
      "Epoch 3757: train loss 4.964150428771973 val loss 4.963931083679199\n",
      "Epoch 3758: train loss 4.963931083679199 val loss 4.964953899383545\n",
      "Epoch 3759: train loss 4.964953899383545 val loss 4.96315860748291\n",
      "Epoch 3760: train loss 4.96315860748291 val loss 4.964913368225098\n",
      "Epoch 3761: train loss 4.964913368225098 val loss 4.965200901031494\n",
      "Epoch 3762: train loss 4.965200901031494 val loss 4.962123394012451\n",
      "Epoch 3763: train loss 4.962123394012451 val loss 4.964273929595947\n",
      "Epoch 3764: train loss 4.964273929595947 val loss 4.962920188903809\n",
      "Epoch 3765: train loss 4.962920188903809 val loss 4.964483737945557\n",
      "Epoch 3766: train loss 4.964483737945557 val loss 4.965211868286133\n",
      "Epoch 3767: train loss 4.965211868286133 val loss 4.961946964263916\n",
      "Epoch 3768: train loss 4.961946964263916 val loss 4.963017463684082\n",
      "Epoch 3769: train loss 4.963017463684082 val loss 4.9618449211120605\n",
      "Epoch 3770: train loss 4.9618449211120605 val loss 4.963139057159424\n",
      "Epoch 3771: train loss 4.963139057159424 val loss 4.961854934692383\n",
      "Epoch 3772: train loss 4.961854934692383 val loss 4.9623122215271\n",
      "Epoch 3773: train loss 4.9623122215271 val loss 4.962471961975098\n",
      "Epoch 3774: train loss 4.962471961975098 val loss 4.9619293212890625\n",
      "Epoch 3775: train loss 4.9619293212890625 val loss 4.9617414474487305\n",
      "Epoch 3776: train loss 4.9617414474487305 val loss 4.961880207061768\n",
      "Epoch 3777: train loss 4.961880207061768 val loss 4.962961196899414\n",
      "Epoch 3778: train loss 4.962961196899414 val loss 4.9620208740234375\n",
      "Epoch 3779: train loss 4.9620208740234375 val loss 4.963141918182373\n",
      "Epoch 3780: train loss 4.963141918182373 val loss 4.961792469024658\n",
      "Epoch 3781: train loss 4.961792469024658 val loss 4.962522029876709\n",
      "Epoch 3782: train loss 4.962522029876709 val loss 4.961675643920898\n",
      "Epoch 3783: train loss 4.961675643920898 val loss 4.963449954986572\n",
      "Epoch 3784: train loss 4.963449954986572 val loss 4.961761951446533\n",
      "Epoch 3785: train loss 4.961761951446533 val loss 4.962100982666016\n",
      "Epoch 3786: train loss 4.962100982666016 val loss 4.961730003356934\n",
      "Epoch 3787: train loss 4.961730003356934 val loss 4.961771488189697\n",
      "Epoch 3788: train loss 4.961771488189697 val loss 4.962311267852783\n",
      "Epoch 3789: train loss 4.962311267852783 val loss 4.96232271194458\n",
      "Epoch 3790: train loss 4.96232271194458 val loss 4.9617156982421875\n",
      "Epoch 3791: train loss 4.9617156982421875 val loss 4.962588787078857\n",
      "Epoch 3792: train loss 4.962588787078857 val loss 4.962357997894287\n",
      "Epoch 3793: train loss 4.962357997894287 val loss 4.961724281311035\n",
      "Epoch 3794: train loss 4.961724281311035 val loss 4.963161468505859\n",
      "Epoch 3795: train loss 4.963161468505859 val loss 4.961684226989746\n",
      "Epoch 3796: train loss 4.961684226989746 val loss 4.9624528884887695\n",
      "Epoch 3797: train loss 4.9624528884887695 val loss 4.962100028991699\n",
      "Epoch 3798: train loss 4.962100028991699 val loss 4.961800575256348\n",
      "Epoch 3799: train loss 4.961800575256348 val loss 4.961640357971191\n",
      "Epoch 3800: train loss 4.961640357971191 val loss 4.962164878845215\n",
      "Epoch 3801: train loss 4.962164878845215 val loss 4.9621357917785645\n",
      "Epoch 3802: train loss 4.9621357917785645 val loss 4.961725234985352\n",
      "Epoch 3803: train loss 4.961725234985352 val loss 4.961916923522949\n",
      "Epoch 3804: train loss 4.961916923522949 val loss 4.96249532699585\n",
      "Epoch 3805: train loss 4.96249532699585 val loss 4.961756706237793\n",
      "Epoch 3806: train loss 4.961756706237793 val loss 4.962821960449219\n",
      "Epoch 3807: train loss 4.962821960449219 val loss 4.961999893188477\n",
      "Epoch 3808: train loss 4.961999893188477 val loss 4.962378978729248\n",
      "Epoch 3809: train loss 4.962378978729248 val loss 4.962286472320557\n",
      "Epoch 3810: train loss 4.962286472320557 val loss 4.961806774139404\n",
      "Epoch 3811: train loss 4.961806774139404 val loss 4.963022232055664\n",
      "Epoch 3812: train loss 4.963022232055664 val loss 4.962251663208008\n",
      "Epoch 3813: train loss 4.962251663208008 val loss 4.96347188949585\n",
      "Epoch 3814: train loss 4.96347188949585 val loss 4.962405681610107\n",
      "Epoch 3815: train loss 4.962405681610107 val loss 4.96351957321167\n",
      "Epoch 3816: train loss 4.96351957321167 val loss 4.963744163513184\n",
      "Epoch 3817: train loss 4.963744163513184 val loss 4.961846351623535\n",
      "Epoch 3818: train loss 4.961846351623535 val loss 4.964447021484375\n",
      "Epoch 3819: train loss 4.964447021484375 val loss 4.962824821472168\n",
      "Epoch 3820: train loss 4.962824821472168 val loss 4.964468955993652\n",
      "Epoch 3821: train loss 4.964468955993652 val loss 4.965778350830078\n",
      "Epoch 3822: train loss 4.965778350830078 val loss 4.963601589202881\n",
      "Epoch 3823: train loss 4.963601589202881 val loss 4.964263916015625\n",
      "Epoch 3824: train loss 4.964263916015625 val loss 4.965676307678223\n",
      "Epoch 3825: train loss 4.965676307678223 val loss 4.962230205535889\n",
      "Epoch 3826: train loss 4.962230205535889 val loss 4.964901924133301\n",
      "Epoch 3827: train loss 4.964901924133301 val loss 4.966047286987305\n",
      "Epoch 3828: train loss 4.966047286987305 val loss 4.964263916015625\n",
      "Epoch 3829: train loss 4.964263916015625 val loss 4.963089942932129\n",
      "Epoch 3830: train loss 4.963089942932129 val loss 4.963716506958008\n",
      "Epoch 3831: train loss 4.963716506958008 val loss 4.963072776794434\n",
      "Epoch 3832: train loss 4.963072776794434 val loss 4.963510513305664\n",
      "Epoch 3833: train loss 4.963510513305664 val loss 4.96248722076416\n",
      "Epoch 3834: train loss 4.96248722076416 val loss 4.96299409866333\n",
      "Epoch 3835: train loss 4.96299409866333 val loss 4.962164878845215\n",
      "Epoch 3836: train loss 4.962164878845215 val loss 4.96242094039917\n",
      "Epoch 3837: train loss 4.96242094039917 val loss 4.96187686920166\n",
      "Epoch 3838: train loss 4.96187686920166 val loss 4.962014675140381\n",
      "Epoch 3839: train loss 4.962014675140381 val loss 4.962012767791748\n",
      "Epoch 3840: train loss 4.962012767791748 val loss 4.961681365966797\n",
      "Epoch 3841: train loss 4.961681365966797 val loss 4.961790561676025\n",
      "Epoch 3842: train loss 4.961790561676025 val loss 4.9616498947143555\n",
      "Epoch 3843: train loss 4.9616498947143555 val loss 4.962414741516113\n",
      "Epoch 3844: train loss 4.962414741516113 val loss 4.961931228637695\n",
      "Epoch 3845: train loss 4.961931228637695 val loss 4.96173095703125\n",
      "Epoch 3846: train loss 4.96173095703125 val loss 4.962294101715088\n",
      "Epoch 3847: train loss 4.962294101715088 val loss 4.961493492126465\n",
      "Epoch 3848: train loss 4.961493492126465 val loss 4.961536407470703\n",
      "Epoch 3849: train loss 4.961536407470703 val loss 4.962018013000488\n",
      "Epoch 3850: train loss 4.962018013000488 val loss 4.961792945861816\n",
      "Epoch 3851: train loss 4.961792945861816 val loss 4.9616546630859375\n",
      "Epoch 3852: train loss 4.9616546630859375 val loss 4.961713790893555\n",
      "Epoch 3853: train loss 4.961713790893555 val loss 4.962031364440918\n",
      "Epoch 3854: train loss 4.962031364440918 val loss 4.961607933044434\n",
      "Epoch 3855: train loss 4.961607933044434 val loss 4.962488174438477\n",
      "Epoch 3856: train loss 4.962488174438477 val loss 4.961641311645508\n",
      "Epoch 3857: train loss 4.961641311645508 val loss 4.9619646072387695\n",
      "Epoch 3858: train loss 4.9619646072387695 val loss 4.961775302886963\n",
      "Epoch 3859: train loss 4.961775302886963 val loss 4.961481094360352\n",
      "Epoch 3860: train loss 4.961481094360352 val loss 4.961917400360107\n",
      "Epoch 3861: train loss 4.961917400360107 val loss 4.961289882659912\n",
      "Epoch 3862: train loss 4.961289882659912 val loss 4.961848735809326\n",
      "Epoch 3863: train loss 4.961848735809326 val loss 4.961907863616943\n",
      "Epoch 3864: train loss 4.961907863616943 val loss 4.96143913269043\n",
      "Epoch 3865: train loss 4.96143913269043 val loss 4.962210178375244\n",
      "Epoch 3866: train loss 4.962210178375244 val loss 4.9614362716674805\n",
      "Epoch 3867: train loss 4.9614362716674805 val loss 4.961267471313477\n",
      "Epoch 3868: train loss 4.961267471313477 val loss 4.96180534362793\n",
      "Epoch 3869: train loss 4.96180534362793 val loss 4.961771011352539\n",
      "Epoch 3870: train loss 4.961771011352539 val loss 4.961639404296875\n",
      "Epoch 3871: train loss 4.961639404296875 val loss 4.962490081787109\n",
      "Epoch 3872: train loss 4.962490081787109 val loss 4.96151876449585\n",
      "Epoch 3873: train loss 4.96151876449585 val loss 4.961846828460693\n",
      "Epoch 3874: train loss 4.961846828460693 val loss 4.96190881729126\n",
      "Epoch 3875: train loss 4.96190881729126 val loss 4.9613847732543945\n",
      "Epoch 3876: train loss 4.9613847732543945 val loss 4.9619340896606445\n",
      "Epoch 3877: train loss 4.9619340896606445 val loss 4.961635589599609\n",
      "Epoch 3878: train loss 4.961635589599609 val loss 4.9617533683776855\n",
      "Epoch 3879: train loss 4.9617533683776855 val loss 4.9621782302856445\n",
      "Epoch 3880: train loss 4.9621782302856445 val loss 4.961928367614746\n",
      "Epoch 3881: train loss 4.961928367614746 val loss 4.961867332458496\n",
      "Epoch 3882: train loss 4.961867332458496 val loss 4.961261749267578\n",
      "Epoch 3883: train loss 4.961261749267578 val loss 4.962717056274414\n",
      "Epoch 3884: train loss 4.962717056274414 val loss 4.961711883544922\n",
      "Epoch 3885: train loss 4.961711883544922 val loss 4.963318347930908\n",
      "Epoch 3886: train loss 4.963318347930908 val loss 4.962920188903809\n",
      "Epoch 3887: train loss 4.962920188903809 val loss 4.961697578430176\n",
      "Epoch 3888: train loss 4.961697578430176 val loss 4.961907386779785\n",
      "Epoch 3889: train loss 4.961907386779785 val loss 4.962070465087891\n",
      "Epoch 3890: train loss 4.962070465087891 val loss 4.961560249328613\n",
      "Epoch 3891: train loss 4.961560249328613 val loss 4.9626641273498535\n",
      "Epoch 3892: train loss 4.9626641273498535 val loss 4.962284088134766\n",
      "Epoch 3893: train loss 4.962284088134766 val loss 4.962143421173096\n",
      "Epoch 3894: train loss 4.962143421173096 val loss 4.961658477783203\n",
      "Epoch 3895: train loss 4.961658477783203 val loss 4.96174955368042\n",
      "Epoch 3896: train loss 4.96174955368042 val loss 4.9615607261657715\n",
      "Epoch 3897: train loss 4.9615607261657715 val loss 4.961786270141602\n",
      "Epoch 3898: train loss 4.961786270141602 val loss 4.961560249328613\n",
      "Epoch 3899: train loss 4.961560249328613 val loss 4.962501049041748\n",
      "Epoch 3900: train loss 4.962501049041748 val loss 4.961574554443359\n",
      "Epoch 3901: train loss 4.961574554443359 val loss 4.961868762969971\n",
      "Epoch 3902: train loss 4.961868762969971 val loss 4.961133003234863\n",
      "Epoch 3903: train loss 4.961133003234863 val loss 4.961321830749512\n",
      "Epoch 3904: train loss 4.961321830749512 val loss 4.961565971374512\n",
      "Epoch 3905: train loss 4.961565971374512 val loss 4.961599826812744\n",
      "Epoch 3906: train loss 4.961599826812744 val loss 4.961647033691406\n",
      "Epoch 3907: train loss 4.961647033691406 val loss 4.9615478515625\n",
      "Epoch 3908: train loss 4.9615478515625 val loss 4.961000442504883\n",
      "Epoch 3909: train loss 4.961000442504883 val loss 4.962259292602539\n",
      "Epoch 3910: train loss 4.962259292602539 val loss 4.9612298011779785\n",
      "Epoch 3911: train loss 4.9612298011779785 val loss 4.962536811828613\n",
      "Epoch 3912: train loss 4.962536811828613 val loss 4.961098670959473\n",
      "Epoch 3913: train loss 4.961098670959473 val loss 4.962397575378418\n",
      "Epoch 3914: train loss 4.962397575378418 val loss 4.961030006408691\n",
      "Epoch 3915: train loss 4.961030006408691 val loss 4.962951183319092\n",
      "Epoch 3916: train loss 4.962951183319092 val loss 4.962244987487793\n",
      "Epoch 3917: train loss 4.962244987487793 val loss 4.962652206420898\n",
      "Epoch 3918: train loss 4.962652206420898 val loss 4.9621262550354\n",
      "Epoch 3919: train loss 4.9621262550354 val loss 4.962523460388184\n",
      "Epoch 3920: train loss 4.962523460388184 val loss 4.962855339050293\n",
      "Epoch 3921: train loss 4.962855339050293 val loss 4.961296081542969\n",
      "Epoch 3922: train loss 4.961296081542969 val loss 4.963899612426758\n",
      "Epoch 3923: train loss 4.963899612426758 val loss 4.962353706359863\n",
      "Epoch 3924: train loss 4.962353706359863 val loss 4.963503360748291\n",
      "Epoch 3925: train loss 4.963503360748291 val loss 4.964766979217529\n",
      "Epoch 3926: train loss 4.964766979217529 val loss 4.962728023529053\n",
      "Epoch 3927: train loss 4.962728023529053 val loss 4.9638590812683105\n",
      "Epoch 3928: train loss 4.9638590812683105 val loss 4.964507102966309\n",
      "Epoch 3929: train loss 4.964507102966309 val loss 4.961792469024658\n",
      "Epoch 3930: train loss 4.961792469024658 val loss 4.963609218597412\n",
      "Epoch 3931: train loss 4.963609218597412 val loss 4.963667392730713\n",
      "Epoch 3932: train loss 4.963667392730713 val loss 4.961587905883789\n",
      "Epoch 3933: train loss 4.961587905883789 val loss 4.963860034942627\n",
      "Epoch 3934: train loss 4.963860034942627 val loss 4.963095664978027\n",
      "Epoch 3935: train loss 4.963095664978027 val loss 4.962639808654785\n",
      "Epoch 3936: train loss 4.962639808654785 val loss 4.963768005371094\n",
      "Epoch 3937: train loss 4.963768005371094 val loss 4.962887763977051\n",
      "Epoch 3938: train loss 4.962887763977051 val loss 4.961475849151611\n",
      "Epoch 3939: train loss 4.961475849151611 val loss 4.961950778961182\n",
      "Epoch 3940: train loss 4.961950778961182 val loss 4.961066246032715\n",
      "Epoch 3941: train loss 4.961066246032715 val loss 4.961069107055664\n",
      "Epoch 3942: train loss 4.961069107055664 val loss 4.961732387542725\n",
      "Epoch 3943: train loss 4.961732387542725 val loss 4.961287021636963\n",
      "Epoch 3944: train loss 4.961287021636963 val loss 4.9624128341674805\n",
      "Epoch 3945: train loss 4.9624128341674805 val loss 4.96185302734375\n",
      "Epoch 3946: train loss 4.96185302734375 val loss 4.961946964263916\n",
      "Epoch 3947: train loss 4.961946964263916 val loss 4.961591720581055\n",
      "Epoch 3948: train loss 4.961591720581055 val loss 4.962496757507324\n",
      "Epoch 3949: train loss 4.962496757507324 val loss 4.962906837463379\n",
      "Epoch 3950: train loss 4.962906837463379 val loss 4.961659908294678\n",
      "Epoch 3951: train loss 4.961659908294678 val loss 4.963217735290527\n",
      "Epoch 3952: train loss 4.963217735290527 val loss 4.962251663208008\n",
      "Epoch 3953: train loss 4.962251663208008 val loss 4.962159156799316\n",
      "Epoch 3954: train loss 4.962159156799316 val loss 4.9634294509887695\n",
      "Epoch 3955: train loss 4.9634294509887695 val loss 4.961575984954834\n",
      "Epoch 3956: train loss 4.961575984954834 val loss 4.963742256164551\n",
      "Epoch 3957: train loss 4.963742256164551 val loss 4.964139461517334\n",
      "Epoch 3958: train loss 4.964139461517334 val loss 4.961032390594482\n",
      "Epoch 3959: train loss 4.961032390594482 val loss 4.962803840637207\n",
      "Epoch 3960: train loss 4.962803840637207 val loss 4.9621076583862305\n",
      "Epoch 3961: train loss 4.9621076583862305 val loss 4.961751937866211\n",
      "Epoch 3962: train loss 4.961751937866211 val loss 4.9626264572143555\n",
      "Epoch 3963: train loss 4.9626264572143555 val loss 4.960897445678711\n",
      "Epoch 3964: train loss 4.960897445678711 val loss 4.96133279800415\n",
      "Epoch 3965: train loss 4.96133279800415 val loss 4.961132049560547\n",
      "Epoch 3966: train loss 4.961132049560547 val loss 4.96075963973999\n",
      "Epoch 3967: train loss 4.96075963973999 val loss 4.9606547355651855\n",
      "Epoch 3968: train loss 4.9606547355651855 val loss 4.961061477661133\n",
      "Epoch 3969: train loss 4.961061477661133 val loss 4.961287975311279\n",
      "Epoch 3970: train loss 4.961287975311279 val loss 4.960831165313721\n",
      "Epoch 3971: train loss 4.960831165313721 val loss 4.961012840270996\n",
      "Epoch 3972: train loss 4.961012840270996 val loss 4.961002349853516\n",
      "Epoch 3973: train loss 4.961002349853516 val loss 4.960890769958496\n",
      "Epoch 3974: train loss 4.960890769958496 val loss 4.960744857788086\n",
      "Epoch 3975: train loss 4.960744857788086 val loss 4.960844993591309\n",
      "Epoch 3976: train loss 4.960844993591309 val loss 4.960705757141113\n",
      "Epoch 3977: train loss 4.960705757141113 val loss 4.960704326629639\n",
      "Epoch 3978: train loss 4.960704326629639 val loss 4.960526466369629\n",
      "Epoch 3979: train loss 4.960526466369629 val loss 4.960638999938965\n",
      "Epoch 3980: train loss 4.960638999938965 val loss 4.9605712890625\n",
      "Epoch 3981: train loss 4.9605712890625 val loss 4.960416793823242\n",
      "Epoch 3982: train loss 4.960416793823242 val loss 4.960898399353027\n",
      "Epoch 3983: train loss 4.960898399353027 val loss 4.960780143737793\n",
      "Epoch 3984: train loss 4.960780143737793 val loss 4.9604949951171875\n",
      "Epoch 3985: train loss 4.9604949951171875 val loss 4.961665153503418\n",
      "Epoch 3986: train loss 4.961665153503418 val loss 4.960542678833008\n",
      "Epoch 3987: train loss 4.960542678833008 val loss 4.96188497543335\n",
      "Epoch 3988: train loss 4.96188497543335 val loss 4.9611921310424805\n",
      "Epoch 3989: train loss 4.9611921310424805 val loss 4.962095260620117\n",
      "Epoch 3990: train loss 4.962095260620117 val loss 4.961318492889404\n",
      "Epoch 3991: train loss 4.961318492889404 val loss 4.961723804473877\n",
      "Epoch 3992: train loss 4.961723804473877 val loss 4.961975574493408\n",
      "Epoch 3993: train loss 4.961975574493408 val loss 4.960736274719238\n",
      "Epoch 3994: train loss 4.960736274719238 val loss 4.961127281188965\n",
      "Epoch 3995: train loss 4.961127281188965 val loss 4.960855484008789\n",
      "Epoch 3996: train loss 4.960855484008789 val loss 4.96052885055542\n",
      "Epoch 3997: train loss 4.96052885055542 val loss 4.961650848388672\n",
      "Epoch 3998: train loss 4.961650848388672 val loss 4.960626125335693\n",
      "Epoch 3999: train loss 4.960626125335693 val loss 4.962111473083496\n",
      "Epoch 4000: train loss 4.962111473083496 val loss 4.961779594421387\n",
      "Epoch 4001: train loss 4.961779594421387 val loss 4.960733413696289\n",
      "Epoch 4002: train loss 4.960733413696289 val loss 4.960688591003418\n",
      "Epoch 4003: train loss 4.960688591003418 val loss 4.961014270782471\n",
      "Epoch 4004: train loss 4.961014270782471 val loss 4.960555076599121\n",
      "Epoch 4005: train loss 4.960555076599121 val loss 4.961587905883789\n",
      "Epoch 4006: train loss 4.961587905883789 val loss 4.960542678833008\n",
      "Epoch 4007: train loss 4.960542678833008 val loss 4.961987018585205\n",
      "Epoch 4008: train loss 4.961987018585205 val loss 4.96154260635376\n",
      "Epoch 4009: train loss 4.96154260635376 val loss 4.960838317871094\n",
      "Epoch 4010: train loss 4.960838317871094 val loss 4.961307525634766\n",
      "Epoch 4011: train loss 4.961307525634766 val loss 4.961062431335449\n",
      "Epoch 4012: train loss 4.961062431335449 val loss 4.961246490478516\n",
      "Epoch 4013: train loss 4.961246490478516 val loss 4.960444450378418\n",
      "Epoch 4014: train loss 4.960444450378418 val loss 4.960465431213379\n",
      "Epoch 4015: train loss 4.960465431213379 val loss 4.960685729980469\n",
      "Epoch 4016: train loss 4.960685729980469 val loss 4.9602861404418945\n",
      "Epoch 4017: train loss 4.9602861404418945 val loss 4.960383415222168\n",
      "Epoch 4018: train loss 4.960383415222168 val loss 4.960403919219971\n",
      "Epoch 4019: train loss 4.960403919219971 val loss 4.96058988571167\n",
      "Epoch 4020: train loss 4.96058988571167 val loss 4.960153579711914\n",
      "Epoch 4021: train loss 4.960153579711914 val loss 4.9612226486206055\n",
      "Epoch 4022: train loss 4.9612226486206055 val loss 4.960147857666016\n",
      "Epoch 4023: train loss 4.960147857666016 val loss 4.960452079772949\n",
      "Epoch 4024: train loss 4.960452079772949 val loss 4.960151195526123\n",
      "Epoch 4025: train loss 4.960151195526123 val loss 4.9603352546691895\n",
      "Epoch 4026: train loss 4.9603352546691895 val loss 4.960256576538086\n",
      "Epoch 4027: train loss 4.960256576538086 val loss 4.9602885246276855\n",
      "Epoch 4028: train loss 4.9602885246276855 val loss 4.96065092086792\n",
      "Epoch 4029: train loss 4.96065092086792 val loss 4.960134506225586\n",
      "Epoch 4030: train loss 4.960134506225586 val loss 4.961130619049072\n",
      "Epoch 4031: train loss 4.961130619049072 val loss 4.960446834564209\n",
      "Epoch 4032: train loss 4.960446834564209 val loss 4.961137771606445\n",
      "Epoch 4033: train loss 4.961137771606445 val loss 4.960335731506348\n",
      "Epoch 4034: train loss 4.960335731506348 val loss 4.961144924163818\n",
      "Epoch 4035: train loss 4.961144924163818 val loss 4.960739612579346\n",
      "Epoch 4036: train loss 4.960739612579346 val loss 4.961161136627197\n",
      "Epoch 4037: train loss 4.961161136627197 val loss 4.960719108581543\n",
      "Epoch 4038: train loss 4.960719108581543 val loss 4.961075782775879\n",
      "Epoch 4039: train loss 4.961075782775879 val loss 4.961130619049072\n",
      "Epoch 4040: train loss 4.961130619049072 val loss 4.96025276184082\n",
      "Epoch 4041: train loss 4.96025276184082 val loss 4.960395336151123\n",
      "Epoch 4042: train loss 4.960395336151123 val loss 4.960296154022217\n",
      "Epoch 4043: train loss 4.960296154022217 val loss 4.960012912750244\n",
      "Epoch 4044: train loss 4.960012912750244 val loss 4.959988117218018\n",
      "Epoch 4045: train loss 4.959988117218018 val loss 4.959880828857422\n",
      "Epoch 4046: train loss 4.959880828857422 val loss 4.960445404052734\n",
      "Epoch 4047: train loss 4.960445404052734 val loss 4.959993362426758\n",
      "Epoch 4048: train loss 4.959993362426758 val loss 4.960173606872559\n",
      "Epoch 4049: train loss 4.960173606872559 val loss 4.960394382476807\n",
      "Epoch 4050: train loss 4.960394382476807 val loss 4.960103988647461\n",
      "Epoch 4051: train loss 4.960103988647461 val loss 4.960249900817871\n",
      "Epoch 4052: train loss 4.960249900817871 val loss 4.9598822593688965\n",
      "Epoch 4053: train loss 4.9598822593688965 val loss 4.959946632385254\n",
      "Epoch 4054: train loss 4.959946632385254 val loss 4.959887981414795\n",
      "Epoch 4055: train loss 4.959887981414795 val loss 4.959862232208252\n",
      "Epoch 4056: train loss 4.959862232208252 val loss 4.960205078125\n",
      "Epoch 4057: train loss 4.960205078125 val loss 4.959848880767822\n",
      "Epoch 4058: train loss 4.959848880767822 val loss 4.960649013519287\n",
      "Epoch 4059: train loss 4.960649013519287 val loss 4.959856033325195\n",
      "Epoch 4060: train loss 4.959856033325195 val loss 4.961787223815918\n",
      "Epoch 4061: train loss 4.961787223815918 val loss 4.961330890655518\n",
      "Epoch 4062: train loss 4.961330890655518 val loss 4.960428237915039\n",
      "Epoch 4063: train loss 4.960428237915039 val loss 4.960903167724609\n",
      "Epoch 4064: train loss 4.960903167724609 val loss 4.959804534912109\n",
      "Epoch 4065: train loss 4.959804534912109 val loss 4.959982395172119\n",
      "Epoch 4066: train loss 4.959982395172119 val loss 4.95980167388916\n",
      "Epoch 4067: train loss 4.95980167388916 val loss 4.95982551574707\n",
      "Epoch 4068: train loss 4.95982551574707 val loss 4.959609031677246\n",
      "Epoch 4069: train loss 4.959609031677246 val loss 4.96010160446167\n",
      "Epoch 4070: train loss 4.96010160446167 val loss 4.959626197814941\n",
      "Epoch 4071: train loss 4.959626197814941 val loss 4.959517478942871\n",
      "Epoch 4072: train loss 4.959517478942871 val loss 4.959863662719727\n",
      "Epoch 4073: train loss 4.959863662719727 val loss 4.9599528312683105\n",
      "Epoch 4074: train loss 4.9599528312683105 val loss 4.959979057312012\n",
      "Epoch 4075: train loss 4.959979057312012 val loss 4.959700107574463\n",
      "Epoch 4076: train loss 4.959700107574463 val loss 4.960556983947754\n",
      "Epoch 4077: train loss 4.960556983947754 val loss 4.959601402282715\n",
      "Epoch 4078: train loss 4.959601402282715 val loss 4.960093021392822\n",
      "Epoch 4079: train loss 4.960093021392822 val loss 4.959473609924316\n",
      "Epoch 4080: train loss 4.959473609924316 val loss 4.959991455078125\n",
      "Epoch 4081: train loss 4.959991455078125 val loss 4.959624290466309\n",
      "Epoch 4082: train loss 4.959624290466309 val loss 4.959586143493652\n",
      "Epoch 4083: train loss 4.959586143493652 val loss 4.960415840148926\n",
      "Epoch 4084: train loss 4.960415840148926 val loss 4.959814071655273\n",
      "Epoch 4085: train loss 4.959814071655273 val loss 4.960503578186035\n",
      "Epoch 4086: train loss 4.960503578186035 val loss 4.960290908813477\n",
      "Epoch 4087: train loss 4.960290908813477 val loss 4.960023403167725\n",
      "Epoch 4088: train loss 4.960023403167725 val loss 4.9604597091674805\n",
      "Epoch 4089: train loss 4.9604597091674805 val loss 4.95974063873291\n",
      "Epoch 4090: train loss 4.95974063873291 val loss 4.959622383117676\n",
      "Epoch 4091: train loss 4.959622383117676 val loss 4.959316730499268\n",
      "Epoch 4092: train loss 4.959316730499268 val loss 4.959706783294678\n",
      "Epoch 4093: train loss 4.959706783294678 val loss 4.959417819976807\n",
      "Epoch 4094: train loss 4.959417819976807 val loss 4.959584712982178\n",
      "Epoch 4095: train loss 4.959584712982178 val loss 4.959342956542969\n",
      "Epoch 4096: train loss 4.959342956542969 val loss 4.959541320800781\n",
      "Epoch 4097: train loss 4.959541320800781 val loss 4.959755897521973\n",
      "Epoch 4098: train loss 4.959755897521973 val loss 4.959690093994141\n",
      "Epoch 4099: train loss 4.959690093994141 val loss 4.9597601890563965\n",
      "Epoch 4100: train loss 4.9597601890563965 val loss 4.9596028327941895\n",
      "Epoch 4101: train loss 4.9596028327941895 val loss 4.959329605102539\n",
      "Epoch 4102: train loss 4.959329605102539 val loss 4.9594268798828125\n",
      "Epoch 4103: train loss 4.9594268798828125 val loss 4.959364414215088\n",
      "Epoch 4104: train loss 4.959364414215088 val loss 4.959981918334961\n",
      "Epoch 4105: train loss 4.959981918334961 val loss 4.959624290466309\n",
      "Epoch 4106: train loss 4.959624290466309 val loss 4.959635257720947\n",
      "Epoch 4107: train loss 4.959635257720947 val loss 4.959656715393066\n",
      "Epoch 4108: train loss 4.959656715393066 val loss 4.959376335144043\n",
      "Epoch 4109: train loss 4.959376335144043 val loss 4.959505081176758\n",
      "Epoch 4110: train loss 4.959505081176758 val loss 4.959359169006348\n",
      "Epoch 4111: train loss 4.959359169006348 val loss 4.959421157836914\n",
      "Epoch 4112: train loss 4.959421157836914 val loss 4.959146499633789\n",
      "Epoch 4113: train loss 4.959146499633789 val loss 4.958769798278809\n",
      "Epoch 4114: train loss 4.958769798278809 val loss 4.95898962020874\n",
      "Epoch 4115: train loss 4.95898962020874 val loss 4.959210395812988\n",
      "Epoch 4116: train loss 4.959210395812988 val loss 4.9593658447265625\n",
      "Epoch 4117: train loss 4.9593658447265625 val loss 4.959109306335449\n",
      "Epoch 4118: train loss 4.959109306335449 val loss 4.95930290222168\n",
      "Epoch 4119: train loss 4.95930290222168 val loss 4.958758354187012\n",
      "Epoch 4120: train loss 4.958758354187012 val loss 4.958703994750977\n",
      "Epoch 4121: train loss 4.958703994750977 val loss 4.958740234375\n",
      "Epoch 4122: train loss 4.958740234375 val loss 4.958747863769531\n",
      "Epoch 4123: train loss 4.958747863769531 val loss 4.959277153015137\n",
      "Epoch 4124: train loss 4.959277153015137 val loss 4.958767890930176\n",
      "Epoch 4125: train loss 4.958767890930176 val loss 4.959133148193359\n",
      "Epoch 4126: train loss 4.959133148193359 val loss 4.958704948425293\n",
      "Epoch 4127: train loss 4.958704948425293 val loss 4.959253787994385\n",
      "Epoch 4128: train loss 4.959253787994385 val loss 4.958609104156494\n",
      "Epoch 4129: train loss 4.958609104156494 val loss 4.958724021911621\n",
      "Epoch 4130: train loss 4.958724021911621 val loss 4.959252834320068\n",
      "Epoch 4131: train loss 4.959252834320068 val loss 4.958833694458008\n",
      "Epoch 4132: train loss 4.958833694458008 val loss 4.9592204093933105\n",
      "Epoch 4133: train loss 4.9592204093933105 val loss 4.958727836608887\n",
      "Epoch 4134: train loss 4.958727836608887 val loss 4.958645343780518\n",
      "Epoch 4135: train loss 4.958645343780518 val loss 4.958277225494385\n",
      "Epoch 4136: train loss 4.958277225494385 val loss 4.959059715270996\n",
      "Epoch 4137: train loss 4.959059715270996 val loss 4.958719730377197\n",
      "Epoch 4138: train loss 4.958719730377197 val loss 4.959038734436035\n",
      "Epoch 4139: train loss 4.959038734436035 val loss 4.958829402923584\n",
      "Epoch 4140: train loss 4.958829402923584 val loss 4.958769798278809\n",
      "Epoch 4141: train loss 4.958769798278809 val loss 4.958867073059082\n",
      "Epoch 4142: train loss 4.958867073059082 val loss 4.958298206329346\n",
      "Epoch 4143: train loss 4.958298206329346 val loss 4.959585189819336\n",
      "Epoch 4144: train loss 4.959585189819336 val loss 4.959001541137695\n",
      "Epoch 4145: train loss 4.959001541137695 val loss 4.959395885467529\n",
      "Epoch 4146: train loss 4.959395885467529 val loss 4.959569931030273\n",
      "Epoch 4147: train loss 4.959569931030273 val loss 4.958471298217773\n",
      "Epoch 4148: train loss 4.958471298217773 val loss 4.96042537689209\n",
      "Epoch 4149: train loss 4.96042537689209 val loss 4.960171699523926\n",
      "Epoch 4150: train loss 4.960171699523926 val loss 4.95833158493042\n",
      "Epoch 4151: train loss 4.95833158493042 val loss 4.959568500518799\n",
      "Epoch 4152: train loss 4.959568500518799 val loss 4.958870887756348\n",
      "Epoch 4153: train loss 4.958870887756348 val loss 4.959233283996582\n",
      "Epoch 4154: train loss 4.959233283996582 val loss 4.959512233734131\n",
      "Epoch 4155: train loss 4.959512233734131 val loss 4.957996368408203\n",
      "Epoch 4156: train loss 4.957996368408203 val loss 4.9595947265625\n",
      "Epoch 4157: train loss 4.9595947265625 val loss 4.958942413330078\n",
      "Epoch 4158: train loss 4.958942413330078 val loss 4.9587883949279785\n",
      "Epoch 4159: train loss 4.9587883949279785 val loss 4.958996295928955\n",
      "Epoch 4160: train loss 4.958996295928955 val loss 4.958262920379639\n",
      "Epoch 4161: train loss 4.958262920379639 val loss 4.958477020263672\n",
      "Epoch 4162: train loss 4.958477020263672 val loss 4.957866668701172\n",
      "Epoch 4163: train loss 4.957866668701172 val loss 4.958350658416748\n",
      "Epoch 4164: train loss 4.958350658416748 val loss 4.957758903503418\n",
      "Epoch 4165: train loss 4.957758903503418 val loss 4.9583024978637695\n",
      "Epoch 4166: train loss 4.9583024978637695 val loss 4.957658767700195\n",
      "Epoch 4167: train loss 4.957658767700195 val loss 4.95786714553833\n",
      "Epoch 4168: train loss 4.95786714553833 val loss 4.957430839538574\n",
      "Epoch 4169: train loss 4.957430839538574 val loss 4.957557678222656\n",
      "Epoch 4170: train loss 4.957557678222656 val loss 4.957775115966797\n",
      "Epoch 4171: train loss 4.957775115966797 val loss 4.958101749420166\n",
      "Epoch 4172: train loss 4.958101749420166 val loss 4.957531929016113\n",
      "Epoch 4173: train loss 4.957531929016113 val loss 4.95793342590332\n",
      "Epoch 4174: train loss 4.95793342590332 val loss 4.957590103149414\n",
      "Epoch 4175: train loss 4.957590103149414 val loss 4.957275390625\n",
      "Epoch 4176: train loss 4.957275390625 val loss 4.957737922668457\n",
      "Epoch 4177: train loss 4.957737922668457 val loss 4.957449436187744\n",
      "Epoch 4178: train loss 4.957449436187744 val loss 4.957223892211914\n",
      "Epoch 4179: train loss 4.957223892211914 val loss 4.9581828117370605\n",
      "Epoch 4180: train loss 4.9581828117370605 val loss 4.957352638244629\n",
      "Epoch 4181: train loss 4.957352638244629 val loss 4.958354473114014\n",
      "Epoch 4182: train loss 4.958354473114014 val loss 4.958028793334961\n",
      "Epoch 4183: train loss 4.958028793334961 val loss 4.957714557647705\n",
      "Epoch 4184: train loss 4.957714557647705 val loss 4.958130836486816\n",
      "Epoch 4185: train loss 4.958130836486816 val loss 4.957596778869629\n",
      "Epoch 4186: train loss 4.957596778869629 val loss 4.958198547363281\n",
      "Epoch 4187: train loss 4.958198547363281 val loss 4.957247734069824\n",
      "Epoch 4188: train loss 4.957247734069824 val loss 4.957951545715332\n",
      "Epoch 4189: train loss 4.957951545715332 val loss 4.957739353179932\n",
      "Epoch 4190: train loss 4.957739353179932 val loss 4.957300186157227\n",
      "Epoch 4191: train loss 4.957300186157227 val loss 4.957459449768066\n",
      "Epoch 4192: train loss 4.957459449768066 val loss 4.957633018493652\n",
      "Epoch 4193: train loss 4.957633018493652 val loss 4.957479476928711\n",
      "Epoch 4194: train loss 4.957479476928711 val loss 4.95720100402832\n",
      "Epoch 4195: train loss 4.95720100402832 val loss 4.956632614135742\n",
      "Epoch 4196: train loss 4.956632614135742 val loss 4.956857681274414\n",
      "Epoch 4197: train loss 4.956857681274414 val loss 4.956567764282227\n",
      "Epoch 4198: train loss 4.956567764282227 val loss 4.956474781036377\n",
      "Epoch 4199: train loss 4.956474781036377 val loss 4.9563798904418945\n",
      "Epoch 4200: train loss 4.9563798904418945 val loss 4.95642614364624\n",
      "Epoch 4201: train loss 4.95642614364624 val loss 4.956233024597168\n",
      "Epoch 4202: train loss 4.956233024597168 val loss 4.95626974105835\n",
      "Epoch 4203: train loss 4.95626974105835 val loss 4.956434726715088\n",
      "Epoch 4204: train loss 4.956434726715088 val loss 4.956172943115234\n",
      "Epoch 4205: train loss 4.956172943115234 val loss 4.956466197967529\n",
      "Epoch 4206: train loss 4.956466197967529 val loss 4.956085205078125\n",
      "Epoch 4207: train loss 4.956085205078125 val loss 4.955811500549316\n",
      "Epoch 4208: train loss 4.955811500549316 val loss 4.956793785095215\n",
      "Epoch 4209: train loss 4.956793785095215 val loss 4.956027507781982\n",
      "Epoch 4210: train loss 4.956027507781982 val loss 4.957140922546387\n",
      "Epoch 4211: train loss 4.957140922546387 val loss 4.956760406494141\n",
      "Epoch 4212: train loss 4.956760406494141 val loss 4.956343650817871\n",
      "Epoch 4213: train loss 4.956343650817871 val loss 4.956307411193848\n",
      "Epoch 4214: train loss 4.956307411193848 val loss 4.956225395202637\n",
      "Epoch 4215: train loss 4.956225395202637 val loss 4.955876350402832\n",
      "Epoch 4216: train loss 4.955876350402832 val loss 4.956672668457031\n",
      "Epoch 4217: train loss 4.956672668457031 val loss 4.956590175628662\n",
      "Epoch 4218: train loss 4.956590175628662 val loss 4.955474853515625\n",
      "Epoch 4219: train loss 4.955474853515625 val loss 4.955606460571289\n",
      "Epoch 4220: train loss 4.955606460571289 val loss 4.955512523651123\n",
      "Epoch 4221: train loss 4.955512523651123 val loss 4.955167770385742\n",
      "Epoch 4222: train loss 4.955167770385742 val loss 4.9558258056640625\n",
      "Epoch 4223: train loss 4.9558258056640625 val loss 4.954891204833984\n",
      "Epoch 4224: train loss 4.954891204833984 val loss 4.9557390213012695\n",
      "Epoch 4225: train loss 4.9557390213012695 val loss 4.95466423034668\n",
      "Epoch 4226: train loss 4.95466423034668 val loss 4.955702304840088\n",
      "Epoch 4227: train loss 4.955702304840088 val loss 4.954651355743408\n",
      "Epoch 4228: train loss 4.954651355743408 val loss 4.954794406890869\n",
      "Epoch 4229: train loss 4.954794406890869 val loss 4.954726696014404\n",
      "Epoch 4230: train loss 4.954726696014404 val loss 4.954605579376221\n",
      "Epoch 4231: train loss 4.954605579376221 val loss 4.954462051391602\n",
      "Epoch 4232: train loss 4.954462051391602 val loss 4.954172134399414\n",
      "Epoch 4233: train loss 4.954172134399414 val loss 4.954237461090088\n",
      "Epoch 4234: train loss 4.954237461090088 val loss 4.9547014236450195\n",
      "Epoch 4235: train loss 4.9547014236450195 val loss 4.9540791511535645\n",
      "Epoch 4236: train loss 4.9540791511535645 val loss 4.95396614074707\n",
      "Epoch 4237: train loss 4.95396614074707 val loss 4.953781604766846\n",
      "Epoch 4238: train loss 4.953781604766846 val loss 4.954074859619141\n",
      "Epoch 4239: train loss 4.954074859619141 val loss 4.95416784286499\n",
      "Epoch 4240: train loss 4.95416784286499 val loss 4.953734874725342\n",
      "Epoch 4241: train loss 4.953734874725342 val loss 4.953517913818359\n",
      "Epoch 4242: train loss 4.953517913818359 val loss 4.953655242919922\n",
      "Epoch 4243: train loss 4.953655242919922 val loss 4.953775405883789\n",
      "Epoch 4244: train loss 4.953775405883789 val loss 4.953526496887207\n",
      "Epoch 4245: train loss 4.953526496887207 val loss 4.953365802764893\n",
      "Epoch 4246: train loss 4.953365802764893 val loss 4.954151630401611\n",
      "Epoch 4247: train loss 4.954151630401611 val loss 4.953176498413086\n",
      "Epoch 4248: train loss 4.953176498413086 val loss 4.953845977783203\n",
      "Epoch 4249: train loss 4.953845977783203 val loss 4.9532270431518555\n",
      "Epoch 4250: train loss 4.9532270431518555 val loss 4.95328426361084\n",
      "Epoch 4251: train loss 4.95328426361084 val loss 4.953090667724609\n",
      "Epoch 4252: train loss 4.953090667724609 val loss 4.952831745147705\n",
      "Epoch 4253: train loss 4.952831745147705 val loss 4.953637599945068\n",
      "Epoch 4254: train loss 4.953637599945068 val loss 4.953214645385742\n",
      "Epoch 4255: train loss 4.953214645385742 val loss 4.953332901000977\n",
      "Epoch 4256: train loss 4.953332901000977 val loss 4.953232288360596\n",
      "Epoch 4257: train loss 4.953232288360596 val loss 4.952495098114014\n",
      "Epoch 4258: train loss 4.952495098114014 val loss 4.952864170074463\n",
      "Epoch 4259: train loss 4.952864170074463 val loss 4.952155113220215\n",
      "Epoch 4260: train loss 4.952155113220215 val loss 4.952914237976074\n",
      "Epoch 4261: train loss 4.952914237976074 val loss 4.952232360839844\n",
      "Epoch 4262: train loss 4.952232360839844 val loss 4.953029155731201\n",
      "Epoch 4263: train loss 4.953029155731201 val loss 4.9519758224487305\n",
      "Epoch 4264: train loss 4.9519758224487305 val loss 4.95235538482666\n",
      "Epoch 4265: train loss 4.95235538482666 val loss 4.952425479888916\n",
      "Epoch 4266: train loss 4.952425479888916 val loss 4.951765537261963\n",
      "Epoch 4267: train loss 4.951765537261963 val loss 4.951903820037842\n",
      "Epoch 4268: train loss 4.951903820037842 val loss 4.951494216918945\n",
      "Epoch 4269: train loss 4.951494216918945 val loss 4.951265335083008\n",
      "Epoch 4270: train loss 4.951265335083008 val loss 4.951380252838135\n",
      "Epoch 4271: train loss 4.951380252838135 val loss 4.950717449188232\n",
      "Epoch 4272: train loss 4.950717449188232 val loss 4.950845241546631\n",
      "Epoch 4273: train loss 4.950845241546631 val loss 4.950748443603516\n",
      "Epoch 4274: train loss 4.950748443603516 val loss 4.950616836547852\n",
      "Epoch 4275: train loss 4.950616836547852 val loss 4.950547218322754\n",
      "Epoch 4276: train loss 4.950547218322754 val loss 4.950428485870361\n",
      "Epoch 4277: train loss 4.950428485870361 val loss 4.950179100036621\n",
      "Epoch 4278: train loss 4.950179100036621 val loss 4.949924468994141\n",
      "Epoch 4279: train loss 4.949924468994141 val loss 4.949702262878418\n",
      "Epoch 4280: train loss 4.949702262878418 val loss 4.9496049880981445\n",
      "Epoch 4281: train loss 4.9496049880981445 val loss 4.949535369873047\n",
      "Epoch 4282: train loss 4.949535369873047 val loss 4.949481964111328\n",
      "Epoch 4283: train loss 4.949481964111328 val loss 4.949365615844727\n",
      "Epoch 4284: train loss 4.949365615844727 val loss 4.948956489562988\n",
      "Epoch 4285: train loss 4.948956489562988 val loss 4.948663711547852\n",
      "Epoch 4286: train loss 4.948663711547852 val loss 4.9487624168396\n",
      "Epoch 4287: train loss 4.9487624168396 val loss 4.948535442352295\n",
      "Epoch 4288: train loss 4.948535442352295 val loss 4.9484758377075195\n",
      "Epoch 4289: train loss 4.9484758377075195 val loss 4.948296546936035\n",
      "Epoch 4290: train loss 4.948296546936035 val loss 4.947924613952637\n",
      "Epoch 4291: train loss 4.947924613952637 val loss 4.947827339172363\n",
      "Epoch 4292: train loss 4.947827339172363 val loss 4.947638034820557\n",
      "Epoch 4293: train loss 4.947638034820557 val loss 4.9476704597473145\n",
      "Epoch 4294: train loss 4.9476704597473145 val loss 4.94734001159668\n",
      "Epoch 4295: train loss 4.94734001159668 val loss 4.947157859802246\n",
      "Epoch 4296: train loss 4.947157859802246 val loss 4.946605682373047\n",
      "Epoch 4297: train loss 4.946605682373047 val loss 4.946498394012451\n",
      "Epoch 4298: train loss 4.946498394012451 val loss 4.946253776550293\n",
      "Epoch 4299: train loss 4.946253776550293 val loss 4.946218013763428\n",
      "Epoch 4300: train loss 4.946218013763428 val loss 4.9459123611450195\n",
      "Epoch 4301: train loss 4.9459123611450195 val loss 4.945552825927734\n",
      "Epoch 4302: train loss 4.945552825927734 val loss 4.945200443267822\n",
      "Epoch 4303: train loss 4.945200443267822 val loss 4.944721698760986\n",
      "Epoch 4304: train loss 4.944721698760986 val loss 4.94443941116333\n",
      "Epoch 4305: train loss 4.94443941116333 val loss 4.944320201873779\n",
      "Epoch 4306: train loss 4.944320201873779 val loss 4.944289207458496\n",
      "Epoch 4307: train loss 4.944289207458496 val loss 4.943943977355957\n",
      "Epoch 4308: train loss 4.943943977355957 val loss 4.943300247192383\n",
      "Epoch 4309: train loss 4.943300247192383 val loss 4.943498134613037\n",
      "Epoch 4310: train loss 4.943498134613037 val loss 4.942636489868164\n",
      "Epoch 4311: train loss 4.942636489868164 val loss 4.94242525100708\n",
      "Epoch 4312: train loss 4.94242525100708 val loss 4.9416303634643555\n",
      "Epoch 4313: train loss 4.9416303634643555 val loss 4.9412970542907715\n",
      "Epoch 4314: train loss 4.9412970542907715 val loss 4.940837860107422\n",
      "Epoch 4315: train loss 4.940837860107422 val loss 4.9402947425842285\n",
      "Epoch 4316: train loss 4.9402947425842285 val loss 4.939774036407471\n",
      "Epoch 4317: train loss 4.939774036407471 val loss 4.93941068649292\n",
      "Epoch 4318: train loss 4.93941068649292 val loss 4.938546657562256\n",
      "Epoch 4319: train loss 4.938546657562256 val loss 4.9378814697265625\n",
      "Epoch 4320: train loss 4.9378814697265625 val loss 4.93705940246582\n",
      "Epoch 4321: train loss 4.93705940246582 val loss 4.937010765075684\n",
      "Epoch 4322: train loss 4.937010765075684 val loss 4.936267852783203\n",
      "Epoch 4323: train loss 4.936267852783203 val loss 4.935679912567139\n",
      "Epoch 4324: train loss 4.935679912567139 val loss 4.934736251831055\n",
      "Epoch 4325: train loss 4.934736251831055 val loss 4.933951377868652\n",
      "Epoch 4326: train loss 4.933951377868652 val loss 4.932919025421143\n",
      "Epoch 4327: train loss 4.932919025421143 val loss 4.931664943695068\n",
      "Epoch 4328: train loss 4.931664943695068 val loss 4.930713176727295\n",
      "Epoch 4329: train loss 4.930713176727295 val loss 4.92955207824707\n",
      "Epoch 4330: train loss 4.92955207824707 val loss 4.928739547729492\n",
      "Epoch 4331: train loss 4.928739547729492 val loss 4.926860332489014\n",
      "Epoch 4332: train loss 4.926860332489014 val loss 4.925656318664551\n",
      "Epoch 4333: train loss 4.925656318664551 val loss 4.923713684082031\n",
      "Epoch 4334: train loss 4.923713684082031 val loss 4.922469139099121\n",
      "Epoch 4335: train loss 4.922469139099121 val loss 4.920248031616211\n",
      "Epoch 4336: train loss 4.920248031616211 val loss 4.91961669921875\n",
      "Epoch 4337: train loss 4.91961669921875 val loss 4.9163641929626465\n",
      "Epoch 4338: train loss 4.9163641929626465 val loss 4.914987564086914\n",
      "Epoch 4339: train loss 4.914987564086914 val loss 4.911174774169922\n",
      "Epoch 4340: train loss 4.911174774169922 val loss 4.908890247344971\n",
      "Epoch 4341: train loss 4.908890247344971 val loss 4.905614852905273\n",
      "Epoch 4342: train loss 4.905614852905273 val loss 4.901799201965332\n",
      "Epoch 4343: train loss 4.901799201965332 val loss 4.8999457359313965\n",
      "Epoch 4344: train loss 4.8999457359313965 val loss 4.893344879150391\n",
      "Epoch 4345: train loss 4.893344879150391 val loss 4.890146255493164\n",
      "Epoch 4346: train loss 4.890146255493164 val loss 4.883578777313232\n",
      "Epoch 4347: train loss 4.883578777313232 val loss 4.876970291137695\n",
      "Epoch 4348: train loss 4.876970291137695 val loss 4.87078857421875\n",
      "Epoch 4349: train loss 4.87078857421875 val loss 4.862732410430908\n",
      "Epoch 4350: train loss 4.862732410430908 val loss 4.854722499847412\n",
      "Epoch 4351: train loss 4.854722499847412 val loss 4.841392993927002\n",
      "Epoch 4352: train loss 4.841392993927002 val loss 4.8294572830200195\n",
      "Epoch 4353: train loss 4.8294572830200195 val loss 4.814472675323486\n",
      "Epoch 4354: train loss 4.814472675323486 val loss 4.79638147354126\n",
      "Epoch 4355: train loss 4.79638147354126 val loss 4.774944305419922\n",
      "Epoch 4356: train loss 4.774944305419922 val loss 4.747066974639893\n",
      "Epoch 4357: train loss 4.747066974639893 val loss 4.714888572692871\n",
      "Epoch 4358: train loss 4.714888572692871 val loss 4.673850059509277\n",
      "Epoch 4359: train loss 4.673850059509277 val loss 4.648134708404541\n",
      "Epoch 4360: train loss 4.648134708404541 val loss 4.599209308624268\n",
      "Epoch 4361: train loss 4.599209308624268 val loss 4.596366882324219\n",
      "Epoch 4362: train loss 4.596366882324219 val loss 4.397443771362305\n",
      "Epoch 4363: train loss 4.397443771362305 val loss 4.901124477386475\n",
      "Epoch 4364: train loss 4.901124477386475 val loss 4.617153167724609\n",
      "Epoch 4365: train loss 4.617153167724609 val loss 4.9158172607421875\n",
      "Epoch 4366: train loss 4.9158172607421875 val loss 4.573023796081543\n",
      "Epoch 4367: train loss 4.573023796081543 val loss 4.2285051345825195\n",
      "Epoch 4368: train loss 4.2285051345825195 val loss 4.166455268859863\n",
      "Epoch 4369: train loss 4.166455268859863 val loss 4.290767669677734\n",
      "Epoch 4370: train loss 4.290767669677734 val loss 4.240513801574707\n",
      "Epoch 4371: train loss 4.240513801574707 val loss 3.8612136840820312\n",
      "Epoch 4372: train loss 3.8612136840820312 val loss 3.82841157913208\n",
      "Epoch 4373: train loss 3.82841157913208 val loss 3.976478099822998\n",
      "Epoch 4374: train loss 3.976478099822998 val loss 3.5181772708892822\n",
      "Epoch 4375: train loss 3.5181772708892822 val loss 3.785935878753662\n",
      "Epoch 4376: train loss 3.785935878753662 val loss 3.4173150062561035\n",
      "Epoch 4377: train loss 3.4173150062561035 val loss 4.019455432891846\n",
      "Epoch 4378: train loss 4.019455432891846 val loss 3.354337692260742\n",
      "Epoch 4379: train loss 3.354337692260742 val loss 3.8066868782043457\n",
      "Epoch 4380: train loss 3.8066868782043457 val loss 3.407083034515381\n",
      "Epoch 4381: train loss 3.407083034515381 val loss 3.901632308959961\n",
      "Epoch 4382: train loss 3.901632308959961 val loss 3.573106288909912\n",
      "Epoch 4383: train loss 3.573106288909912 val loss 3.52748703956604\n",
      "Epoch 4384: train loss 3.52748703956604 val loss 3.6388635635375977\n",
      "Epoch 4385: train loss 3.6388635635375977 val loss 3.0958189964294434\n",
      "Epoch 4386: train loss 3.0958189964294434 val loss 3.481605052947998\n",
      "Epoch 4387: train loss 3.481605052947998 val loss 3.3733863830566406\n",
      "Epoch 4388: train loss 3.3733863830566406 val loss 3.0378830432891846\n",
      "Epoch 4389: train loss 3.0378830432891846 val loss 3.294949531555176\n",
      "Epoch 4390: train loss 3.294949531555176 val loss 3.068084955215454\n",
      "Epoch 4391: train loss 3.068084955215454 val loss 3.171900749206543\n",
      "Epoch 4392: train loss 3.171900749206543 val loss 3.2576537132263184\n",
      "Epoch 4393: train loss 3.2576537132263184 val loss 3.148188829421997\n",
      "Epoch 4394: train loss 3.148188829421997 val loss 3.085784435272217\n",
      "Epoch 4395: train loss 3.085784435272217 val loss 3.218010902404785\n",
      "Epoch 4396: train loss 3.218010902404785 val loss 3.0826921463012695\n",
      "Epoch 4397: train loss 3.0826921463012695 val loss 3.1044697761535645\n",
      "Epoch 4398: train loss 3.1044697761535645 val loss 3.1482467651367188\n",
      "Epoch 4399: train loss 3.1482467651367188 val loss 3.077688217163086\n",
      "Epoch 4400: train loss 3.077688217163086 val loss 3.0207064151763916\n",
      "Epoch 4401: train loss 3.0207064151763916 val loss 3.0814764499664307\n",
      "Epoch 4402: train loss 3.0814764499664307 val loss 2.972785711288452\n",
      "Epoch 4403: train loss 2.972785711288452 val loss 2.9901020526885986\n",
      "Epoch 4404: train loss 2.9901020526885986 val loss 2.967745065689087\n",
      "Epoch 4405: train loss 2.967745065689087 val loss 2.9276695251464844\n",
      "Epoch 4406: train loss 2.9276695251464844 val loss 2.928244113922119\n",
      "Epoch 4407: train loss 2.928244113922119 val loss 2.9238743782043457\n",
      "Epoch 4408: train loss 2.9238743782043457 val loss 2.8982746601104736\n",
      "Epoch 4409: train loss 2.8982746601104736 val loss 2.9165029525756836\n",
      "Epoch 4410: train loss 2.9165029525756836 val loss 2.87703275680542\n",
      "Epoch 4411: train loss 2.87703275680542 val loss 2.8975512981414795\n",
      "Epoch 4412: train loss 2.8975512981414795 val loss 2.886138439178467\n",
      "Epoch 4413: train loss 2.886138439178467 val loss 2.8712871074676514\n",
      "Epoch 4414: train loss 2.8712871074676514 val loss 2.8783907890319824\n",
      "Epoch 4415: train loss 2.8783907890319824 val loss 2.860785961151123\n",
      "Epoch 4416: train loss 2.860785961151123 val loss 2.8694050312042236\n",
      "Epoch 4417: train loss 2.8694050312042236 val loss 2.8606672286987305\n",
      "Epoch 4418: train loss 2.8606672286987305 val loss 2.8414924144744873\n",
      "Epoch 4419: train loss 2.8414924144744873 val loss 2.8549842834472656\n",
      "Epoch 4420: train loss 2.8549842834472656 val loss 2.861333131790161\n",
      "Epoch 4421: train loss 2.861333131790161 val loss 2.825294256210327\n",
      "Epoch 4422: train loss 2.825294256210327 val loss 2.8273510932922363\n",
      "Epoch 4423: train loss 2.8273510932922363 val loss 2.8526127338409424\n",
      "Epoch 4424: train loss 2.8526127338409424 val loss 2.8102760314941406\n",
      "Epoch 4425: train loss 2.8102760314941406 val loss 2.8139216899871826\n",
      "Epoch 4426: train loss 2.8139216899871826 val loss 2.817598819732666\n",
      "Epoch 4427: train loss 2.817598819732666 val loss 2.796452760696411\n",
      "Epoch 4428: train loss 2.796452760696411 val loss 2.829186201095581\n",
      "Epoch 4429: train loss 2.829186201095581 val loss 2.8130080699920654\n",
      "Epoch 4430: train loss 2.8130080699920654 val loss 2.8062663078308105\n",
      "Epoch 4431: train loss 2.8062663078308105 val loss 2.821805477142334\n",
      "Epoch 4432: train loss 2.821805477142334 val loss 2.785585880279541\n",
      "Epoch 4433: train loss 2.785585880279541 val loss 2.801990509033203\n",
      "Epoch 4434: train loss 2.801990509033203 val loss 2.7805237770080566\n",
      "Epoch 4435: train loss 2.7805237770080566 val loss 2.781729221343994\n",
      "Epoch 4436: train loss 2.781729221343994 val loss 2.780128240585327\n",
      "Epoch 4437: train loss 2.780128240585327 val loss 2.772632360458374\n",
      "Epoch 4438: train loss 2.772632360458374 val loss 2.775717258453369\n",
      "Epoch 4439: train loss 2.775717258453369 val loss 2.76363205909729\n",
      "Epoch 4440: train loss 2.76363205909729 val loss 2.7650742530822754\n",
      "Epoch 4441: train loss 2.7650742530822754 val loss 2.759467840194702\n",
      "Epoch 4442: train loss 2.759467840194702 val loss 2.752622604370117\n",
      "Epoch 4443: train loss 2.752622604370117 val loss 2.749630928039551\n",
      "Epoch 4444: train loss 2.749630928039551 val loss 2.7460684776306152\n",
      "Epoch 4445: train loss 2.7460684776306152 val loss 2.743598699569702\n",
      "Epoch 4446: train loss 2.743598699569702 val loss 2.739776372909546\n",
      "Epoch 4447: train loss 2.739776372909546 val loss 2.735534191131592\n",
      "Epoch 4448: train loss 2.735534191131592 val loss 2.7437658309936523\n",
      "Epoch 4449: train loss 2.7437658309936523 val loss 2.7452621459960938\n",
      "Epoch 4450: train loss 2.7452621459960938 val loss 2.7318427562713623\n",
      "Epoch 4451: train loss 2.7318427562713623 val loss 2.7253100872039795\n",
      "Epoch 4452: train loss 2.7253100872039795 val loss 2.7213780879974365\n",
      "Epoch 4453: train loss 2.7213780879974365 val loss 2.718905448913574\n",
      "Epoch 4454: train loss 2.718905448913574 val loss 2.718273162841797\n",
      "Epoch 4455: train loss 2.718273162841797 val loss 2.7140817642211914\n",
      "Epoch 4456: train loss 2.7140817642211914 val loss 2.71376895904541\n",
      "Epoch 4457: train loss 2.71376895904541 val loss 2.708645820617676\n",
      "Epoch 4458: train loss 2.708645820617676 val loss 2.712376356124878\n",
      "Epoch 4459: train loss 2.712376356124878 val loss 2.717052459716797\n",
      "Epoch 4460: train loss 2.717052459716797 val loss 2.733071804046631\n",
      "Epoch 4461: train loss 2.733071804046631 val loss 2.7067172527313232\n",
      "Epoch 4462: train loss 2.7067172527313232 val loss 2.713331699371338\n",
      "Epoch 4463: train loss 2.713331699371338 val loss 2.7746872901916504\n",
      "Epoch 4464: train loss 2.7746872901916504 val loss 2.7009849548339844\n",
      "Epoch 4465: train loss 2.7009849548339844 val loss 2.7279934883117676\n",
      "Epoch 4466: train loss 2.7279934883117676 val loss 2.748568534851074\n",
      "Epoch 4467: train loss 2.748568534851074 val loss 2.6919984817504883\n",
      "Epoch 4468: train loss 2.6919984817504883 val loss 2.7177352905273438\n",
      "Epoch 4469: train loss 2.7177352905273438 val loss 2.735832452774048\n",
      "Epoch 4470: train loss 2.735832452774048 val loss 2.6852493286132812\n",
      "Epoch 4471: train loss 2.6852493286132812 val loss 2.713575839996338\n",
      "Epoch 4472: train loss 2.713575839996338 val loss 2.6986069679260254\n",
      "Epoch 4473: train loss 2.6986069679260254 val loss 2.6867923736572266\n",
      "Epoch 4474: train loss 2.6867923736572266 val loss 2.7291605472564697\n",
      "Epoch 4475: train loss 2.7291605472564697 val loss 2.689061164855957\n",
      "Epoch 4476: train loss 2.689061164855957 val loss 2.679621696472168\n",
      "Epoch 4477: train loss 2.679621696472168 val loss 2.685854911804199\n",
      "Epoch 4478: train loss 2.685854911804199 val loss 2.6707024574279785\n",
      "Epoch 4479: train loss 2.6707024574279785 val loss 2.690363883972168\n",
      "Epoch 4480: train loss 2.690363883972168 val loss 2.6822421550750732\n",
      "Epoch 4481: train loss 2.6822421550750732 val loss 2.6670141220092773\n",
      "Epoch 4482: train loss 2.6670141220092773 val loss 2.686066150665283\n",
      "Epoch 4483: train loss 2.686066150665283 val loss 2.670325517654419\n",
      "Epoch 4484: train loss 2.670325517654419 val loss 2.664966583251953\n",
      "Epoch 4485: train loss 2.664966583251953 val loss 2.6712756156921387\n",
      "Epoch 4486: train loss 2.6712756156921387 val loss 2.6629514694213867\n",
      "Epoch 4487: train loss 2.6629514694213867 val loss 2.668231725692749\n",
      "Epoch 4488: train loss 2.668231725692749 val loss 2.6783833503723145\n",
      "Epoch 4489: train loss 2.6783833503723145 val loss 2.654367208480835\n",
      "Epoch 4490: train loss 2.654367208480835 val loss 2.658311367034912\n",
      "Epoch 4491: train loss 2.658311367034912 val loss 2.6747279167175293\n",
      "Epoch 4492: train loss 2.6747279167175293 val loss 2.6507070064544678\n",
      "Epoch 4493: train loss 2.6507070064544678 val loss 2.659557819366455\n",
      "Epoch 4494: train loss 2.659557819366455 val loss 2.6840343475341797\n",
      "Epoch 4495: train loss 2.6840343475341797 val loss 2.644832134246826\n",
      "Epoch 4496: train loss 2.644832134246826 val loss 2.661004066467285\n",
      "Epoch 4497: train loss 2.661004066467285 val loss 2.6588144302368164\n",
      "Epoch 4498: train loss 2.6588144302368164 val loss 2.641867160797119\n",
      "Epoch 4499: train loss 2.641867160797119 val loss 2.6468732357025146\n",
      "Epoch 4500: train loss 2.6468732357025146 val loss 2.6526265144348145\n",
      "Epoch 4501: train loss 2.6526265144348145 val loss 2.6473774909973145\n",
      "Epoch 4502: train loss 2.6473774909973145 val loss 2.637432813644409\n",
      "Epoch 4503: train loss 2.637432813644409 val loss 2.649440288543701\n",
      "Epoch 4504: train loss 2.649440288543701 val loss 2.6415302753448486\n",
      "Epoch 4505: train loss 2.6415302753448486 val loss 2.632272958755493\n",
      "Epoch 4506: train loss 2.632272958755493 val loss 2.632932662963867\n",
      "Epoch 4507: train loss 2.632932662963867 val loss 2.648393154144287\n",
      "Epoch 4508: train loss 2.648393154144287 val loss 2.6336257457733154\n",
      "Epoch 4509: train loss 2.6336257457733154 val loss 2.6250240802764893\n",
      "Epoch 4510: train loss 2.6250240802764893 val loss 2.6256606578826904\n",
      "Epoch 4511: train loss 2.6256606578826904 val loss 2.622772216796875\n",
      "Epoch 4512: train loss 2.622772216796875 val loss 2.626908540725708\n",
      "Epoch 4513: train loss 2.626908540725708 val loss 2.640821933746338\n",
      "Epoch 4514: train loss 2.640821933746338 val loss 2.619540214538574\n",
      "Epoch 4515: train loss 2.619540214538574 val loss 2.634507894515991\n",
      "Epoch 4516: train loss 2.634507894515991 val loss 2.6676254272460938\n",
      "Epoch 4517: train loss 2.6676254272460938 val loss 2.63261342048645\n",
      "Epoch 4518: train loss 2.63261342048645 val loss 2.6152961254119873\n",
      "Epoch 4519: train loss 2.6152961254119873 val loss 2.6191391944885254\n",
      "Epoch 4520: train loss 2.6191391944885254 val loss 2.616788625717163\n",
      "Epoch 4521: train loss 2.616788625717163 val loss 2.612590789794922\n",
      "Epoch 4522: train loss 2.612590789794922 val loss 2.612283706665039\n",
      "Epoch 4523: train loss 2.612283706665039 val loss 2.610102653503418\n",
      "Epoch 4524: train loss 2.610102653503418 val loss 2.6093506813049316\n",
      "Epoch 4525: train loss 2.6093506813049316 val loss 2.6066231727600098\n",
      "Epoch 4526: train loss 2.6066231727600098 val loss 2.6069278717041016\n",
      "Epoch 4527: train loss 2.6069278717041016 val loss 2.6053779125213623\n",
      "Epoch 4528: train loss 2.6053779125213623 val loss 2.6031112670898438\n",
      "Epoch 4529: train loss 2.6031112670898438 val loss 2.6030445098876953\n",
      "Epoch 4530: train loss 2.6030445098876953 val loss 2.6037354469299316\n",
      "Epoch 4531: train loss 2.6037354469299316 val loss 2.6017746925354004\n",
      "Epoch 4532: train loss 2.6017746925354004 val loss 2.6011829376220703\n",
      "Epoch 4533: train loss 2.6011829376220703 val loss 2.5980544090270996\n",
      "Epoch 4534: train loss 2.5980544090270996 val loss 2.5977861881256104\n",
      "Epoch 4535: train loss 2.5977861881256104 val loss 2.595907688140869\n",
      "Epoch 4536: train loss 2.595907688140869 val loss 2.5953845977783203\n",
      "Epoch 4537: train loss 2.5953845977783203 val loss 2.592966318130493\n",
      "Epoch 4538: train loss 2.592966318130493 val loss 2.591683864593506\n",
      "Epoch 4539: train loss 2.591683864593506 val loss 2.591754674911499\n",
      "Epoch 4540: train loss 2.591754674911499 val loss 2.5896925926208496\n",
      "Epoch 4541: train loss 2.5896925926208496 val loss 2.588801383972168\n",
      "Epoch 4542: train loss 2.588801383972168 val loss 2.5866615772247314\n",
      "Epoch 4543: train loss 2.5866615772247314 val loss 2.586310863494873\n",
      "Epoch 4544: train loss 2.586310863494873 val loss 2.58636474609375\n",
      "Epoch 4545: train loss 2.58636474609375 val loss 2.584214210510254\n",
      "Epoch 4546: train loss 2.584214210510254 val loss 2.581782817840576\n",
      "Epoch 4547: train loss 2.581782817840576 val loss 2.5807390213012695\n",
      "Epoch 4548: train loss 2.5807390213012695 val loss 2.5803678035736084\n",
      "Epoch 4549: train loss 2.5803678035736084 val loss 2.580129623413086\n",
      "Epoch 4550: train loss 2.580129623413086 val loss 2.5768489837646484\n",
      "Epoch 4551: train loss 2.5768489837646484 val loss 2.5756330490112305\n",
      "Epoch 4552: train loss 2.5756330490112305 val loss 2.575420618057251\n",
      "Epoch 4553: train loss 2.575420618057251 val loss 2.5729362964630127\n",
      "Epoch 4554: train loss 2.5729362964630127 val loss 2.5723159313201904\n",
      "Epoch 4555: train loss 2.5723159313201904 val loss 2.5703001022338867\n",
      "Epoch 4556: train loss 2.5703001022338867 val loss 2.573537826538086\n",
      "Epoch 4557: train loss 2.573537826538086 val loss 2.571864366531372\n",
      "Epoch 4558: train loss 2.571864366531372 val loss 2.5664522647857666\n",
      "Epoch 4559: train loss 2.5664522647857666 val loss 2.5671725273132324\n",
      "Epoch 4560: train loss 2.5671725273132324 val loss 2.565842628479004\n",
      "Epoch 4561: train loss 2.565842628479004 val loss 2.5626087188720703\n",
      "Epoch 4562: train loss 2.5626087188720703 val loss 2.5630271434783936\n",
      "Epoch 4563: train loss 2.5630271434783936 val loss 2.559476137161255\n",
      "Epoch 4564: train loss 2.559476137161255 val loss 2.560490131378174\n",
      "Epoch 4565: train loss 2.560490131378174 val loss 2.5564846992492676\n",
      "Epoch 4566: train loss 2.5564846992492676 val loss 2.5566935539245605\n",
      "Epoch 4567: train loss 2.5566935539245605 val loss 2.5535969734191895\n",
      "Epoch 4568: train loss 2.5535969734191895 val loss 2.553154230117798\n",
      "Epoch 4569: train loss 2.553154230117798 val loss 2.552748918533325\n",
      "Epoch 4570: train loss 2.552748918533325 val loss 2.549449920654297\n",
      "Epoch 4571: train loss 2.549449920654297 val loss 2.552244186401367\n",
      "Epoch 4572: train loss 2.552244186401367 val loss 2.5465636253356934\n",
      "Epoch 4573: train loss 2.5465636253356934 val loss 2.5460424423217773\n",
      "Epoch 4574: train loss 2.5460424423217773 val loss 2.5435149669647217\n",
      "Epoch 4575: train loss 2.5435149669647217 val loss 2.545743942260742\n",
      "Epoch 4576: train loss 2.545743942260742 val loss 2.5410566329956055\n",
      "Epoch 4577: train loss 2.5410566329956055 val loss 2.5416171550750732\n",
      "Epoch 4578: train loss 2.5416171550750732 val loss 2.544304370880127\n",
      "Epoch 4579: train loss 2.544304370880127 val loss 2.5360898971557617\n",
      "Epoch 4580: train loss 2.5360898971557617 val loss 2.5415782928466797\n",
      "Epoch 4581: train loss 2.5415782928466797 val loss 2.549511671066284\n",
      "Epoch 4582: train loss 2.549511671066284 val loss 2.5370709896087646\n",
      "Epoch 4583: train loss 2.5370709896087646 val loss 2.5311193466186523\n",
      "Epoch 4584: train loss 2.5311193466186523 val loss 2.537381887435913\n",
      "Epoch 4585: train loss 2.537381887435913 val loss 2.528256416320801\n",
      "Epoch 4586: train loss 2.528256416320801 val loss 2.5324606895446777\n",
      "Epoch 4587: train loss 2.5324606895446777 val loss 2.555173397064209\n",
      "Epoch 4588: train loss 2.555173397064209 val loss 2.568523406982422\n",
      "Epoch 4589: train loss 2.568523406982422 val loss 2.550419807434082\n",
      "Epoch 4590: train loss 2.550419807434082 val loss 2.52541446685791\n",
      "Epoch 4591: train loss 2.52541446685791 val loss 2.520982265472412\n",
      "Epoch 4592: train loss 2.520982265472412 val loss 2.520481824874878\n",
      "Epoch 4593: train loss 2.520481824874878 val loss 2.519260883331299\n",
      "Epoch 4594: train loss 2.519260883331299 val loss 2.517475128173828\n",
      "Epoch 4595: train loss 2.517475128173828 val loss 2.5174098014831543\n",
      "Epoch 4596: train loss 2.5174098014831543 val loss 2.5149343013763428\n",
      "Epoch 4597: train loss 2.5149343013763428 val loss 2.514031410217285\n",
      "Epoch 4598: train loss 2.514031410217285 val loss 2.512178421020508\n",
      "Epoch 4599: train loss 2.512178421020508 val loss 2.512148857116699\n",
      "Epoch 4600: train loss 2.512148857116699 val loss 2.5101938247680664\n",
      "Epoch 4601: train loss 2.5101938247680664 val loss 2.5089051723480225\n",
      "Epoch 4602: train loss 2.5089051723480225 val loss 2.5075643062591553\n",
      "Epoch 4603: train loss 2.5075643062591553 val loss 2.505312919616699\n",
      "Epoch 4604: train loss 2.505312919616699 val loss 2.50526762008667\n",
      "Epoch 4605: train loss 2.50526762008667 val loss 2.50394868850708\n",
      "Epoch 4606: train loss 2.50394868850708 val loss 2.5018484592437744\n",
      "Epoch 4607: train loss 2.5018484592437744 val loss 2.501194477081299\n",
      "Epoch 4608: train loss 2.501194477081299 val loss 2.4991564750671387\n",
      "Epoch 4609: train loss 2.4991564750671387 val loss 2.5009682178497314\n",
      "Epoch 4610: train loss 2.5009682178497314 val loss 2.5167603492736816\n",
      "Epoch 4611: train loss 2.5167603492736816 val loss 2.527863025665283\n",
      "Epoch 4612: train loss 2.527863025665283 val loss 2.5103445053100586\n",
      "Epoch 4613: train loss 2.5103445053100586 val loss 2.4945263862609863\n",
      "Epoch 4614: train loss 2.4945263862609863 val loss 2.527695655822754\n",
      "Epoch 4615: train loss 2.527695655822754 val loss 2.5365946292877197\n",
      "Epoch 4616: train loss 2.5365946292877197 val loss 2.5181732177734375\n",
      "Epoch 4617: train loss 2.5181732177734375 val loss 2.490046501159668\n",
      "Epoch 4618: train loss 2.490046501159668 val loss 2.4945192337036133\n",
      "Epoch 4619: train loss 2.4945192337036133 val loss 2.4903831481933594\n",
      "Epoch 4620: train loss 2.4903831481933594 val loss 2.486961841583252\n",
      "Epoch 4621: train loss 2.486961841583252 val loss 2.4830636978149414\n",
      "Epoch 4622: train loss 2.4830636978149414 val loss 2.4825334548950195\n",
      "Epoch 4623: train loss 2.4825334548950195 val loss 2.4904677867889404\n",
      "Epoch 4624: train loss 2.4904677867889404 val loss 2.5033907890319824\n",
      "Epoch 4625: train loss 2.5033907890319824 val loss 2.4983296394348145\n",
      "Epoch 4626: train loss 2.4983296394348145 val loss 2.4862561225891113\n",
      "Epoch 4627: train loss 2.4862561225891113 val loss 2.4844157695770264\n",
      "Epoch 4628: train loss 2.4844157695770264 val loss 2.49396014213562\n",
      "Epoch 4629: train loss 2.49396014213562 val loss 2.49838924407959\n",
      "Epoch 4630: train loss 2.49838924407959 val loss 2.483722686767578\n",
      "Epoch 4631: train loss 2.483722686767578 val loss 2.472896099090576\n",
      "Epoch 4632: train loss 2.472896099090576 val loss 2.4851419925689697\n",
      "Epoch 4633: train loss 2.4851419925689697 val loss 2.495835304260254\n",
      "Epoch 4634: train loss 2.495835304260254 val loss 2.474944591522217\n",
      "Epoch 4635: train loss 2.474944591522217 val loss 2.4718945026397705\n",
      "Epoch 4636: train loss 2.4718945026397705 val loss 2.4801025390625\n",
      "Epoch 4637: train loss 2.4801025390625 val loss 2.469334840774536\n",
      "Epoch 4638: train loss 2.469334840774536 val loss 2.466686725616455\n",
      "Epoch 4639: train loss 2.466686725616455 val loss 2.476808547973633\n",
      "Epoch 4640: train loss 2.476808547973633 val loss 2.467414617538452\n",
      "Epoch 4641: train loss 2.467414617538452 val loss 2.4630126953125\n",
      "Epoch 4642: train loss 2.4630126953125 val loss 2.4629499912261963\n",
      "Epoch 4643: train loss 2.4629499912261963 val loss 2.4604616165161133\n",
      "Epoch 4644: train loss 2.4604616165161133 val loss 2.462686061859131\n",
      "Epoch 4645: train loss 2.462686061859131 val loss 2.469742774963379\n",
      "Epoch 4646: train loss 2.469742774963379 val loss 2.457512855529785\n",
      "Epoch 4647: train loss 2.457512855529785 val loss 2.470149517059326\n",
      "Epoch 4648: train loss 2.470149517059326 val loss 2.4912025928497314\n",
      "Epoch 4649: train loss 2.4912025928497314 val loss 2.469771385192871\n",
      "Epoch 4650: train loss 2.469771385192871 val loss 2.4526524543762207\n",
      "Epoch 4651: train loss 2.4526524543762207 val loss 2.4633378982543945\n",
      "Epoch 4652: train loss 2.4633378982543945 val loss 2.4518966674804688\n",
      "Epoch 4653: train loss 2.4518966674804688 val loss 2.4674389362335205\n",
      "Epoch 4654: train loss 2.4674389362335205 val loss 2.51104736328125\n",
      "Epoch 4655: train loss 2.51104736328125 val loss 2.4703290462493896\n",
      "Epoch 4656: train loss 2.4703290462493896 val loss 2.4455881118774414\n",
      "Epoch 4657: train loss 2.4455881118774414 val loss 2.4582910537719727\n",
      "Epoch 4658: train loss 2.4582910537719727 val loss 2.4727725982666016\n",
      "Epoch 4659: train loss 2.4727725982666016 val loss 2.451672077178955\n",
      "Epoch 4660: train loss 2.451672077178955 val loss 2.4436306953430176\n",
      "Epoch 4661: train loss 2.4436306953430176 val loss 2.4699904918670654\n",
      "Epoch 4662: train loss 2.4699904918670654 val loss 2.4564385414123535\n",
      "Epoch 4663: train loss 2.4564385414123535 val loss 2.438077926635742\n",
      "Epoch 4664: train loss 2.438077926635742 val loss 2.466189384460449\n",
      "Epoch 4665: train loss 2.466189384460449 val loss 2.453373670578003\n",
      "Epoch 4666: train loss 2.453373670578003 val loss 2.435412883758545\n",
      "Epoch 4667: train loss 2.435412883758545 val loss 2.4768590927124023\n",
      "Epoch 4668: train loss 2.4768590927124023 val loss 2.463408946990967\n",
      "Epoch 4669: train loss 2.463408946990967 val loss 2.4285054206848145\n",
      "Epoch 4670: train loss 2.4285054206848145 val loss 2.45469331741333\n",
      "Epoch 4671: train loss 2.45469331741333 val loss 2.445558547973633\n",
      "Epoch 4672: train loss 2.445558547973633 val loss 2.4280097484588623\n",
      "Epoch 4673: train loss 2.4280097484588623 val loss 2.459657669067383\n",
      "Epoch 4674: train loss 2.459657669067383 val loss 2.435696840286255\n",
      "Epoch 4675: train loss 2.435696840286255 val loss 2.4264862537384033\n",
      "Epoch 4676: train loss 2.4264862537384033 val loss 2.4527783393859863\n",
      "Epoch 4677: train loss 2.4527783393859863 val loss 2.4289865493774414\n",
      "Epoch 4678: train loss 2.4289865493774414 val loss 2.419703722000122\n",
      "Epoch 4679: train loss 2.419703722000122 val loss 2.431140899658203\n",
      "Epoch 4680: train loss 2.431140899658203 val loss 2.4124321937561035\n",
      "Epoch 4681: train loss 2.4124321937561035 val loss 2.42573881149292\n",
      "Epoch 4682: train loss 2.42573881149292 val loss 2.423501491546631\n",
      "Epoch 4683: train loss 2.423501491546631 val loss 2.4067957401275635\n",
      "Epoch 4684: train loss 2.4067957401275635 val loss 2.417381525039673\n",
      "Epoch 4685: train loss 2.417381525039673 val loss 2.409820079803467\n",
      "Epoch 4686: train loss 2.409820079803467 val loss 2.4007949829101562\n",
      "Epoch 4687: train loss 2.4007949829101562 val loss 2.4032177925109863\n",
      "Epoch 4688: train loss 2.4032177925109863 val loss 2.3988289833068848\n",
      "Epoch 4689: train loss 2.3988289833068848 val loss 2.3973443508148193\n",
      "Epoch 4690: train loss 2.3973443508148193 val loss 2.4017744064331055\n",
      "Epoch 4691: train loss 2.4017744064331055 val loss 2.3921196460723877\n",
      "Epoch 4692: train loss 2.3921196460723877 val loss 2.3990321159362793\n",
      "Epoch 4693: train loss 2.3990321159362793 val loss 2.4094176292419434\n",
      "Epoch 4694: train loss 2.4094176292419434 val loss 2.385260581970215\n",
      "Epoch 4695: train loss 2.385260581970215 val loss 2.4102277755737305\n",
      "Epoch 4696: train loss 2.4102277755737305 val loss 2.4073915481567383\n",
      "Epoch 4697: train loss 2.4073915481567383 val loss 2.378793716430664\n",
      "Epoch 4698: train loss 2.378793716430664 val loss 2.386127233505249\n",
      "Epoch 4699: train loss 2.386127233505249 val loss 2.3831162452697754\n",
      "Epoch 4700: train loss 2.3831162452697754 val loss 2.3724918365478516\n",
      "Epoch 4701: train loss 2.3724918365478516 val loss 2.3836112022399902\n",
      "Epoch 4702: train loss 2.3836112022399902 val loss 2.3899986743927\n",
      "Epoch 4703: train loss 2.3899986743927 val loss 2.366347312927246\n",
      "Epoch 4704: train loss 2.366347312927246 val loss 2.3778820037841797\n",
      "Epoch 4705: train loss 2.3778820037841797 val loss 2.3747217655181885\n",
      "Epoch 4706: train loss 2.3747217655181885 val loss 2.358989715576172\n",
      "Epoch 4707: train loss 2.358989715576172 val loss 2.369443893432617\n",
      "Epoch 4708: train loss 2.369443893432617 val loss 2.356921434402466\n",
      "Epoch 4709: train loss 2.356921434402466 val loss 2.3548617362976074\n",
      "Epoch 4710: train loss 2.3548617362976074 val loss 2.3568994998931885\n",
      "Epoch 4711: train loss 2.3568994998931885 val loss 2.347255229949951\n",
      "Epoch 4712: train loss 2.347255229949951 val loss 2.3564562797546387\n",
      "Epoch 4713: train loss 2.3564562797546387 val loss 2.349611759185791\n",
      "Epoch 4714: train loss 2.349611759185791 val loss 2.3436577320098877\n",
      "Epoch 4715: train loss 2.3436577320098877 val loss 2.3519415855407715\n",
      "Epoch 4716: train loss 2.3519415855407715 val loss 2.340308666229248\n",
      "Epoch 4717: train loss 2.340308666229248 val loss 2.3401927947998047\n",
      "Epoch 4718: train loss 2.3401927947998047 val loss 2.332991123199463\n",
      "Epoch 4719: train loss 2.332991123199463 val loss 2.328761100769043\n",
      "Epoch 4720: train loss 2.328761100769043 val loss 2.3371336460113525\n",
      "Epoch 4721: train loss 2.3371336460113525 val loss 2.3216283321380615\n",
      "Epoch 4722: train loss 2.3216283321380615 val loss 2.3335442543029785\n",
      "Epoch 4723: train loss 2.3335442543029785 val loss 2.332227945327759\n",
      "Epoch 4724: train loss 2.332227945327759 val loss 2.321453094482422\n",
      "Epoch 4725: train loss 2.321453094482422 val loss 2.3244707584381104\n",
      "Epoch 4726: train loss 2.3244707584381104 val loss 2.310135841369629\n",
      "Epoch 4727: train loss 2.310135841369629 val loss 2.3078091144561768\n",
      "Epoch 4728: train loss 2.3078091144561768 val loss 2.3106470108032227\n",
      "Epoch 4729: train loss 2.3106470108032227 val loss 2.30003023147583\n",
      "Epoch 4730: train loss 2.30003023147583 val loss 2.3023571968078613\n",
      "Epoch 4731: train loss 2.3023571968078613 val loss 2.295985698699951\n",
      "Epoch 4732: train loss 2.295985698699951 val loss 2.2931063175201416\n",
      "Epoch 4733: train loss 2.2931063175201416 val loss 2.291785717010498\n",
      "Epoch 4734: train loss 2.291785717010498 val loss 2.2875876426696777\n",
      "Epoch 4735: train loss 2.2875876426696777 val loss 2.285256862640381\n",
      "Epoch 4736: train loss 2.285256862640381 val loss 2.282222270965576\n",
      "Epoch 4737: train loss 2.282222270965576 val loss 2.2814316749572754\n",
      "Epoch 4738: train loss 2.2814316749572754 val loss 2.2775347232818604\n",
      "Epoch 4739: train loss 2.2775347232818604 val loss 2.275474786758423\n",
      "Epoch 4740: train loss 2.275474786758423 val loss 2.2723302841186523\n",
      "Epoch 4741: train loss 2.2723302841186523 val loss 2.2697882652282715\n",
      "Epoch 4742: train loss 2.2697882652282715 val loss 2.267206907272339\n",
      "Epoch 4743: train loss 2.267206907272339 val loss 2.264474391937256\n",
      "Epoch 4744: train loss 2.264474391937256 val loss 2.261979103088379\n",
      "Epoch 4745: train loss 2.261979103088379 val loss 2.2591652870178223\n",
      "Epoch 4746: train loss 2.2591652870178223 val loss 2.2565135955810547\n",
      "Epoch 4747: train loss 2.2565135955810547 val loss 2.2537856101989746\n",
      "Epoch 4748: train loss 2.2537856101989746 val loss 2.2504987716674805\n",
      "Epoch 4749: train loss 2.2504987716674805 val loss 2.247954845428467\n",
      "Epoch 4750: train loss 2.247954845428467 val loss 2.247877597808838\n",
      "Epoch 4751: train loss 2.247877597808838 val loss 2.2424821853637695\n",
      "Epoch 4752: train loss 2.2424821853637695 val loss 2.238955497741699\n",
      "Epoch 4753: train loss 2.238955497741699 val loss 2.237433671951294\n",
      "Epoch 4754: train loss 2.237433671951294 val loss 2.2334704399108887\n",
      "Epoch 4755: train loss 2.2334704399108887 val loss 2.233105182647705\n",
      "Epoch 4756: train loss 2.233105182647705 val loss 2.227419376373291\n",
      "Epoch 4757: train loss 2.227419376373291 val loss 2.2329258918762207\n",
      "Epoch 4758: train loss 2.2329258918762207 val loss 2.229292869567871\n",
      "Epoch 4759: train loss 2.229292869567871 val loss 2.2278544902801514\n",
      "Epoch 4760: train loss 2.2278544902801514 val loss 2.2172389030456543\n",
      "Epoch 4761: train loss 2.2172389030456543 val loss 2.2325925827026367\n",
      "Epoch 4762: train loss 2.2325925827026367 val loss 2.2475714683532715\n",
      "Epoch 4763: train loss 2.2475714683532715 val loss 2.2187514305114746\n",
      "Epoch 4764: train loss 2.2187514305114746 val loss 2.2216200828552246\n",
      "Epoch 4765: train loss 2.2216200828552246 val loss 2.205641269683838\n",
      "Epoch 4766: train loss 2.205641269683838 val loss 2.207658052444458\n",
      "Epoch 4767: train loss 2.207658052444458 val loss 2.199336528778076\n",
      "Epoch 4768: train loss 2.199336528778076 val loss 2.2059357166290283\n",
      "Epoch 4769: train loss 2.2059357166290283 val loss 2.1960277557373047\n",
      "Epoch 4770: train loss 2.1960277557373047 val loss 2.198641538619995\n",
      "Epoch 4771: train loss 2.198641538619995 val loss 2.1902432441711426\n",
      "Epoch 4772: train loss 2.1902432441711426 val loss 2.191108226776123\n",
      "Epoch 4773: train loss 2.191108226776123 val loss 2.185429096221924\n",
      "Epoch 4774: train loss 2.185429096221924 val loss 2.1862902641296387\n",
      "Epoch 4775: train loss 2.1862902641296387 val loss 2.180668830871582\n",
      "Epoch 4776: train loss 2.180668830871582 val loss 2.1812937259674072\n",
      "Epoch 4777: train loss 2.1812937259674072 val loss 2.1771700382232666\n",
      "Epoch 4778: train loss 2.1771700382232666 val loss 2.175013542175293\n",
      "Epoch 4779: train loss 2.175013542175293 val loss 2.1731996536254883\n",
      "Epoch 4780: train loss 2.1731996536254883 val loss 2.169577121734619\n",
      "Epoch 4781: train loss 2.169577121734619 val loss 2.168030023574829\n",
      "Epoch 4782: train loss 2.168030023574829 val loss 2.1647822856903076\n",
      "Epoch 4783: train loss 2.1647822856903076 val loss 2.1631546020507812\n",
      "Epoch 4784: train loss 2.1631546020507812 val loss 2.161271572113037\n",
      "Epoch 4785: train loss 2.161271572113037 val loss 2.1579172611236572\n",
      "Epoch 4786: train loss 2.1579172611236572 val loss 2.156581401824951\n",
      "Epoch 4787: train loss 2.156581401824951 val loss 2.154210329055786\n",
      "Epoch 4788: train loss 2.154210329055786 val loss 2.1532745361328125\n",
      "Epoch 4789: train loss 2.1532745361328125 val loss 2.1496386528015137\n",
      "Epoch 4790: train loss 2.1496386528015137 val loss 2.151170015335083\n",
      "Epoch 4791: train loss 2.151170015335083 val loss 2.1454520225524902\n",
      "Epoch 4792: train loss 2.1454520225524902 val loss 2.1455495357513428\n",
      "Epoch 4793: train loss 2.1455495357513428 val loss 2.1413047313690186\n",
      "Epoch 4794: train loss 2.1413047313690186 val loss 2.1423532962799072\n",
      "Epoch 4795: train loss 2.1423532962799072 val loss 2.1372318267822266\n",
      "Epoch 4796: train loss 2.1372318267822266 val loss 2.136281728744507\n",
      "Epoch 4797: train loss 2.136281728744507 val loss 2.1334004402160645\n",
      "Epoch 4798: train loss 2.1334004402160645 val loss 2.133322238922119\n",
      "Epoch 4799: train loss 2.133322238922119 val loss 2.1304149627685547\n",
      "Epoch 4800: train loss 2.1304149627685547 val loss 2.1283769607543945\n",
      "Epoch 4801: train loss 2.1283769607543945 val loss 2.1268582344055176\n",
      "Epoch 4802: train loss 2.1268582344055176 val loss 2.122668504714966\n",
      "Epoch 4803: train loss 2.122668504714966 val loss 2.1225497722625732\n",
      "Epoch 4804: train loss 2.1225497722625732 val loss 2.118155002593994\n",
      "Epoch 4805: train loss 2.118155002593994 val loss 2.1188511848449707\n",
      "Epoch 4806: train loss 2.1188511848449707 val loss 2.1149959564208984\n",
      "Epoch 4807: train loss 2.1149959564208984 val loss 2.115675449371338\n",
      "Epoch 4808: train loss 2.115675449371338 val loss 2.11331844329834\n",
      "Epoch 4809: train loss 2.11331844329834 val loss 2.1086459159851074\n",
      "Epoch 4810: train loss 2.1086459159851074 val loss 2.1073429584503174\n",
      "Epoch 4811: train loss 2.1073429584503174 val loss 2.1038379669189453\n",
      "Epoch 4812: train loss 2.1038379669189453 val loss 2.1032164096832275\n",
      "Epoch 4813: train loss 2.1032164096832275 val loss 2.1000168323516846\n",
      "Epoch 4814: train loss 2.1000168323516846 val loss 2.098771572113037\n",
      "Epoch 4815: train loss 2.098771572113037 val loss 2.0951380729675293\n",
      "Epoch 4816: train loss 2.0951380729675293 val loss 2.094939708709717\n",
      "Epoch 4817: train loss 2.094939708709717 val loss 2.091076135635376\n",
      "Epoch 4818: train loss 2.091076135635376 val loss 2.089359760284424\n",
      "Epoch 4819: train loss 2.089359760284424 val loss 2.086792230606079\n",
      "Epoch 4820: train loss 2.086792230606079 val loss 2.0852246284484863\n",
      "Epoch 4821: train loss 2.0852246284484863 val loss 2.082462787628174\n",
      "Epoch 4822: train loss 2.082462787628174 val loss 2.080517530441284\n",
      "Epoch 4823: train loss 2.080517530441284 val loss 2.077727794647217\n",
      "Epoch 4824: train loss 2.077727794647217 val loss 2.0766992568969727\n",
      "Epoch 4825: train loss 2.0766992568969727 val loss 2.0732250213623047\n",
      "Epoch 4826: train loss 2.0732250213623047 val loss 2.0712924003601074\n",
      "Epoch 4827: train loss 2.0712924003601074 val loss 2.0686047077178955\n",
      "Epoch 4828: train loss 2.0686047077178955 val loss 2.066615104675293\n",
      "Epoch 4829: train loss 2.066615104675293 val loss 2.0637447834014893\n",
      "Epoch 4830: train loss 2.0637447834014893 val loss 2.0614469051361084\n",
      "Epoch 4831: train loss 2.0614469051361084 val loss 2.0587635040283203\n",
      "Epoch 4832: train loss 2.0587635040283203 val loss 2.056427478790283\n",
      "Epoch 4833: train loss 2.056427478790283 val loss 2.053938865661621\n",
      "Epoch 4834: train loss 2.053938865661621 val loss 2.051750659942627\n",
      "Epoch 4835: train loss 2.051750659942627 val loss 2.049168348312378\n",
      "Epoch 4836: train loss 2.049168348312378 val loss 2.046555995941162\n",
      "Epoch 4837: train loss 2.046555995941162 val loss 2.0438907146453857\n",
      "Epoch 4838: train loss 2.0438907146453857 val loss 2.041274309158325\n",
      "Epoch 4839: train loss 2.041274309158325 val loss 2.0391342639923096\n",
      "Epoch 4840: train loss 2.0391342639923096 val loss 2.03660249710083\n",
      "Epoch 4841: train loss 2.03660249710083 val loss 2.0336227416992188\n",
      "Epoch 4842: train loss 2.0336227416992188 val loss 2.031647205352783\n",
      "Epoch 4843: train loss 2.031647205352783 val loss 2.0283079147338867\n",
      "Epoch 4844: train loss 2.0283079147338867 val loss 2.0257411003112793\n",
      "Epoch 4845: train loss 2.0257411003112793 val loss 2.0228755474090576\n",
      "Epoch 4846: train loss 2.0228755474090576 val loss 2.020156145095825\n",
      "Epoch 4847: train loss 2.020156145095825 val loss 2.017414093017578\n",
      "Epoch 4848: train loss 2.017414093017578 val loss 2.0147042274475098\n",
      "Epoch 4849: train loss 2.0147042274475098 val loss 2.0116066932678223\n",
      "Epoch 4850: train loss 2.0116066932678223 val loss 2.008924961090088\n",
      "Epoch 4851: train loss 2.008924961090088 val loss 2.0075583457946777\n",
      "Epoch 4852: train loss 2.0075583457946777 val loss 2.0044190883636475\n",
      "Epoch 4853: train loss 2.0044190883636475 val loss 2.0000221729278564\n",
      "Epoch 4854: train loss 2.0000221729278564 val loss 1.9972233772277832\n",
      "Epoch 4855: train loss 1.9972233772277832 val loss 1.9941396713256836\n",
      "Epoch 4856: train loss 1.9941396713256836 val loss 1.9909418821334839\n",
      "Epoch 4857: train loss 1.9909418821334839 val loss 1.9877500534057617\n",
      "Epoch 4858: train loss 1.9877500534057617 val loss 1.9855506420135498\n",
      "Epoch 4859: train loss 1.9855506420135498 val loss 1.9873589277267456\n",
      "Epoch 4860: train loss 1.9873589277267456 val loss 1.9790592193603516\n",
      "Epoch 4861: train loss 1.9790592193603516 val loss 1.9751478433609009\n",
      "Epoch 4862: train loss 1.9751478433609009 val loss 1.9734890460968018\n",
      "Epoch 4863: train loss 1.9734890460968018 val loss 1.9785584211349487\n",
      "Epoch 4864: train loss 1.9785584211349487 val loss 1.96626877784729\n",
      "Epoch 4865: train loss 1.96626877784729 val loss 1.9691123962402344\n",
      "Epoch 4866: train loss 1.9691123962402344 val loss 1.9720648527145386\n",
      "Epoch 4867: train loss 1.9720648527145386 val loss 1.9581167697906494\n",
      "Epoch 4868: train loss 1.9581167697906494 val loss 1.9798228740692139\n",
      "Epoch 4869: train loss 1.9798228740692139 val loss 1.9558296203613281\n",
      "Epoch 4870: train loss 1.9558296203613281 val loss 1.9648549556732178\n",
      "Epoch 4871: train loss 1.9648549556732178 val loss 1.9509745836257935\n",
      "Epoch 4872: train loss 1.9509745836257935 val loss 1.9430458545684814\n",
      "Epoch 4873: train loss 1.9430458545684814 val loss 1.9404150247573853\n",
      "Epoch 4874: train loss 1.9404150247573853 val loss 1.9342982769012451\n",
      "Epoch 4875: train loss 1.9342982769012451 val loss 1.9313607215881348\n",
      "Epoch 4876: train loss 1.9313607215881348 val loss 1.9278972148895264\n",
      "Epoch 4877: train loss 1.9278972148895264 val loss 1.924272060394287\n",
      "Epoch 4878: train loss 1.924272060394287 val loss 1.9237885475158691\n",
      "Epoch 4879: train loss 1.9237885475158691 val loss 1.9303150177001953\n",
      "Epoch 4880: train loss 1.9303150177001953 val loss 1.9138487577438354\n",
      "Epoch 4881: train loss 1.9138487577438354 val loss 1.9265446662902832\n",
      "Epoch 4882: train loss 1.9265446662902832 val loss 1.9153375625610352\n",
      "Epoch 4883: train loss 1.9153375625610352 val loss 1.9061484336853027\n",
      "Epoch 4884: train loss 1.9061484336853027 val loss 1.9146994352340698\n",
      "Epoch 4885: train loss 1.9146994352340698 val loss 1.8968653678894043\n",
      "Epoch 4886: train loss 1.8968653678894043 val loss 1.9202483892440796\n",
      "Epoch 4887: train loss 1.9202483892440796 val loss 1.892594814300537\n",
      "Epoch 4888: train loss 1.892594814300537 val loss 1.8872017860412598\n",
      "Epoch 4889: train loss 1.8872017860412598 val loss 1.8979395627975464\n",
      "Epoch 4890: train loss 1.8979395627975464 val loss 1.8789288997650146\n",
      "Epoch 4891: train loss 1.8789288997650146 val loss 1.8939226865768433\n",
      "Epoch 4892: train loss 1.8939226865768433 val loss 1.8835186958312988\n",
      "Epoch 4893: train loss 1.8835186958312988 val loss 1.8711142539978027\n",
      "Epoch 4894: train loss 1.8711142539978027 val loss 1.893649935722351\n",
      "Epoch 4895: train loss 1.893649935722351 val loss 1.8605693578720093\n",
      "Epoch 4896: train loss 1.8605693578720093 val loss 1.8680952787399292\n",
      "Epoch 4897: train loss 1.8680952787399292 val loss 1.8614387512207031\n",
      "Epoch 4898: train loss 1.8614387512207031 val loss 1.8509669303894043\n",
      "Epoch 4899: train loss 1.8509669303894043 val loss 1.8731684684753418\n",
      "Epoch 4900: train loss 1.8731684684753418 val loss 1.8492404222488403\n",
      "Epoch 4901: train loss 1.8492404222488403 val loss 1.8462469577789307\n",
      "Epoch 4902: train loss 1.8462469577789307 val loss 1.8526808023452759\n",
      "Epoch 4903: train loss 1.8526808023452759 val loss 1.8315500020980835\n",
      "Epoch 4904: train loss 1.8315500020980835 val loss 1.8276631832122803\n",
      "Epoch 4905: train loss 1.8276631832122803 val loss 1.8382196426391602\n",
      "Epoch 4906: train loss 1.8382196426391602 val loss 1.8316645622253418\n",
      "Epoch 4907: train loss 1.8316645622253418 val loss 1.8155733346939087\n",
      "Epoch 4908: train loss 1.8155733346939087 val loss 1.8201673030853271\n",
      "Epoch 4909: train loss 1.8201673030853271 val loss 1.8184987306594849\n",
      "Epoch 4910: train loss 1.8184987306594849 val loss 1.8078432083129883\n",
      "Epoch 4911: train loss 1.8078432083129883 val loss 1.8022873401641846\n",
      "Epoch 4912: train loss 1.8022873401641846 val loss 1.7972900867462158\n",
      "Epoch 4913: train loss 1.7972900867462158 val loss 1.7959715127944946\n",
      "Epoch 4914: train loss 1.7959715127944946 val loss 1.7955195903778076\n",
      "Epoch 4915: train loss 1.7955195903778076 val loss 1.786998987197876\n",
      "Epoch 4916: train loss 1.786998987197876 val loss 1.7833209037780762\n",
      "Epoch 4917: train loss 1.7833209037780762 val loss 1.7813886404037476\n",
      "Epoch 4918: train loss 1.7813886404037476 val loss 1.7810393571853638\n",
      "Epoch 4919: train loss 1.7810393571853638 val loss 1.7720706462860107\n",
      "Epoch 4920: train loss 1.7720706462860107 val loss 1.7680234909057617\n",
      "Epoch 4921: train loss 1.7680234909057617 val loss 1.769105076789856\n",
      "Epoch 4922: train loss 1.769105076789856 val loss 1.7648186683654785\n",
      "Epoch 4923: train loss 1.7648186683654785 val loss 1.7686914205551147\n",
      "Epoch 4924: train loss 1.7686914205551147 val loss 1.754883885383606\n",
      "Epoch 4925: train loss 1.754883885383606 val loss 1.7703008651733398\n",
      "Epoch 4926: train loss 1.7703008651733398 val loss 1.7641932964324951\n",
      "Epoch 4927: train loss 1.7641932964324951 val loss 1.7422599792480469\n",
      "Epoch 4928: train loss 1.7422599792480469 val loss 1.7757148742675781\n",
      "Epoch 4929: train loss 1.7757148742675781 val loss 1.794668197631836\n",
      "Epoch 4930: train loss 1.794668197631836 val loss 1.7378013134002686\n",
      "Epoch 4931: train loss 1.7378013134002686 val loss 1.7479244470596313\n",
      "Epoch 4932: train loss 1.7479244470596313 val loss 1.7863409519195557\n",
      "Epoch 4933: train loss 1.7863409519195557 val loss 1.7302932739257812\n",
      "Epoch 4934: train loss 1.7302932739257812 val loss 1.7495454549789429\n",
      "Epoch 4935: train loss 1.7495454549789429 val loss 1.7634425163269043\n",
      "Epoch 4936: train loss 1.7634425163269043 val loss 1.709786295890808\n",
      "Epoch 4937: train loss 1.709786295890808 val loss 1.728672742843628\n",
      "Epoch 4938: train loss 1.728672742843628 val loss 1.7366920709609985\n",
      "Epoch 4939: train loss 1.7366920709609985 val loss 1.7105010747909546\n",
      "Epoch 4940: train loss 1.7105010747909546 val loss 1.6932384967803955\n",
      "Epoch 4941: train loss 1.6932384967803955 val loss 1.7134387493133545\n",
      "Epoch 4942: train loss 1.7134387493133545 val loss 1.738114356994629\n",
      "Epoch 4943: train loss 1.738114356994629 val loss 1.690463900566101\n",
      "Epoch 4944: train loss 1.690463900566101 val loss 1.7134337425231934\n",
      "Epoch 4945: train loss 1.7134337425231934 val loss 1.760007619857788\n",
      "Epoch 4946: train loss 1.760007619857788 val loss 1.7034283876419067\n",
      "Epoch 4947: train loss 1.7034283876419067 val loss 1.6696839332580566\n",
      "Epoch 4948: train loss 1.6696839332580566 val loss 1.7116825580596924\n",
      "Epoch 4949: train loss 1.7116825580596924 val loss 1.6721234321594238\n",
      "Epoch 4950: train loss 1.6721234321594238 val loss 1.663056492805481\n",
      "Epoch 4951: train loss 1.663056492805481 val loss 1.7137327194213867\n",
      "Epoch 4952: train loss 1.7137327194213867 val loss 1.6713998317718506\n",
      "Epoch 4953: train loss 1.6713998317718506 val loss 1.6424230337142944\n",
      "Epoch 4954: train loss 1.6424230337142944 val loss 1.687645435333252\n",
      "Epoch 4955: train loss 1.687645435333252 val loss 1.6827514171600342\n",
      "Epoch 4956: train loss 1.6827514171600342 val loss 1.6332738399505615\n",
      "Epoch 4957: train loss 1.6332738399505615 val loss 1.6475003957748413\n",
      "Epoch 4958: train loss 1.6475003957748413 val loss 1.6914719343185425\n",
      "Epoch 4959: train loss 1.6914719343185425 val loss 1.6233795881271362\n",
      "Epoch 4960: train loss 1.6233795881271362 val loss 1.6454088687896729\n",
      "Epoch 4961: train loss 1.6454088687896729 val loss 1.669884443283081\n",
      "Epoch 4962: train loss 1.669884443283081 val loss 1.6134439706802368\n",
      "Epoch 4963: train loss 1.6134439706802368 val loss 1.6046987771987915\n",
      "Epoch 4964: train loss 1.6046987771987915 val loss 1.618086576461792\n",
      "Epoch 4965: train loss 1.618086576461792 val loss 1.6151913404464722\n",
      "Epoch 4966: train loss 1.6151913404464722 val loss 1.609010934829712\n",
      "Epoch 4967: train loss 1.609010934829712 val loss 1.5891063213348389\n",
      "Epoch 4968: train loss 1.5891063213348389 val loss 1.5832481384277344\n",
      "Epoch 4969: train loss 1.5832481384277344 val loss 1.6051664352416992\n",
      "Epoch 4970: train loss 1.6051664352416992 val loss 1.6414211988449097\n",
      "Epoch 4971: train loss 1.6414211988449097 val loss 1.603568196296692\n",
      "Epoch 4972: train loss 1.603568196296692 val loss 1.5671015977859497\n",
      "Epoch 4973: train loss 1.5671015977859497 val loss 1.5653939247131348\n",
      "Epoch 4974: train loss 1.5653939247131348 val loss 1.623563528060913\n",
      "Epoch 4975: train loss 1.623563528060913 val loss 1.5926380157470703\n",
      "Epoch 4976: train loss 1.5926380157470703 val loss 1.5680785179138184\n",
      "Epoch 4977: train loss 1.5680785179138184 val loss 1.5488202571868896\n",
      "Epoch 4978: train loss 1.5488202571868896 val loss 1.5436878204345703\n",
      "Epoch 4979: train loss 1.5436878204345703 val loss 1.5421490669250488\n",
      "Epoch 4980: train loss 1.5421490669250488 val loss 1.5881876945495605\n",
      "Epoch 4981: train loss 1.5881876945495605 val loss 1.5672708749771118\n",
      "Epoch 4982: train loss 1.5672708749771118 val loss 1.5577690601348877\n",
      "Epoch 4983: train loss 1.5577690601348877 val loss 1.5511908531188965\n",
      "Epoch 4984: train loss 1.5511908531188965 val loss 1.577350378036499\n",
      "Epoch 4985: train loss 1.577350378036499 val loss 1.5680179595947266\n",
      "Epoch 4986: train loss 1.5680179595947266 val loss 1.5270825624465942\n",
      "Epoch 4987: train loss 1.5270825624465942 val loss 1.507182240486145\n",
      "Epoch 4988: train loss 1.507182240486145 val loss 1.5276097059249878\n",
      "Epoch 4989: train loss 1.5276097059249878 val loss 1.591054916381836\n",
      "Epoch 4990: train loss 1.591054916381836 val loss 1.5873501300811768\n",
      "Epoch 4991: train loss 1.5873501300811768 val loss 1.508049488067627\n",
      "Epoch 4992: train loss 1.508049488067627 val loss 1.496412992477417\n",
      "Epoch 4993: train loss 1.496412992477417 val loss 1.5836024284362793\n",
      "Epoch 4994: train loss 1.5836024284362793 val loss 1.578879475593567\n",
      "Epoch 4995: train loss 1.578879475593567 val loss 1.505704641342163\n",
      "Epoch 4996: train loss 1.505704641342163 val loss 1.4723858833312988\n",
      "Epoch 4997: train loss 1.4723858833312988 val loss 1.4690914154052734\n",
      "Epoch 4998: train loss 1.4690914154052734 val loss 1.465475082397461\n",
      "Epoch 4999: train loss 1.465475082397461 val loss 1.4811971187591553\n",
      "Epoch 5000: train loss 1.4811971187591553 val loss 1.5150032043457031\n",
      "Epoch 5001: train loss 1.5150032043457031 val loss 1.5642642974853516\n",
      "Epoch 5002: train loss 1.5642642974853516 val loss 1.524728775024414\n",
      "Epoch 5003: train loss 1.524728775024414 val loss 1.4606703519821167\n",
      "Epoch 5004: train loss 1.4606703519821167 val loss 1.4539697170257568\n",
      "Epoch 5005: train loss 1.4539697170257568 val loss 1.5490151643753052\n",
      "Epoch 5006: train loss 1.5490151643753052 val loss 1.5807058811187744\n",
      "Epoch 5007: train loss 1.5807058811187744 val loss 1.4576222896575928\n",
      "Epoch 5008: train loss 1.4576222896575928 val loss 1.447972059249878\n",
      "Epoch 5009: train loss 1.447972059249878 val loss 1.5740859508514404\n",
      "Epoch 5010: train loss 1.5740859508514404 val loss 1.5225944519042969\n",
      "Epoch 5011: train loss 1.5225944519042969 val loss 1.4523319005966187\n",
      "Epoch 5012: train loss 1.4523319005966187 val loss 1.4483373165130615\n",
      "Epoch 5013: train loss 1.4483373165130615 val loss 1.5262306928634644\n",
      "Epoch 5014: train loss 1.5262306928634644 val loss 1.471672534942627\n",
      "Epoch 5015: train loss 1.471672534942627 val loss 1.4782094955444336\n",
      "Epoch 5016: train loss 1.4782094955444336 val loss 1.4641101360321045\n",
      "Epoch 5017: train loss 1.4641101360321045 val loss 1.4741101264953613\n",
      "Epoch 5018: train loss 1.4741101264953613 val loss 1.4586951732635498\n",
      "Epoch 5019: train loss 1.4586951732635498 val loss 1.4747265577316284\n",
      "Epoch 5020: train loss 1.4747265577316284 val loss 1.4727625846862793\n",
      "Epoch 5021: train loss 1.4727625846862793 val loss 1.4654333591461182\n",
      "Epoch 5022: train loss 1.4654333591461182 val loss 1.4246954917907715\n",
      "Epoch 5023: train loss 1.4246954917907715 val loss 1.4324917793273926\n",
      "Epoch 5024: train loss 1.4324917793273926 val loss 1.4970204830169678\n",
      "Epoch 5025: train loss 1.4970204830169678 val loss 1.5198054313659668\n",
      "Epoch 5026: train loss 1.5198054313659668 val loss 1.3885287046432495\n",
      "Epoch 5027: train loss 1.3885287046432495 val loss 1.4217865467071533\n",
      "Epoch 5028: train loss 1.4217865467071533 val loss 1.5782628059387207\n",
      "Epoch 5029: train loss 1.5782628059387207 val loss 1.388085961341858\n",
      "Epoch 5030: train loss 1.388085961341858 val loss 1.4179060459136963\n",
      "Epoch 5031: train loss 1.4179060459136963 val loss 1.5844593048095703\n",
      "Epoch 5032: train loss 1.5844593048095703 val loss 1.378424882888794\n",
      "Epoch 5033: train loss 1.378424882888794 val loss 1.624875783920288\n",
      "Epoch 5034: train loss 1.624875783920288 val loss 1.4430055618286133\n",
      "Epoch 5035: train loss 1.4430055618286133 val loss 1.4088046550750732\n",
      "Epoch 5036: train loss 1.4088046550750732 val loss 1.6721837520599365\n",
      "Epoch 5037: train loss 1.6721837520599365 val loss 1.3665286302566528\n",
      "Epoch 5038: train loss 1.3665286302566528 val loss 1.5448582172393799\n",
      "Epoch 5039: train loss 1.5448582172393799 val loss 1.458272099494934\n",
      "Epoch 5040: train loss 1.458272099494934 val loss 1.3608028888702393\n",
      "Epoch 5041: train loss 1.3608028888702393 val loss 1.3982371091842651\n",
      "Epoch 5042: train loss 1.3982371091842651 val loss 1.5197999477386475\n",
      "Epoch 5043: train loss 1.5197999477386475 val loss 1.3893156051635742\n",
      "Epoch 5044: train loss 1.3893156051635742 val loss 1.350269079208374\n",
      "Epoch 5045: train loss 1.350269079208374 val loss 1.37047278881073\n",
      "Epoch 5046: train loss 1.37047278881073 val loss 1.5027732849121094\n",
      "Epoch 5047: train loss 1.5027732849121094 val loss 1.383138656616211\n",
      "Epoch 5048: train loss 1.383138656616211 val loss 1.3427722454071045\n",
      "Epoch 5049: train loss 1.3427722454071045 val loss 1.4455127716064453\n",
      "Epoch 5050: train loss 1.4455127716064453 val loss 1.4479209184646606\n",
      "Epoch 5051: train loss 1.4479209184646606 val loss 1.3604671955108643\n",
      "Epoch 5052: train loss 1.3604671955108643 val loss 1.3666205406188965\n",
      "Epoch 5053: train loss 1.3666205406188965 val loss 1.3738763332366943\n",
      "Epoch 5054: train loss 1.3738763332366943 val loss 1.4288197755813599\n",
      "Epoch 5055: train loss 1.4288197755813599 val loss 1.4201061725616455\n",
      "Epoch 5056: train loss 1.4201061725616455 val loss 1.335510492324829\n",
      "Epoch 5057: train loss 1.335510492324829 val loss 1.3391681909561157\n",
      "Epoch 5058: train loss 1.3391681909561157 val loss 1.3681046962738037\n",
      "Epoch 5059: train loss 1.3681046962738037 val loss 1.4001801013946533\n",
      "Epoch 5060: train loss 1.4001801013946533 val loss 1.4373574256896973\n",
      "Epoch 5061: train loss 1.4373574256896973 val loss 1.3314602375030518\n",
      "Epoch 5062: train loss 1.3314602375030518 val loss 1.3528715372085571\n",
      "Epoch 5063: train loss 1.3528715372085571 val loss 1.482219934463501\n",
      "Epoch 5064: train loss 1.482219934463501 val loss 1.337700366973877\n",
      "Epoch 5065: train loss 1.337700366973877 val loss 1.3274304866790771\n",
      "Epoch 5066: train loss 1.3274304866790771 val loss 1.3892006874084473\n",
      "Epoch 5067: train loss 1.3892006874084473 val loss 1.3691744804382324\n",
      "Epoch 5068: train loss 1.3691744804382324 val loss 1.4111707210540771\n",
      "Epoch 5069: train loss 1.4111707210540771 val loss 1.3319485187530518\n",
      "Epoch 5070: train loss 1.3319485187530518 val loss 1.323323369026184\n",
      "Epoch 5071: train loss 1.323323369026184 val loss 1.3137757778167725\n",
      "Epoch 5072: train loss 1.3137757778167725 val loss 1.3094512224197388\n",
      "Epoch 5073: train loss 1.3094512224197388 val loss 1.3122717142105103\n",
      "Epoch 5074: train loss 1.3122717142105103 val loss 1.308176040649414\n",
      "Epoch 5075: train loss 1.308176040649414 val loss 1.3057992458343506\n",
      "Epoch 5076: train loss 1.3057992458343506 val loss 1.3033075332641602\n",
      "Epoch 5077: train loss 1.3033075332641602 val loss 1.3032944202423096\n",
      "Epoch 5078: train loss 1.3032944202423096 val loss 1.3212629556655884\n",
      "Epoch 5079: train loss 1.3212629556655884 val loss 1.4448466300964355\n",
      "Epoch 5080: train loss 1.4448466300964355 val loss 1.342268705368042\n",
      "Epoch 5081: train loss 1.342268705368042 val loss 1.3187785148620605\n",
      "Epoch 5082: train loss 1.3187785148620605 val loss 1.3994109630584717\n",
      "Epoch 5083: train loss 1.3994109630584717 val loss 1.3971028327941895\n",
      "Epoch 5084: train loss 1.3971028327941895 val loss 1.2956490516662598\n",
      "Epoch 5085: train loss 1.2956490516662598 val loss 1.297084093093872\n",
      "Epoch 5086: train loss 1.297084093093872 val loss 1.3445656299591064\n",
      "Epoch 5087: train loss 1.3445656299591064 val loss 1.4196834564208984\n",
      "Epoch 5088: train loss 1.4196834564208984 val loss 1.2979443073272705\n",
      "Epoch 5089: train loss 1.2979443073272705 val loss 1.3079211711883545\n",
      "Epoch 5090: train loss 1.3079211711883545 val loss 1.3427274227142334\n",
      "Epoch 5091: train loss 1.3427274227142334 val loss 1.325878620147705\n",
      "Epoch 5092: train loss 1.325878620147705 val loss 1.3350951671600342\n",
      "Epoch 5093: train loss 1.3350951671600342 val loss 1.3430615663528442\n",
      "Epoch 5094: train loss 1.3430615663528442 val loss 1.3646066188812256\n",
      "Epoch 5095: train loss 1.3646066188812256 val loss 1.3047783374786377\n",
      "Epoch 5096: train loss 1.3047783374786377 val loss 1.3015475273132324\n",
      "Epoch 5097: train loss 1.3015475273132324 val loss 1.3498899936676025\n",
      "Epoch 5098: train loss 1.3498899936676025 val loss 1.3496971130371094\n",
      "Epoch 5099: train loss 1.3496971130371094 val loss 1.3159277439117432\n",
      "Epoch 5100: train loss 1.3159277439117432 val loss 1.3104453086853027\n",
      "Epoch 5101: train loss 1.3104453086853027 val loss 1.2782557010650635\n",
      "Epoch 5102: train loss 1.2782557010650635 val loss 1.2794671058654785\n",
      "Epoch 5103: train loss 1.2794671058654785 val loss 1.2721868753433228\n",
      "Epoch 5104: train loss 1.2721868753433228 val loss 1.290108561515808\n",
      "Epoch 5105: train loss 1.290108561515808 val loss 1.4308524131774902\n",
      "Epoch 5106: train loss 1.4308524131774902 val loss 1.3489916324615479\n",
      "Epoch 5107: train loss 1.3489916324615479 val loss 1.2661008834838867\n",
      "Epoch 5108: train loss 1.2661008834838867 val loss 1.334558367729187\n",
      "Epoch 5109: train loss 1.334558367729187 val loss 1.3597567081451416\n",
      "Epoch 5110: train loss 1.3597567081451416 val loss 1.3013578653335571\n",
      "Epoch 5111: train loss 1.3013578653335571 val loss 1.2621620893478394\n",
      "Epoch 5112: train loss 1.2621620893478394 val loss 1.2812033891677856\n",
      "Epoch 5113: train loss 1.2812033891677856 val loss 1.3785731792449951\n",
      "Epoch 5114: train loss 1.3785731792449951 val loss 1.3555562496185303\n",
      "Epoch 5115: train loss 1.3555562496185303 val loss 1.2607367038726807\n",
      "Epoch 5116: train loss 1.2607367038726807 val loss 1.2697323560714722\n",
      "Epoch 5117: train loss 1.2697323560714722 val loss 1.3187687397003174\n",
      "Epoch 5118: train loss 1.3187687397003174 val loss 1.3505982160568237\n",
      "Epoch 5119: train loss 1.3505982160568237 val loss 1.3193302154541016\n",
      "Epoch 5120: train loss 1.3193302154541016 val loss 1.2676855325698853\n",
      "Epoch 5121: train loss 1.2676855325698853 val loss 1.2553298473358154\n",
      "Epoch 5122: train loss 1.2553298473358154 val loss 1.261032223701477\n",
      "Epoch 5123: train loss 1.261032223701477 val loss 1.271578311920166\n",
      "Epoch 5124: train loss 1.271578311920166 val loss 1.2640631198883057\n",
      "Epoch 5125: train loss 1.2640631198883057 val loss 1.284224271774292\n",
      "Epoch 5126: train loss 1.284224271774292 val loss 1.2504098415374756\n",
      "Epoch 5127: train loss 1.2504098415374756 val loss 1.2575881481170654\n",
      "Epoch 5128: train loss 1.2575881481170654 val loss 1.2742938995361328\n",
      "Epoch 5129: train loss 1.2742938995361328 val loss 1.2491087913513184\n",
      "Epoch 5130: train loss 1.2491087913513184 val loss 1.2505345344543457\n",
      "Epoch 5131: train loss 1.2505345344543457 val loss 1.277920126914978\n",
      "Epoch 5132: train loss 1.277920126914978 val loss 1.2470581531524658\n",
      "Epoch 5133: train loss 1.2470581531524658 val loss 1.265052318572998\n",
      "Epoch 5134: train loss 1.265052318572998 val loss 1.402994155883789\n",
      "Epoch 5135: train loss 1.402994155883789 val loss 1.2979567050933838\n",
      "Epoch 5136: train loss 1.2979567050933838 val loss 1.260708212852478\n",
      "Epoch 5137: train loss 1.260708212852478 val loss 1.2454347610473633\n",
      "Epoch 5138: train loss 1.2454347610473633 val loss 1.3155497312545776\n",
      "Epoch 5139: train loss 1.3155497312545776 val loss 1.3851499557495117\n",
      "Epoch 5140: train loss 1.3851499557495117 val loss 1.2626571655273438\n",
      "Epoch 5141: train loss 1.2626571655273438 val loss 1.2409389019012451\n",
      "Epoch 5142: train loss 1.2409389019012451 val loss 1.2842174768447876\n",
      "Epoch 5143: train loss 1.2842174768447876 val loss 1.333509087562561\n",
      "Epoch 5144: train loss 1.333509087562561 val loss 1.2646312713623047\n",
      "Epoch 5145: train loss 1.2646312713623047 val loss 1.2586512565612793\n",
      "Epoch 5146: train loss 1.2586512565612793 val loss 1.3340122699737549\n",
      "Epoch 5147: train loss 1.3340122699737549 val loss 1.2795183658599854\n",
      "Epoch 5148: train loss 1.2795183658599854 val loss 1.2382968664169312\n",
      "Epoch 5149: train loss 1.2382968664169312 val loss 1.2494940757751465\n",
      "Epoch 5150: train loss 1.2494940757751465 val loss 1.3065029382705688\n",
      "Epoch 5151: train loss 1.3065029382705688 val loss 1.3217017650604248\n",
      "Epoch 5152: train loss 1.3217017650604248 val loss 1.2360951900482178\n",
      "Epoch 5153: train loss 1.2360951900482178 val loss 1.322404384613037\n",
      "Epoch 5154: train loss 1.322404384613037 val loss 1.4404385089874268\n",
      "Epoch 5155: train loss 1.4404385089874268 val loss 1.2465269565582275\n",
      "Epoch 5156: train loss 1.2465269565582275 val loss 1.5707683563232422\n",
      "Epoch 5157: train loss 1.5707683563232422 val loss 1.259721279144287\n",
      "Epoch 5158: train loss 1.259721279144287 val loss 1.6046921014785767\n",
      "Epoch 5159: train loss 1.6046921014785767 val loss 1.3025082349777222\n",
      "Epoch 5160: train loss 1.3025082349777222 val loss 1.8283535242080688\n",
      "Epoch 5161: train loss 1.8283535242080688 val loss 1.7364152669906616\n",
      "Epoch 5162: train loss 1.7364152669906616 val loss 1.403780460357666\n",
      "Epoch 5163: train loss 1.403780460357666 val loss 1.5497392416000366\n",
      "Epoch 5164: train loss 1.5497392416000366 val loss 1.3274930715560913\n",
      "Epoch 5165: train loss 1.3274930715560913 val loss 1.2722187042236328\n",
      "Epoch 5166: train loss 1.2722187042236328 val loss 1.4930098056793213\n",
      "Epoch 5167: train loss 1.4930098056793213 val loss 1.325408697128296\n",
      "Epoch 5168: train loss 1.325408697128296 val loss 1.618208885192871\n",
      "Epoch 5169: train loss 1.618208885192871 val loss 1.4400970935821533\n",
      "Epoch 5170: train loss 1.4400970935821533 val loss 1.6227226257324219\n",
      "Epoch 5171: train loss 1.6227226257324219 val loss 1.6605546474456787\n",
      "Epoch 5172: train loss 1.6605546474456787 val loss 1.2852696180343628\n",
      "Epoch 5173: train loss 1.2852696180343628 val loss 1.5085381269454956\n",
      "Epoch 5174: train loss 1.5085381269454956 val loss 1.2758053541183472\n",
      "Epoch 5175: train loss 1.2758053541183472 val loss 1.3478518724441528\n",
      "Epoch 5176: train loss 1.3478518724441528 val loss 1.279119849205017\n",
      "Epoch 5177: train loss 1.279119849205017 val loss 1.2615306377410889\n",
      "Epoch 5178: train loss 1.2615306377410889 val loss 1.3182806968688965\n",
      "Epoch 5179: train loss 1.3182806968688965 val loss 1.2318966388702393\n",
      "Epoch 5180: train loss 1.2318966388702393 val loss 1.3248904943466187\n",
      "Epoch 5181: train loss 1.3248904943466187 val loss 1.2328221797943115\n",
      "Epoch 5182: train loss 1.2328221797943115 val loss 1.3369908332824707\n",
      "Epoch 5183: train loss 1.3369908332824707 val loss 1.287351131439209\n",
      "Epoch 5184: train loss 1.287351131439209 val loss 1.2396364212036133\n",
      "Epoch 5185: train loss 1.2396364212036133 val loss 1.3648505210876465\n",
      "Epoch 5186: train loss 1.3648505210876465 val loss 1.2219053506851196\n",
      "Epoch 5187: train loss 1.2219053506851196 val loss 1.2940731048583984\n",
      "Epoch 5188: train loss 1.2940731048583984 val loss 1.2414467334747314\n",
      "Epoch 5189: train loss 1.2414467334747314 val loss 1.2188665866851807\n",
      "Epoch 5190: train loss 1.2188665866851807 val loss 1.2524385452270508\n",
      "Epoch 5191: train loss 1.2524385452270508 val loss 1.2227699756622314\n",
      "Epoch 5192: train loss 1.2227699756622314 val loss 1.2339781522750854\n",
      "Epoch 5193: train loss 1.2339781522750854 val loss 1.2570394277572632\n",
      "Epoch 5194: train loss 1.2570394277572632 val loss 1.223488450050354\n",
      "Epoch 5195: train loss 1.223488450050354 val loss 1.267293930053711\n",
      "Epoch 5196: train loss 1.267293930053711 val loss 1.2451521158218384\n",
      "Epoch 5197: train loss 1.2451521158218384 val loss 1.238262414932251\n",
      "Epoch 5198: train loss 1.238262414932251 val loss 1.2766549587249756\n",
      "Epoch 5199: train loss 1.2766549587249756 val loss 1.2203948497772217\n",
      "Epoch 5200: train loss 1.2203948497772217 val loss 1.2720947265625\n",
      "Epoch 5201: train loss 1.2720947265625 val loss 1.2918715476989746\n",
      "Epoch 5202: train loss 1.2918715476989746 val loss 1.2163538932800293\n",
      "Epoch 5203: train loss 1.2163538932800293 val loss 1.2487868070602417\n",
      "Epoch 5204: train loss 1.2487868070602417 val loss 1.2140259742736816\n",
      "Epoch 5205: train loss 1.2140259742736816 val loss 1.253584623336792\n",
      "Epoch 5206: train loss 1.253584623336792 val loss 1.2970266342163086\n",
      "Epoch 5207: train loss 1.2970266342163086 val loss 1.2105145454406738\n",
      "Epoch 5208: train loss 1.2105145454406738 val loss 1.272530436515808\n",
      "Epoch 5209: train loss 1.272530436515808 val loss 1.2772918939590454\n",
      "Epoch 5210: train loss 1.2772918939590454 val loss 1.20908522605896\n",
      "Epoch 5211: train loss 1.20908522605896 val loss 1.2648619413375854\n",
      "Epoch 5212: train loss 1.2648619413375854 val loss 1.2614860534667969\n",
      "Epoch 5213: train loss 1.2614860534667969 val loss 1.2110674381256104\n",
      "Epoch 5214: train loss 1.2110674381256104 val loss 1.3108179569244385\n",
      "Epoch 5215: train loss 1.3108179569244385 val loss 1.2199594974517822\n",
      "Epoch 5216: train loss 1.2199594974517822 val loss 1.259183406829834\n",
      "Epoch 5217: train loss 1.259183406829834 val loss 1.2902696132659912\n",
      "Epoch 5218: train loss 1.2902696132659912 val loss 1.2084205150604248\n",
      "Epoch 5219: train loss 1.2084205150604248 val loss 1.2863707542419434\n",
      "Epoch 5220: train loss 1.2863707542419434 val loss 1.222329020500183\n",
      "Epoch 5221: train loss 1.222329020500183 val loss 1.2345554828643799\n",
      "Epoch 5222: train loss 1.2345554828643799 val loss 1.258507251739502\n",
      "Epoch 5223: train loss 1.258507251739502 val loss 1.2102694511413574\n",
      "Epoch 5224: train loss 1.2102694511413574 val loss 1.256883144378662\n",
      "Epoch 5225: train loss 1.256883144378662 val loss 1.2857105731964111\n",
      "Epoch 5226: train loss 1.2857105731964111 val loss 1.2050949335098267\n",
      "Epoch 5227: train loss 1.2050949335098267 val loss 1.2681398391723633\n",
      "Epoch 5228: train loss 1.2681398391723633 val loss 1.2316827774047852\n",
      "Epoch 5229: train loss 1.2316827774047852 val loss 1.2242064476013184\n",
      "Epoch 5230: train loss 1.2242064476013184 val loss 1.2580983638763428\n",
      "Epoch 5231: train loss 1.2580983638763428 val loss 1.2085938453674316\n",
      "Epoch 5232: train loss 1.2085938453674316 val loss 1.2180135250091553\n",
      "Epoch 5233: train loss 1.2180135250091553 val loss 1.2176060676574707\n",
      "Epoch 5234: train loss 1.2176060676574707 val loss 1.2012155055999756\n",
      "Epoch 5235: train loss 1.2012155055999756 val loss 1.2139720916748047\n",
      "Epoch 5236: train loss 1.2139720916748047 val loss 1.2003939151763916\n",
      "Epoch 5237: train loss 1.2003939151763916 val loss 1.2154321670532227\n",
      "Epoch 5238: train loss 1.2154321670532227 val loss 1.2103972434997559\n",
      "Epoch 5239: train loss 1.2103972434997559 val loss 1.207641839981079\n",
      "Epoch 5240: train loss 1.207641839981079 val loss 1.2185695171356201\n",
      "Epoch 5241: train loss 1.2185695171356201 val loss 1.1979787349700928\n",
      "Epoch 5242: train loss 1.1979787349700928 val loss 1.2158761024475098\n",
      "Epoch 5243: train loss 1.2158761024475098 val loss 1.208221673965454\n",
      "Epoch 5244: train loss 1.208221673965454 val loss 1.2098172903060913\n",
      "Epoch 5245: train loss 1.2098172903060913 val loss 1.2035796642303467\n",
      "Epoch 5246: train loss 1.2035796642303467 val loss 1.197092056274414\n",
      "Epoch 5247: train loss 1.197092056274414 val loss 1.2017260789871216\n",
      "Epoch 5248: train loss 1.2017260789871216 val loss 1.1958070993423462\n",
      "Epoch 5249: train loss 1.1958070993423462 val loss 1.1979776620864868\n",
      "Epoch 5250: train loss 1.1979776620864868 val loss 1.2046208381652832\n",
      "Epoch 5251: train loss 1.2046208381652832 val loss 1.1945786476135254\n",
      "Epoch 5252: train loss 1.1945786476135254 val loss 1.2112948894500732\n",
      "Epoch 5253: train loss 1.2112948894500732 val loss 1.1935980319976807\n",
      "Epoch 5254: train loss 1.1935980319976807 val loss 1.215541124343872\n",
      "Epoch 5255: train loss 1.215541124343872 val loss 1.2333166599273682\n",
      "Epoch 5256: train loss 1.2333166599273682 val loss 1.193778395652771\n",
      "Epoch 5257: train loss 1.193778395652771 val loss 1.2265174388885498\n",
      "Epoch 5258: train loss 1.2265174388885498 val loss 1.2152085304260254\n",
      "Epoch 5259: train loss 1.2152085304260254 val loss 1.2105149030685425\n",
      "Epoch 5260: train loss 1.2105149030685425 val loss 1.2243103981018066\n",
      "Epoch 5261: train loss 1.2243103981018066 val loss 1.1942781209945679\n",
      "Epoch 5262: train loss 1.1942781209945679 val loss 1.2208693027496338\n",
      "Epoch 5263: train loss 1.2208693027496338 val loss 1.2052221298217773\n",
      "Epoch 5264: train loss 1.2052221298217773 val loss 1.1909695863723755\n",
      "Epoch 5265: train loss 1.1909695863723755 val loss 1.2203079462051392\n",
      "Epoch 5266: train loss 1.2203079462051392 val loss 1.1899800300598145\n",
      "Epoch 5267: train loss 1.1899800300598145 val loss 1.2419664859771729\n",
      "Epoch 5268: train loss 1.2419664859771729 val loss 1.239924430847168\n",
      "Epoch 5269: train loss 1.239924430847168 val loss 1.1915751695632935\n",
      "Epoch 5270: train loss 1.1915751695632935 val loss 1.2249486446380615\n",
      "Epoch 5271: train loss 1.2249486446380615 val loss 1.2022722959518433\n",
      "Epoch 5272: train loss 1.2022722959518433 val loss 1.215051531791687\n",
      "Epoch 5273: train loss 1.215051531791687 val loss 1.2160166501998901\n",
      "Epoch 5274: train loss 1.2160166501998901 val loss 1.1885757446289062\n",
      "Epoch 5275: train loss 1.1885757446289062 val loss 1.2089917659759521\n",
      "Epoch 5276: train loss 1.2089917659759521 val loss 1.1886320114135742\n",
      "Epoch 5277: train loss 1.1886320114135742 val loss 1.18772554397583\n",
      "Epoch 5278: train loss 1.18772554397583 val loss 1.2052773237228394\n",
      "Epoch 5279: train loss 1.2052773237228394 val loss 1.1881940364837646\n",
      "Epoch 5280: train loss 1.1881940364837646 val loss 1.1986883878707886\n",
      "Epoch 5281: train loss 1.1986883878707886 val loss 1.1901931762695312\n",
      "Epoch 5282: train loss 1.1901931762695312 val loss 1.187544584274292\n",
      "Epoch 5283: train loss 1.187544584274292 val loss 1.1845383644104004\n",
      "Epoch 5284: train loss 1.1845383644104004 val loss 1.1940245628356934\n",
      "Epoch 5285: train loss 1.1940245628356934 val loss 1.1833752393722534\n",
      "Epoch 5286: train loss 1.1833752393722534 val loss 1.1884746551513672\n",
      "Epoch 5287: train loss 1.1884746551513672 val loss 1.1895594596862793\n",
      "Epoch 5288: train loss 1.1895594596862793 val loss 1.1825125217437744\n",
      "Epoch 5289: train loss 1.1825125217437744 val loss 1.190841794013977\n",
      "Epoch 5290: train loss 1.190841794013977 val loss 1.182450294494629\n",
      "Epoch 5291: train loss 1.182450294494629 val loss 1.1830484867095947\n",
      "Epoch 5292: train loss 1.1830484867095947 val loss 1.1821763515472412\n",
      "Epoch 5293: train loss 1.1821763515472412 val loss 1.1831257343292236\n",
      "Epoch 5294: train loss 1.1831257343292236 val loss 1.1802611351013184\n",
      "Epoch 5295: train loss 1.1802611351013184 val loss 1.187775731086731\n",
      "Epoch 5296: train loss 1.187775731086731 val loss 1.1815038919448853\n",
      "Epoch 5297: train loss 1.1815038919448853 val loss 1.18478262424469\n",
      "Epoch 5298: train loss 1.18478262424469 val loss 1.1838527917861938\n",
      "Epoch 5299: train loss 1.1838527917861938 val loss 1.178464651107788\n",
      "Epoch 5300: train loss 1.178464651107788 val loss 1.1791332960128784\n",
      "Epoch 5301: train loss 1.1791332960128784 val loss 1.1775305271148682\n",
      "Epoch 5302: train loss 1.1775305271148682 val loss 1.1826894283294678\n",
      "Epoch 5303: train loss 1.1826894283294678 val loss 1.1831746101379395\n",
      "Epoch 5304: train loss 1.1831746101379395 val loss 1.1766655445098877\n",
      "Epoch 5305: train loss 1.1766655445098877 val loss 1.1963486671447754\n",
      "Epoch 5306: train loss 1.1963486671447754 val loss 1.1883413791656494\n",
      "Epoch 5307: train loss 1.1883413791656494 val loss 1.1786003112792969\n",
      "Epoch 5308: train loss 1.1786003112792969 val loss 1.1784589290618896\n",
      "Epoch 5309: train loss 1.1784589290618896 val loss 1.1773451566696167\n",
      "Epoch 5310: train loss 1.1773451566696167 val loss 1.178902506828308\n",
      "Epoch 5311: train loss 1.178902506828308 val loss 1.1759796142578125\n",
      "Epoch 5312: train loss 1.1759796142578125 val loss 1.1784911155700684\n",
      "Epoch 5313: train loss 1.1784911155700684 val loss 1.1766716241836548\n",
      "Epoch 5314: train loss 1.1766716241836548 val loss 1.1789569854736328\n",
      "Epoch 5315: train loss 1.1789569854736328 val loss 1.1731630563735962\n",
      "Epoch 5316: train loss 1.1731630563735962 val loss 1.176544427871704\n",
      "Epoch 5317: train loss 1.176544427871704 val loss 1.1809309720993042\n",
      "Epoch 5318: train loss 1.1809309720993042 val loss 1.1723271608352661\n",
      "Epoch 5319: train loss 1.1723271608352661 val loss 1.1878042221069336\n",
      "Epoch 5320: train loss 1.1878042221069336 val loss 1.188781499862671\n",
      "Epoch 5321: train loss 1.188781499862671 val loss 1.1718863248825073\n",
      "Epoch 5322: train loss 1.1718863248825073 val loss 1.1887850761413574\n",
      "Epoch 5323: train loss 1.1887850761413574 val loss 1.185272216796875\n",
      "Epoch 5324: train loss 1.185272216796875 val loss 1.175430417060852\n",
      "Epoch 5325: train loss 1.175430417060852 val loss 1.1741478443145752\n",
      "Epoch 5326: train loss 1.1741478443145752 val loss 1.1704976558685303\n",
      "Epoch 5327: train loss 1.1704976558685303 val loss 1.170209288597107\n",
      "Epoch 5328: train loss 1.170209288597107 val loss 1.1694824695587158\n",
      "Epoch 5329: train loss 1.1694824695587158 val loss 1.1705620288848877\n",
      "Epoch 5330: train loss 1.1705620288848877 val loss 1.1718652248382568\n",
      "Epoch 5331: train loss 1.1718652248382568 val loss 1.1721646785736084\n",
      "Epoch 5332: train loss 1.1721646785736084 val loss 1.1693650484085083\n",
      "Epoch 5333: train loss 1.1693650484085083 val loss 1.168166160583496\n",
      "Epoch 5334: train loss 1.168166160583496 val loss 1.1678709983825684\n",
      "Epoch 5335: train loss 1.1678709983825684 val loss 1.1697351932525635\n",
      "Epoch 5336: train loss 1.1697351932525635 val loss 1.1694042682647705\n",
      "Epoch 5337: train loss 1.1694042682647705 val loss 1.1723976135253906\n",
      "Epoch 5338: train loss 1.1723976135253906 val loss 1.1661336421966553\n",
      "Epoch 5339: train loss 1.1661336421966553 val loss 1.1713614463806152\n",
      "Epoch 5340: train loss 1.1713614463806152 val loss 1.1716738939285278\n",
      "Epoch 5341: train loss 1.1716738939285278 val loss 1.172688603401184\n",
      "Epoch 5342: train loss 1.172688603401184 val loss 1.1665053367614746\n",
      "Epoch 5343: train loss 1.1665053367614746 val loss 1.166323184967041\n",
      "Epoch 5344: train loss 1.166323184967041 val loss 1.1694283485412598\n",
      "Epoch 5345: train loss 1.1694283485412598 val loss 1.1699435710906982\n",
      "Epoch 5346: train loss 1.1699435710906982 val loss 1.1653883457183838\n",
      "Epoch 5347: train loss 1.1653883457183838 val loss 1.172255277633667\n",
      "Epoch 5348: train loss 1.172255277633667 val loss 1.1662869453430176\n",
      "Epoch 5349: train loss 1.1662869453430176 val loss 1.171978235244751\n",
      "Epoch 5350: train loss 1.171978235244751 val loss 1.1900511980056763\n",
      "Epoch 5351: train loss 1.1900511980056763 val loss 1.1676946878433228\n",
      "Epoch 5352: train loss 1.1676946878433228 val loss 1.1838377714157104\n",
      "Epoch 5353: train loss 1.1838377714157104 val loss 1.1825058460235596\n",
      "Epoch 5354: train loss 1.1825058460235596 val loss 1.1636323928833008\n",
      "Epoch 5355: train loss 1.1636323928833008 val loss 1.1739404201507568\n",
      "Epoch 5356: train loss 1.1739404201507568 val loss 1.16482412815094\n",
      "Epoch 5357: train loss 1.16482412815094 val loss 1.1608190536499023\n",
      "Epoch 5358: train loss 1.1608190536499023 val loss 1.163041114807129\n",
      "Epoch 5359: train loss 1.163041114807129 val loss 1.1617026329040527\n",
      "Epoch 5360: train loss 1.1617026329040527 val loss 1.166194200515747\n",
      "Epoch 5361: train loss 1.166194200515747 val loss 1.1705873012542725\n",
      "Epoch 5362: train loss 1.1705873012542725 val loss 1.163170337677002\n",
      "Epoch 5363: train loss 1.163170337677002 val loss 1.163125991821289\n",
      "Epoch 5364: train loss 1.163125991821289 val loss 1.16043221950531\n",
      "Epoch 5365: train loss 1.16043221950531 val loss 1.1612015962600708\n",
      "Epoch 5366: train loss 1.1612015962600708 val loss 1.158691644668579\n",
      "Epoch 5367: train loss 1.158691644668579 val loss 1.160745620727539\n",
      "Epoch 5368: train loss 1.160745620727539 val loss 1.161560297012329\n",
      "Epoch 5369: train loss 1.161560297012329 val loss 1.15928053855896\n",
      "Epoch 5370: train loss 1.15928053855896 val loss 1.1609190702438354\n",
      "Epoch 5371: train loss 1.1609190702438354 val loss 1.1572067737579346\n",
      "Epoch 5372: train loss 1.1572067737579346 val loss 1.158393383026123\n",
      "Epoch 5373: train loss 1.158393383026123 val loss 1.1554348468780518\n",
      "Epoch 5374: train loss 1.1554348468780518 val loss 1.1630239486694336\n",
      "Epoch 5375: train loss 1.1630239486694336 val loss 1.177093267440796\n",
      "Epoch 5376: train loss 1.177093267440796 val loss 1.161328911781311\n",
      "Epoch 5377: train loss 1.161328911781311 val loss 1.1669938564300537\n",
      "Epoch 5378: train loss 1.1669938564300537 val loss 1.2121973037719727\n",
      "Epoch 5379: train loss 1.2121973037719727 val loss 1.1568098068237305\n",
      "Epoch 5380: train loss 1.1568098068237305 val loss 1.2028992176055908\n",
      "Epoch 5381: train loss 1.2028992176055908 val loss 1.2144328355789185\n",
      "Epoch 5382: train loss 1.2144328355789185 val loss 1.1709142923355103\n",
      "Epoch 5383: train loss 1.1709142923355103 val loss 1.2460603713989258\n",
      "Epoch 5384: train loss 1.2460603713989258 val loss 1.1582506895065308\n",
      "Epoch 5385: train loss 1.1582506895065308 val loss 1.1791471242904663\n",
      "Epoch 5386: train loss 1.1791471242904663 val loss 1.189387321472168\n",
      "Epoch 5387: train loss 1.189387321472168 val loss 1.1527721881866455\n",
      "Epoch 5388: train loss 1.1527721881866455 val loss 1.1681756973266602\n",
      "Epoch 5389: train loss 1.1681756973266602 val loss 1.1620523929595947\n",
      "Epoch 5390: train loss 1.1620523929595947 val loss 1.1615173816680908\n",
      "Epoch 5391: train loss 1.1615173816680908 val loss 1.1659178733825684\n",
      "Epoch 5392: train loss 1.1659178733825684 val loss 1.1521081924438477\n",
      "Epoch 5393: train loss 1.1521081924438477 val loss 1.1547572612762451\n",
      "Epoch 5394: train loss 1.1547572612762451 val loss 1.1550359725952148\n",
      "Epoch 5395: train loss 1.1550359725952148 val loss 1.154313564300537\n",
      "Epoch 5396: train loss 1.154313564300537 val loss 1.1551103591918945\n",
      "Epoch 5397: train loss 1.1551103591918945 val loss 1.15333890914917\n",
      "Epoch 5398: train loss 1.15333890914917 val loss 1.1520872116088867\n",
      "Epoch 5399: train loss 1.1520872116088867 val loss 1.149949312210083\n",
      "Epoch 5400: train loss 1.149949312210083 val loss 1.1482713222503662\n",
      "Epoch 5401: train loss 1.1482713222503662 val loss 1.1479476690292358\n",
      "Epoch 5402: train loss 1.1479476690292358 val loss 1.1480426788330078\n",
      "Epoch 5403: train loss 1.1480426788330078 val loss 1.1519184112548828\n",
      "Epoch 5404: train loss 1.1519184112548828 val loss 1.1478108167648315\n",
      "Epoch 5405: train loss 1.1478108167648315 val loss 1.156662940979004\n",
      "Epoch 5406: train loss 1.156662940979004 val loss 1.149776577949524\n",
      "Epoch 5407: train loss 1.149776577949524 val loss 1.148874282836914\n",
      "Epoch 5408: train loss 1.148874282836914 val loss 1.1538827419281006\n",
      "Epoch 5409: train loss 1.1538827419281006 val loss 1.1466925144195557\n",
      "Epoch 5410: train loss 1.1466925144195557 val loss 1.1500248908996582\n",
      "Epoch 5411: train loss 1.1500248908996582 val loss 1.1465308666229248\n",
      "Epoch 5412: train loss 1.1465308666229248 val loss 1.1564445495605469\n",
      "Epoch 5413: train loss 1.1564445495605469 val loss 1.1748340129852295\n",
      "Epoch 5414: train loss 1.1748340129852295 val loss 1.1519372463226318\n",
      "Epoch 5415: train loss 1.1519372463226318 val loss 1.1521953344345093\n",
      "Epoch 5416: train loss 1.1521953344345093 val loss 1.1719987392425537\n",
      "Epoch 5417: train loss 1.1719987392425537 val loss 1.144755244255066\n",
      "Epoch 5418: train loss 1.144755244255066 val loss 1.1519389152526855\n",
      "Epoch 5419: train loss 1.1519389152526855 val loss 1.1445789337158203\n",
      "Epoch 5420: train loss 1.1445789337158203 val loss 1.1460239887237549\n",
      "Epoch 5421: train loss 1.1460239887237549 val loss 1.1443994045257568\n",
      "Epoch 5422: train loss 1.1443994045257568 val loss 1.1424891948699951\n",
      "Epoch 5423: train loss 1.1424891948699951 val loss 1.1562557220458984\n",
      "Epoch 5424: train loss 1.1562557220458984 val loss 1.1579091548919678\n",
      "Epoch 5425: train loss 1.1579091548919678 val loss 1.1481130123138428\n",
      "Epoch 5426: train loss 1.1481130123138428 val loss 1.1497621536254883\n",
      "Epoch 5427: train loss 1.1497621536254883 val loss 1.1473175287246704\n",
      "Epoch 5428: train loss 1.1473175287246704 val loss 1.143142580986023\n",
      "Epoch 5429: train loss 1.143142580986023 val loss 1.143999695777893\n",
      "Epoch 5430: train loss 1.143999695777893 val loss 1.140995979309082\n",
      "Epoch 5431: train loss 1.140995979309082 val loss 1.1497642993927002\n",
      "Epoch 5432: train loss 1.1497642993927002 val loss 1.146303415298462\n",
      "Epoch 5433: train loss 1.146303415298462 val loss 1.1406666040420532\n",
      "Epoch 5434: train loss 1.1406666040420532 val loss 1.1516066789627075\n",
      "Epoch 5435: train loss 1.1516066789627075 val loss 1.141151785850525\n",
      "Epoch 5436: train loss 1.141151785850525 val loss 1.1421645879745483\n",
      "Epoch 5437: train loss 1.1421645879745483 val loss 1.1409473419189453\n",
      "Epoch 5438: train loss 1.1409473419189453 val loss 1.1399706602096558\n",
      "Epoch 5439: train loss 1.1399706602096558 val loss 1.1393887996673584\n",
      "Epoch 5440: train loss 1.1393887996673584 val loss 1.1387722492218018\n",
      "Epoch 5441: train loss 1.1387722492218018 val loss 1.1405597925186157\n",
      "Epoch 5442: train loss 1.1405597925186157 val loss 1.1677680015563965\n",
      "Epoch 5443: train loss 1.1677680015563965 val loss 1.1478526592254639\n",
      "Epoch 5444: train loss 1.1478526592254639 val loss 1.1469463109970093\n",
      "Epoch 5445: train loss 1.1469463109970093 val loss 1.1590213775634766\n",
      "Epoch 5446: train loss 1.1590213775634766 val loss 1.141711950302124\n",
      "Epoch 5447: train loss 1.141711950302124 val loss 1.1502947807312012\n",
      "Epoch 5448: train loss 1.1502947807312012 val loss 1.1466593742370605\n",
      "Epoch 5449: train loss 1.1466593742370605 val loss 1.1367688179016113\n",
      "Epoch 5450: train loss 1.1367688179016113 val loss 1.1469883918762207\n",
      "Epoch 5451: train loss 1.1469883918762207 val loss 1.1458135843276978\n",
      "Epoch 5452: train loss 1.1458135843276978 val loss 1.137128233909607\n",
      "Epoch 5453: train loss 1.137128233909607 val loss 1.1645487546920776\n",
      "Epoch 5454: train loss 1.1645487546920776 val loss 1.1456749439239502\n",
      "Epoch 5455: train loss 1.1456749439239502 val loss 1.1373666524887085\n",
      "Epoch 5456: train loss 1.1373666524887085 val loss 1.1471340656280518\n",
      "Epoch 5457: train loss 1.1471340656280518 val loss 1.137618064880371\n",
      "Epoch 5458: train loss 1.137618064880371 val loss 1.1453815698623657\n",
      "Epoch 5459: train loss 1.1453815698623657 val loss 1.175977110862732\n",
      "Epoch 5460: train loss 1.175977110862732 val loss 1.1490881443023682\n",
      "Epoch 5461: train loss 1.1490881443023682 val loss 1.1358827352523804\n",
      "Epoch 5462: train loss 1.1358827352523804 val loss 1.1512601375579834\n",
      "Epoch 5463: train loss 1.1512601375579834 val loss 1.147350788116455\n",
      "Epoch 5464: train loss 1.147350788116455 val loss 1.1350797414779663\n",
      "Epoch 5465: train loss 1.1350797414779663 val loss 1.1388291120529175\n",
      "Epoch 5466: train loss 1.1388291120529175 val loss 1.1341259479522705\n",
      "Epoch 5467: train loss 1.1341259479522705 val loss 1.1336350440979004\n",
      "Epoch 5468: train loss 1.1336350440979004 val loss 1.1359848976135254\n",
      "Epoch 5469: train loss 1.1359848976135254 val loss 1.138980507850647\n",
      "Epoch 5470: train loss 1.138980507850647 val loss 1.1334222555160522\n",
      "Epoch 5471: train loss 1.1334222555160522 val loss 1.13262939453125\n",
      "Epoch 5472: train loss 1.13262939453125 val loss 1.1429442167282104\n",
      "Epoch 5473: train loss 1.1429442167282104 val loss 1.1509780883789062\n",
      "Epoch 5474: train loss 1.1509780883789062 val loss 1.1350164413452148\n",
      "Epoch 5475: train loss 1.1350164413452148 val loss 1.1372535228729248\n",
      "Epoch 5476: train loss 1.1372535228729248 val loss 1.1419215202331543\n",
      "Epoch 5477: train loss 1.1419215202331543 val loss 1.1319142580032349\n",
      "Epoch 5478: train loss 1.1319142580032349 val loss 1.1575385332107544\n",
      "Epoch 5479: train loss 1.1575385332107544 val loss 1.1555017232894897\n",
      "Epoch 5480: train loss 1.1555017232894897 val loss 1.1304042339324951\n",
      "Epoch 5481: train loss 1.1304042339324951 val loss 1.1525193452835083\n",
      "Epoch 5482: train loss 1.1525193452835083 val loss 1.1605453491210938\n",
      "Epoch 5483: train loss 1.1605453491210938 val loss 1.1306490898132324\n",
      "Epoch 5484: train loss 1.1306490898132324 val loss 1.1620197296142578\n",
      "Epoch 5485: train loss 1.1620197296142578 val loss 1.1508936882019043\n",
      "Epoch 5486: train loss 1.1508936882019043 val loss 1.130643367767334\n",
      "Epoch 5487: train loss 1.130643367767334 val loss 1.1604437828063965\n",
      "Epoch 5488: train loss 1.1604437828063965 val loss 1.1488499641418457\n",
      "Epoch 5489: train loss 1.1488499641418457 val loss 1.1291720867156982\n",
      "Epoch 5490: train loss 1.1291720867156982 val loss 1.1515297889709473\n",
      "Epoch 5491: train loss 1.1515297889709473 val loss 1.133617639541626\n",
      "Epoch 5492: train loss 1.133617639541626 val loss 1.149707317352295\n",
      "Epoch 5493: train loss 1.149707317352295 val loss 1.176560401916504\n",
      "Epoch 5494: train loss 1.176560401916504 val loss 1.1298445463180542\n",
      "Epoch 5495: train loss 1.1298445463180542 val loss 1.144537091255188\n",
      "Epoch 5496: train loss 1.144537091255188 val loss 1.1300413608551025\n",
      "Epoch 5497: train loss 1.1300413608551025 val loss 1.1432805061340332\n",
      "Epoch 5498: train loss 1.1432805061340332 val loss 1.1570507287979126\n",
      "Epoch 5499: train loss 1.1570507287979126 val loss 1.126260757446289\n",
      "Epoch 5500: train loss 1.126260757446289 val loss 1.150996446609497\n",
      "Epoch 5501: train loss 1.150996446609497 val loss 1.1364322900772095\n",
      "Epoch 5502: train loss 1.1364322900772095 val loss 1.130560040473938\n",
      "Epoch 5503: train loss 1.130560040473938 val loss 1.1379969120025635\n",
      "Epoch 5504: train loss 1.1379969120025635 val loss 1.1367957592010498\n",
      "Epoch 5505: train loss 1.1367957592010498 val loss 1.1285499334335327\n",
      "Epoch 5506: train loss 1.1285499334335327 val loss 1.1425057649612427\n",
      "Epoch 5507: train loss 1.1425057649612427 val loss 1.1491749286651611\n",
      "Epoch 5508: train loss 1.1491749286651611 val loss 1.1264820098876953\n",
      "Epoch 5509: train loss 1.1264820098876953 val loss 1.146427869796753\n",
      "Epoch 5510: train loss 1.146427869796753 val loss 1.1587918996810913\n",
      "Epoch 5511: train loss 1.1587918996810913 val loss 1.1257469654083252\n",
      "Epoch 5512: train loss 1.1257469654083252 val loss 1.1694523096084595\n",
      "Epoch 5513: train loss 1.1694523096084595 val loss 1.1450024843215942\n",
      "Epoch 5514: train loss 1.1450024843215942 val loss 1.1274727582931519\n",
      "Epoch 5515: train loss 1.1274727582931519 val loss 1.1555685997009277\n",
      "Epoch 5516: train loss 1.1555685997009277 val loss 1.136151671409607\n",
      "Epoch 5517: train loss 1.136151671409607 val loss 1.1379376649856567\n",
      "Epoch 5518: train loss 1.1379376649856567 val loss 1.1513298749923706\n",
      "Epoch 5519: train loss 1.1513298749923706 val loss 1.1233868598937988\n",
      "Epoch 5520: train loss 1.1233868598937988 val loss 1.164644718170166\n",
      "Epoch 5521: train loss 1.164644718170166 val loss 1.1536328792572021\n",
      "Epoch 5522: train loss 1.1536328792572021 val loss 1.122572422027588\n",
      "Epoch 5523: train loss 1.122572422027588 val loss 1.1317846775054932\n",
      "Epoch 5524: train loss 1.1317846775054932 val loss 1.1251840591430664\n",
      "Epoch 5525: train loss 1.1251840591430664 val loss 1.1252532005310059\n",
      "Epoch 5526: train loss 1.1252532005310059 val loss 1.1274657249450684\n",
      "Epoch 5527: train loss 1.1274657249450684 val loss 1.124680995941162\n",
      "Epoch 5528: train loss 1.124680995941162 val loss 1.1341816186904907\n",
      "Epoch 5529: train loss 1.1341816186904907 val loss 1.1279981136322021\n",
      "Epoch 5530: train loss 1.1279981136322021 val loss 1.1221156120300293\n",
      "Epoch 5531: train loss 1.1221156120300293 val loss 1.1306328773498535\n",
      "Epoch 5532: train loss 1.1306328773498535 val loss 1.1342381238937378\n",
      "Epoch 5533: train loss 1.1342381238937378 val loss 1.1215254068374634\n",
      "Epoch 5534: train loss 1.1215254068374634 val loss 1.159849762916565\n",
      "Epoch 5535: train loss 1.159849762916565 val loss 1.1379773616790771\n",
      "Epoch 5536: train loss 1.1379773616790771 val loss 1.1393694877624512\n",
      "Epoch 5537: train loss 1.1393694877624512 val loss 1.198641061782837\n",
      "Epoch 5538: train loss 1.198641061782837 val loss 1.1308417320251465\n",
      "Epoch 5539: train loss 1.1308417320251465 val loss 1.255619764328003\n",
      "Epoch 5540: train loss 1.255619764328003 val loss 1.1344441175460815\n",
      "Epoch 5541: train loss 1.1344441175460815 val loss 1.1420176029205322\n",
      "Epoch 5542: train loss 1.1420176029205322 val loss 1.1824184656143188\n",
      "Epoch 5543: train loss 1.1824184656143188 val loss 1.1357438564300537\n",
      "Epoch 5544: train loss 1.1357438564300537 val loss 1.124530553817749\n",
      "Epoch 5545: train loss 1.124530553817749 val loss 1.1700067520141602\n",
      "Epoch 5546: train loss 1.1700067520141602 val loss 1.1257176399230957\n",
      "Epoch 5547: train loss 1.1257176399230957 val loss 1.1308109760284424\n",
      "Epoch 5548: train loss 1.1308109760284424 val loss 1.1550981998443604\n",
      "Epoch 5549: train loss 1.1550981998443604 val loss 1.1220060586929321\n",
      "Epoch 5550: train loss 1.1220060586929321 val loss 1.119364619255066\n",
      "Epoch 5551: train loss 1.119364619255066 val loss 1.1243234872817993\n",
      "Epoch 5552: train loss 1.1243234872817993 val loss 1.1219539642333984\n",
      "Epoch 5553: train loss 1.1219539642333984 val loss 1.1301679611206055\n",
      "Epoch 5554: train loss 1.1301679611206055 val loss 1.121951699256897\n",
      "Epoch 5555: train loss 1.121951699256897 val loss 1.1229287385940552\n",
      "Epoch 5556: train loss 1.1229287385940552 val loss 1.1366477012634277\n",
      "Epoch 5557: train loss 1.1366477012634277 val loss 1.1293330192565918\n",
      "Epoch 5558: train loss 1.1293330192565918 val loss 1.1235461235046387\n",
      "Epoch 5559: train loss 1.1235461235046387 val loss 1.143523931503296\n",
      "Epoch 5560: train loss 1.143523931503296 val loss 1.1319682598114014\n",
      "Epoch 5561: train loss 1.1319682598114014 val loss 1.1213935613632202\n",
      "Epoch 5562: train loss 1.1213935613632202 val loss 1.135790467262268\n",
      "Epoch 5563: train loss 1.135790467262268 val loss 1.1281795501708984\n",
      "Epoch 5564: train loss 1.1281795501708984 val loss 1.1287968158721924\n",
      "Epoch 5565: train loss 1.1287968158721924 val loss 1.126946210861206\n",
      "Epoch 5566: train loss 1.126946210861206 val loss 1.1158877611160278\n",
      "Epoch 5567: train loss 1.1158877611160278 val loss 1.1197257041931152\n",
      "Epoch 5568: train loss 1.1197257041931152 val loss 1.11617112159729\n",
      "Epoch 5569: train loss 1.11617112159729 val loss 1.1193861961364746\n",
      "Epoch 5570: train loss 1.1193861961364746 val loss 1.132554054260254\n",
      "Epoch 5571: train loss 1.132554054260254 val loss 1.1162019968032837\n",
      "Epoch 5572: train loss 1.1162019968032837 val loss 1.1505513191223145\n",
      "Epoch 5573: train loss 1.1505513191223145 val loss 1.1375219821929932\n",
      "Epoch 5574: train loss 1.1375219821929932 val loss 1.1149442195892334\n",
      "Epoch 5575: train loss 1.1149442195892334 val loss 1.1617231369018555\n",
      "Epoch 5576: train loss 1.1617231369018555 val loss 1.129665493965149\n",
      "Epoch 5577: train loss 1.129665493965149 val loss 1.1317355632781982\n",
      "Epoch 5578: train loss 1.1317355632781982 val loss 1.1543020009994507\n",
      "Epoch 5579: train loss 1.1543020009994507 val loss 1.115405797958374\n",
      "Epoch 5580: train loss 1.115405797958374 val loss 1.1305562257766724\n",
      "Epoch 5581: train loss 1.1305562257766724 val loss 1.1248207092285156\n",
      "Epoch 5582: train loss 1.1248207092285156 val loss 1.1139047145843506\n",
      "Epoch 5583: train loss 1.1139047145843506 val loss 1.1286054849624634\n",
      "Epoch 5584: train loss 1.1286054849624634 val loss 1.1146093606948853\n",
      "Epoch 5585: train loss 1.1146093606948853 val loss 1.1205980777740479\n",
      "Epoch 5586: train loss 1.1205980777740479 val loss 1.1224061250686646\n",
      "Epoch 5587: train loss 1.1224061250686646 val loss 1.1135286092758179\n",
      "Epoch 5588: train loss 1.1135286092758179 val loss 1.1381345987319946\n",
      "Epoch 5589: train loss 1.1381345987319946 val loss 1.1407088041305542\n",
      "Epoch 5590: train loss 1.1407088041305542 val loss 1.1122636795043945\n",
      "Epoch 5591: train loss 1.1122636795043945 val loss 1.1535667181015015\n",
      "Epoch 5592: train loss 1.1535667181015015 val loss 1.133485198020935\n",
      "Epoch 5593: train loss 1.133485198020935 val loss 1.130154013633728\n",
      "Epoch 5594: train loss 1.130154013633728 val loss 1.1535993814468384\n",
      "Epoch 5595: train loss 1.1535993814468384 val loss 1.1120636463165283\n",
      "Epoch 5596: train loss 1.1120636463165283 val loss 1.1390137672424316\n",
      "Epoch 5597: train loss 1.1390137672424316 val loss 1.1416492462158203\n",
      "Epoch 5598: train loss 1.1416492462158203 val loss 1.1120275259017944\n",
      "Epoch 5599: train loss 1.1120275259017944 val loss 1.1259665489196777\n",
      "Epoch 5600: train loss 1.1259665489196777 val loss 1.114375352859497\n",
      "Epoch 5601: train loss 1.114375352859497 val loss 1.110456943511963\n",
      "Epoch 5602: train loss 1.110456943511963 val loss 1.1107358932495117\n",
      "Epoch 5603: train loss 1.1107358932495117 val loss 1.110938310623169\n",
      "Epoch 5604: train loss 1.110938310623169 val loss 1.1101641654968262\n",
      "Epoch 5605: train loss 1.1101641654968262 val loss 1.1100413799285889\n",
      "Epoch 5606: train loss 1.1100413799285889 val loss 1.1102137565612793\n",
      "Epoch 5607: train loss 1.1102137565612793 val loss 1.1097936630249023\n",
      "Epoch 5608: train loss 1.1097936630249023 val loss 1.1094462871551514\n",
      "Epoch 5609: train loss 1.1094462871551514 val loss 1.1123147010803223\n",
      "Epoch 5610: train loss 1.1123147010803223 val loss 1.120837926864624\n",
      "Epoch 5611: train loss 1.120837926864624 val loss 1.1106075048446655\n",
      "Epoch 5612: train loss 1.1106075048446655 val loss 1.128572940826416\n",
      "Epoch 5613: train loss 1.128572940826416 val loss 1.1157411336898804\n",
      "Epoch 5614: train loss 1.1157411336898804 val loss 1.1126055717468262\n",
      "Epoch 5615: train loss 1.1126055717468262 val loss 1.121352195739746\n",
      "Epoch 5616: train loss 1.121352195739746 val loss 1.1110286712646484\n",
      "Epoch 5617: train loss 1.1110286712646484 val loss 1.1155378818511963\n",
      "Epoch 5618: train loss 1.1155378818511963 val loss 1.1255916357040405\n",
      "Epoch 5619: train loss 1.1255916357040405 val loss 1.114774227142334\n",
      "Epoch 5620: train loss 1.114774227142334 val loss 1.11124587059021\n",
      "Epoch 5621: train loss 1.11124587059021 val loss 1.1186327934265137\n",
      "Epoch 5622: train loss 1.1186327934265137 val loss 1.1105811595916748\n",
      "Epoch 5623: train loss 1.1105811595916748 val loss 1.1107083559036255\n",
      "Epoch 5624: train loss 1.1107083559036255 val loss 1.1149343252182007\n",
      "Epoch 5625: train loss 1.1149343252182007 val loss 1.107473611831665\n",
      "Epoch 5626: train loss 1.107473611831665 val loss 1.1084164381027222\n",
      "Epoch 5627: train loss 1.1084164381027222 val loss 1.1099859476089478\n",
      "Epoch 5628: train loss 1.1099859476089478 val loss 1.110243320465088\n",
      "Epoch 5629: train loss 1.110243320465088 val loss 1.10676908493042\n",
      "Epoch 5630: train loss 1.10676908493042 val loss 1.116629958152771\n",
      "Epoch 5631: train loss 1.116629958152771 val loss 1.1085366010665894\n",
      "Epoch 5632: train loss 1.1085366010665894 val loss 1.1107242107391357\n",
      "Epoch 5633: train loss 1.1107242107391357 val loss 1.1223944425582886\n",
      "Epoch 5634: train loss 1.1223944425582886 val loss 1.1121106147766113\n",
      "Epoch 5635: train loss 1.1121106147766113 val loss 1.1078728437423706\n",
      "Epoch 5636: train loss 1.1078728437423706 val loss 1.1220710277557373\n",
      "Epoch 5637: train loss 1.1220710277557373 val loss 1.1083770990371704\n",
      "Epoch 5638: train loss 1.1083770990371704 val loss 1.1109204292297363\n",
      "Epoch 5639: train loss 1.1109204292297363 val loss 1.1231253147125244\n",
      "Epoch 5640: train loss 1.1231253147125244 val loss 1.1070191860198975\n",
      "Epoch 5641: train loss 1.1070191860198975 val loss 1.1090890169143677\n",
      "Epoch 5642: train loss 1.1090890169143677 val loss 1.1088495254516602\n",
      "Epoch 5643: train loss 1.1088495254516602 val loss 1.1065263748168945\n",
      "Epoch 5644: train loss 1.1065263748168945 val loss 1.1056263446807861\n",
      "Epoch 5645: train loss 1.1056263446807861 val loss 1.1115331649780273\n",
      "Epoch 5646: train loss 1.1115331649780273 val loss 1.1060682535171509\n",
      "Epoch 5647: train loss 1.1060682535171509 val loss 1.1064763069152832\n",
      "Epoch 5648: train loss 1.1064763069152832 val loss 1.1117401123046875\n",
      "Epoch 5649: train loss 1.1117401123046875 val loss 1.1036319732666016\n",
      "Epoch 5650: train loss 1.1036319732666016 val loss 1.103947639465332\n",
      "Epoch 5651: train loss 1.103947639465332 val loss 1.1076300144195557\n",
      "Epoch 5652: train loss 1.1076300144195557 val loss 1.1097148656845093\n",
      "Epoch 5653: train loss 1.1097148656845093 val loss 1.1025149822235107\n",
      "Epoch 5654: train loss 1.1025149822235107 val loss 1.1041417121887207\n",
      "Epoch 5655: train loss 1.1041417121887207 val loss 1.102005958557129\n",
      "Epoch 5656: train loss 1.102005958557129 val loss 1.1023296117782593\n",
      "Epoch 5657: train loss 1.1023296117782593 val loss 1.1016732454299927\n",
      "Epoch 5658: train loss 1.1016732454299927 val loss 1.1036202907562256\n",
      "Epoch 5659: train loss 1.1036202907562256 val loss 1.1217358112335205\n",
      "Epoch 5660: train loss 1.1217358112335205 val loss 1.1030831336975098\n",
      "Epoch 5661: train loss 1.1030831336975098 val loss 1.1154751777648926\n",
      "Epoch 5662: train loss 1.1154751777648926 val loss 1.1257374286651611\n",
      "Epoch 5663: train loss 1.1257374286651611 val loss 1.1021877527236938\n",
      "Epoch 5664: train loss 1.1021877527236938 val loss 1.1326205730438232\n",
      "Epoch 5665: train loss 1.1326205730438232 val loss 1.1087065935134888\n",
      "Epoch 5666: train loss 1.1087065935134888 val loss 1.1107876300811768\n",
      "Epoch 5667: train loss 1.1107876300811768 val loss 1.1405346393585205\n",
      "Epoch 5668: train loss 1.1405346393585205 val loss 1.1010504961013794\n",
      "Epoch 5669: train loss 1.1010504961013794 val loss 1.1195005178451538\n",
      "Epoch 5670: train loss 1.1195005178451538 val loss 1.1071364879608154\n",
      "Epoch 5671: train loss 1.1071364879608154 val loss 1.100895643234253\n",
      "Epoch 5672: train loss 1.100895643234253 val loss 1.1063337326049805\n",
      "Epoch 5673: train loss 1.1063337326049805 val loss 1.1010549068450928\n",
      "Epoch 5674: train loss 1.1010549068450928 val loss 1.1032845973968506\n",
      "Epoch 5675: train loss 1.1032845973968506 val loss 1.0990533828735352\n",
      "Epoch 5676: train loss 1.0990533828735352 val loss 1.1084206104278564\n",
      "Epoch 5677: train loss 1.1084206104278564 val loss 1.1143414974212646\n",
      "Epoch 5678: train loss 1.1143414974212646 val loss 1.0993164777755737\n",
      "Epoch 5679: train loss 1.0993164777755737 val loss 1.1261563301086426\n",
      "Epoch 5680: train loss 1.1261563301086426 val loss 1.1166529655456543\n",
      "Epoch 5681: train loss 1.1166529655456543 val loss 1.1023311614990234\n",
      "Epoch 5682: train loss 1.1023311614990234 val loss 1.1289725303649902\n",
      "Epoch 5683: train loss 1.1289725303649902 val loss 1.1025869846343994\n",
      "Epoch 5684: train loss 1.1025869846343994 val loss 1.1109788417816162\n",
      "Epoch 5685: train loss 1.1109788417816162 val loss 1.1196348667144775\n",
      "Epoch 5686: train loss 1.1196348667144775 val loss 1.098154902458191\n",
      "Epoch 5687: train loss 1.098154902458191 val loss 1.100917935371399\n",
      "Epoch 5688: train loss 1.100917935371399 val loss 1.1045887470245361\n",
      "Epoch 5689: train loss 1.1045887470245361 val loss 1.0986757278442383\n",
      "Epoch 5690: train loss 1.0986757278442383 val loss 1.0969834327697754\n",
      "Epoch 5691: train loss 1.0969834327697754 val loss 1.1075210571289062\n",
      "Epoch 5692: train loss 1.1075210571289062 val loss 1.0981262922286987\n",
      "Epoch 5693: train loss 1.0981262922286987 val loss 1.0981521606445312\n",
      "Epoch 5694: train loss 1.0981521606445312 val loss 1.1040910482406616\n",
      "Epoch 5695: train loss 1.1040910482406616 val loss 1.0993177890777588\n",
      "Epoch 5696: train loss 1.0993177890777588 val loss 1.0984997749328613\n",
      "Epoch 5697: train loss 1.0984997749328613 val loss 1.1017922163009644\n",
      "Epoch 5698: train loss 1.1017922163009644 val loss 1.0979573726654053\n",
      "Epoch 5699: train loss 1.0979573726654053 val loss 1.095923662185669\n",
      "Epoch 5700: train loss 1.095923662185669 val loss 1.110203504562378\n",
      "Epoch 5701: train loss 1.110203504562378 val loss 1.1036860942840576\n",
      "Epoch 5702: train loss 1.1036860942840576 val loss 1.094878911972046\n",
      "Epoch 5703: train loss 1.094878911972046 val loss 1.124219298362732\n",
      "Epoch 5704: train loss 1.124219298362732 val loss 1.1130731105804443\n",
      "Epoch 5705: train loss 1.1130731105804443 val loss 1.0982978343963623\n",
      "Epoch 5706: train loss 1.0982978343963623 val loss 1.1292688846588135\n",
      "Epoch 5707: train loss 1.1292688846588135 val loss 1.1006819009780884\n",
      "Epoch 5708: train loss 1.1006819009780884 val loss 1.0997800827026367\n",
      "Epoch 5709: train loss 1.0997800827026367 val loss 1.104268193244934\n",
      "Epoch 5710: train loss 1.104268193244934 val loss 1.0935778617858887\n",
      "Epoch 5711: train loss 1.0935778617858887 val loss 1.093766450881958\n",
      "Epoch 5712: train loss 1.093766450881958 val loss 1.0999500751495361\n",
      "Epoch 5713: train loss 1.0999500751495361 val loss 1.098603367805481\n",
      "Epoch 5714: train loss 1.098603367805481 val loss 1.0992742776870728\n",
      "Epoch 5715: train loss 1.0992742776870728 val loss 1.0943069458007812\n",
      "Epoch 5716: train loss 1.0943069458007812 val loss 1.0932157039642334\n",
      "Epoch 5717: train loss 1.0932157039642334 val loss 1.1010304689407349\n",
      "Epoch 5718: train loss 1.1010304689407349 val loss 1.1099765300750732\n",
      "Epoch 5719: train loss 1.1099765300750732 val loss 1.0949455499649048\n",
      "Epoch 5720: train loss 1.0949455499649048 val loss 1.0929174423217773\n",
      "Epoch 5721: train loss 1.0929174423217773 val loss 1.095361590385437\n",
      "Epoch 5722: train loss 1.095361590385437 val loss 1.112370491027832\n",
      "Epoch 5723: train loss 1.112370491027832 val loss 1.0923383235931396\n",
      "Epoch 5724: train loss 1.0923383235931396 val loss 1.129051685333252\n",
      "Epoch 5725: train loss 1.129051685333252 val loss 1.106300950050354\n",
      "Epoch 5726: train loss 1.106300950050354 val loss 1.0944708585739136\n",
      "Epoch 5727: train loss 1.0944708585739136 val loss 1.1151527166366577\n",
      "Epoch 5728: train loss 1.1151527166366577 val loss 1.1091865301132202\n",
      "Epoch 5729: train loss 1.1091865301132202 val loss 1.09287691116333\n",
      "Epoch 5730: train loss 1.09287691116333 val loss 1.117962121963501\n",
      "Epoch 5731: train loss 1.117962121963501 val loss 1.0966758728027344\n",
      "Epoch 5732: train loss 1.0966758728027344 val loss 1.0940585136413574\n",
      "Epoch 5733: train loss 1.0940585136413574 val loss 1.1035549640655518\n",
      "Epoch 5734: train loss 1.1035549640655518 val loss 1.0956971645355225\n",
      "Epoch 5735: train loss 1.0956971645355225 val loss 1.094027042388916\n",
      "Epoch 5736: train loss 1.094027042388916 val loss 1.0996155738830566\n",
      "Epoch 5737: train loss 1.0996155738830566 val loss 1.0917797088623047\n",
      "Epoch 5738: train loss 1.0917797088623047 val loss 1.091381549835205\n",
      "Epoch 5739: train loss 1.091381549835205 val loss 1.0943505764007568\n",
      "Epoch 5740: train loss 1.0943505764007568 val loss 1.0959242582321167\n",
      "Epoch 5741: train loss 1.0959242582321167 val loss 1.0913203954696655\n",
      "Epoch 5742: train loss 1.0913203954696655 val loss 1.110210657119751\n",
      "Epoch 5743: train loss 1.110210657119751 val loss 1.108373999595642\n",
      "Epoch 5744: train loss 1.108373999595642 val loss 1.090879201889038\n",
      "Epoch 5745: train loss 1.090879201889038 val loss 1.1203501224517822\n",
      "Epoch 5746: train loss 1.1203501224517822 val loss 1.0976524353027344\n",
      "Epoch 5747: train loss 1.0976524353027344 val loss 1.088895320892334\n",
      "Epoch 5748: train loss 1.088895320892334 val loss 1.104480504989624\n",
      "Epoch 5749: train loss 1.104480504989624 val loss 1.1071093082427979\n",
      "Epoch 5750: train loss 1.1071093082427979 val loss 1.0883090496063232\n",
      "Epoch 5751: train loss 1.0883090496063232 val loss 1.1283698081970215\n",
      "Epoch 5752: train loss 1.1283698081970215 val loss 1.1042866706848145\n",
      "Epoch 5753: train loss 1.1042866706848145 val loss 1.0947574377059937\n",
      "Epoch 5754: train loss 1.0947574377059937 val loss 1.1246039867401123\n",
      "Epoch 5755: train loss 1.1246039867401123 val loss 1.0935399532318115\n",
      "Epoch 5756: train loss 1.0935399532318115 val loss 1.1161766052246094\n",
      "Epoch 5757: train loss 1.1161766052246094 val loss 1.0983415842056274\n",
      "Epoch 5758: train loss 1.0983415842056274 val loss 1.0916157960891724\n",
      "Epoch 5759: train loss 1.0916157960891724 val loss 1.1119428873062134\n",
      "Epoch 5760: train loss 1.1119428873062134 val loss 1.0869152545928955\n",
      "Epoch 5761: train loss 1.0869152545928955 val loss 1.0971202850341797\n",
      "Epoch 5762: train loss 1.0971202850341797 val loss 1.104142427444458\n",
      "Epoch 5763: train loss 1.104142427444458 val loss 1.0878939628601074\n",
      "Epoch 5764: train loss 1.0878939628601074 val loss 1.087862491607666\n",
      "Epoch 5765: train loss 1.087862491607666 val loss 1.086103081703186\n",
      "Epoch 5766: train loss 1.086103081703186 val loss 1.0974388122558594\n",
      "Epoch 5767: train loss 1.0974388122558594 val loss 1.0902100801467896\n",
      "Epoch 5768: train loss 1.0902100801467896 val loss 1.088697910308838\n",
      "Epoch 5769: train loss 1.088697910308838 val loss 1.0997352600097656\n",
      "Epoch 5770: train loss 1.0997352600097656 val loss 1.0961908102035522\n",
      "Epoch 5771: train loss 1.0961908102035522 val loss 1.0886240005493164\n",
      "Epoch 5772: train loss 1.0886240005493164 val loss 1.0874478816986084\n",
      "Epoch 5773: train loss 1.0874478816986084 val loss 1.0846989154815674\n",
      "Epoch 5774: train loss 1.0846989154815674 val loss 1.0946190357208252\n",
      "Epoch 5775: train loss 1.0946190357208252 val loss 1.0878407955169678\n",
      "Epoch 5776: train loss 1.0878407955169678 val loss 1.086742877960205\n",
      "Epoch 5777: train loss 1.086742877960205 val loss 1.0978095531463623\n",
      "Epoch 5778: train loss 1.0978095531463623 val loss 1.108750581741333\n",
      "Epoch 5779: train loss 1.108750581741333 val loss 1.085269808769226\n",
      "Epoch 5780: train loss 1.085269808769226 val loss 1.1230180263519287\n",
      "Epoch 5781: train loss 1.1230180263519287 val loss 1.0960317850112915\n",
      "Epoch 5782: train loss 1.0960317850112915 val loss 1.0840517282485962\n",
      "Epoch 5783: train loss 1.0840517282485962 val loss 1.1066184043884277\n",
      "Epoch 5784: train loss 1.1066184043884277 val loss 1.0930147171020508\n",
      "Epoch 5785: train loss 1.0930147171020508 val loss 1.0853197574615479\n",
      "Epoch 5786: train loss 1.0853197574615479 val loss 1.0841431617736816\n",
      "Epoch 5787: train loss 1.0841431617736816 val loss 1.0856304168701172\n",
      "Epoch 5788: train loss 1.0856304168701172 val loss 1.0957249402999878\n",
      "Epoch 5789: train loss 1.0957249402999878 val loss 1.0945205688476562\n",
      "Epoch 5790: train loss 1.0945205688476562 val loss 1.0841867923736572\n",
      "Epoch 5791: train loss 1.0841867923736572 val loss 1.0969479084014893\n",
      "Epoch 5792: train loss 1.0969479084014893 val loss 1.1000072956085205\n",
      "Epoch 5793: train loss 1.1000072956085205 val loss 1.0852618217468262\n",
      "Epoch 5794: train loss 1.0852618217468262 val loss 1.0854361057281494\n",
      "Epoch 5795: train loss 1.0854361057281494 val loss 1.092395544052124\n",
      "Epoch 5796: train loss 1.092395544052124 val loss 1.0849426984786987\n",
      "Epoch 5797: train loss 1.0849426984786987 val loss 1.0832123756408691\n",
      "Epoch 5798: train loss 1.0832123756408691 val loss 1.0891339778900146\n",
      "Epoch 5799: train loss 1.0891339778900146 val loss 1.0879759788513184\n",
      "Epoch 5800: train loss 1.0879759788513184 val loss 1.0824875831604004\n",
      "Epoch 5801: train loss 1.0824875831604004 val loss 1.1060094833374023\n",
      "Epoch 5802: train loss 1.1060094833374023 val loss 1.1043031215667725\n",
      "Epoch 5803: train loss 1.1043031215667725 val loss 1.0850768089294434\n",
      "Epoch 5804: train loss 1.0850768089294434 val loss 1.1156513690948486\n",
      "Epoch 5805: train loss 1.1156513690948486 val loss 1.08601713180542\n",
      "Epoch 5806: train loss 1.08601713180542 val loss 1.1025234460830688\n",
      "Epoch 5807: train loss 1.1025234460830688 val loss 1.1074457168579102\n",
      "Epoch 5808: train loss 1.1074457168579102 val loss 1.0801899433135986\n",
      "Epoch 5809: train loss 1.0801899433135986 val loss 1.1231515407562256\n",
      "Epoch 5810: train loss 1.1231515407562256 val loss 1.1704097986221313\n",
      "Epoch 5811: train loss 1.1704097986221313 val loss 1.0835974216461182\n",
      "Epoch 5812: train loss 1.0835974216461182 val loss 1.129837155342102\n",
      "Epoch 5813: train loss 1.129837155342102 val loss 1.175060749053955\n",
      "Epoch 5814: train loss 1.175060749053955 val loss 1.0849331617355347\n",
      "Epoch 5815: train loss 1.0849331617355347 val loss 1.211576223373413\n",
      "Epoch 5816: train loss 1.211576223373413 val loss 1.1121046543121338\n",
      "Epoch 5817: train loss 1.1121046543121338 val loss 1.1286365985870361\n",
      "Epoch 5818: train loss 1.1286365985870361 val loss 1.1515047550201416\n",
      "Epoch 5819: train loss 1.1515047550201416 val loss 1.0813969373703003\n",
      "Epoch 5820: train loss 1.0813969373703003 val loss 1.1626172065734863\n",
      "Epoch 5821: train loss 1.1626172065734863 val loss 1.1095759868621826\n",
      "Epoch 5822: train loss 1.1095759868621826 val loss 1.1300983428955078\n",
      "Epoch 5823: train loss 1.1300983428955078 val loss 1.228806972503662\n",
      "Epoch 5824: train loss 1.228806972503662 val loss 1.0842517614364624\n",
      "Epoch 5825: train loss 1.0842517614364624 val loss 1.2529728412628174\n",
      "Epoch 5826: train loss 1.2529728412628174 val loss 1.0838642120361328\n",
      "Epoch 5827: train loss 1.0838642120361328 val loss 1.1873676776885986\n",
      "Epoch 5828: train loss 1.1873676776885986 val loss 1.1149954795837402\n",
      "Epoch 5829: train loss 1.1149954795837402 val loss 1.1160051822662354\n",
      "Epoch 5830: train loss 1.1160051822662354 val loss 1.1508314609527588\n",
      "Epoch 5831: train loss 1.1508314609527588 val loss 1.0797760486602783\n",
      "Epoch 5832: train loss 1.0797760486602783 val loss 1.1098287105560303\n",
      "Epoch 5833: train loss 1.1098287105560303 val loss 1.0787990093231201\n",
      "Epoch 5834: train loss 1.0787990093231201 val loss 1.094788670539856\n",
      "Epoch 5835: train loss 1.094788670539856 val loss 1.0944838523864746\n",
      "Epoch 5836: train loss 1.0944838523864746 val loss 1.0794427394866943\n",
      "Epoch 5837: train loss 1.0794427394866943 val loss 1.086161732673645\n",
      "Epoch 5838: train loss 1.086161732673645 val loss 1.1043832302093506\n",
      "Epoch 5839: train loss 1.1043832302093506 val loss 1.079249382019043\n",
      "Epoch 5840: train loss 1.079249382019043 val loss 1.1150637865066528\n",
      "Epoch 5841: train loss 1.1150637865066528 val loss 1.0838520526885986\n",
      "Epoch 5842: train loss 1.0838520526885986 val loss 1.0861793756484985\n",
      "Epoch 5843: train loss 1.0861793756484985 val loss 1.1070092916488647\n",
      "Epoch 5844: train loss 1.1070092916488647 val loss 1.078387975692749\n",
      "Epoch 5845: train loss 1.078387975692749 val loss 1.0825676918029785\n",
      "Epoch 5846: train loss 1.0825676918029785 val loss 1.083449363708496\n",
      "Epoch 5847: train loss 1.083449363708496 val loss 1.0776143074035645\n",
      "Epoch 5848: train loss 1.0776143074035645 val loss 1.0773361921310425\n",
      "Epoch 5849: train loss 1.0773361921310425 val loss 1.0769572257995605\n",
      "Epoch 5850: train loss 1.0769572257995605 val loss 1.080345630645752\n",
      "Epoch 5851: train loss 1.080345630645752 val loss 1.0817289352416992\n",
      "Epoch 5852: train loss 1.0817289352416992 val loss 1.0785579681396484\n",
      "Epoch 5853: train loss 1.0785579681396484 val loss 1.077035903930664\n",
      "Epoch 5854: train loss 1.077035903930664 val loss 1.0913593769073486\n",
      "Epoch 5855: train loss 1.0913593769073486 val loss 1.0912601947784424\n",
      "Epoch 5856: train loss 1.0912601947784424 val loss 1.0793378353118896\n",
      "Epoch 5857: train loss 1.0793378353118896 val loss 1.0777475833892822\n",
      "Epoch 5858: train loss 1.0777475833892822 val loss 1.0788251161575317\n",
      "Epoch 5859: train loss 1.0788251161575317 val loss 1.0818188190460205\n",
      "Epoch 5860: train loss 1.0818188190460205 val loss 1.0802342891693115\n",
      "Epoch 5861: train loss 1.0802342891693115 val loss 1.0760818719863892\n",
      "Epoch 5862: train loss 1.0760818719863892 val loss 1.0778374671936035\n",
      "Epoch 5863: train loss 1.0778374671936035 val loss 1.0885579586029053\n",
      "Epoch 5864: train loss 1.0885579586029053 val loss 1.0952532291412354\n",
      "Epoch 5865: train loss 1.0952532291412354 val loss 1.0748231410980225\n",
      "Epoch 5866: train loss 1.0748231410980225 val loss 1.0797988176345825\n",
      "Epoch 5867: train loss 1.0797988176345825 val loss 1.0873165130615234\n",
      "Epoch 5868: train loss 1.0873165130615234 val loss 1.0807983875274658\n",
      "Epoch 5869: train loss 1.0807983875274658 val loss 1.0747500658035278\n",
      "Epoch 5870: train loss 1.0747500658035278 val loss 1.075465440750122\n",
      "Epoch 5871: train loss 1.075465440750122 val loss 1.0752285718917847\n",
      "Epoch 5872: train loss 1.0752285718917847 val loss 1.0818191766738892\n",
      "Epoch 5873: train loss 1.0818191766738892 val loss 1.0772578716278076\n",
      "Epoch 5874: train loss 1.0772578716278076 val loss 1.0749562978744507\n",
      "Epoch 5875: train loss 1.0749562978744507 val loss 1.074429988861084\n",
      "Epoch 5876: train loss 1.074429988861084 val loss 1.0751036405563354\n",
      "Epoch 5877: train loss 1.0751036405563354 val loss 1.075585961341858\n",
      "Epoch 5878: train loss 1.075585961341858 val loss 1.0784013271331787\n",
      "Epoch 5879: train loss 1.0784013271331787 val loss 1.0795817375183105\n",
      "Epoch 5880: train loss 1.0795817375183105 val loss 1.0727789402008057\n",
      "Epoch 5881: train loss 1.0727789402008057 val loss 1.087982177734375\n",
      "Epoch 5882: train loss 1.087982177734375 val loss 1.0987539291381836\n",
      "Epoch 5883: train loss 1.0987539291381836 val loss 1.0738956928253174\n",
      "Epoch 5884: train loss 1.0738956928253174 val loss 1.1032406091690063\n",
      "Epoch 5885: train loss 1.1032406091690063 val loss 1.0991151332855225\n",
      "Epoch 5886: train loss 1.0991151332855225 val loss 1.0744260549545288\n",
      "Epoch 5887: train loss 1.0744260549545288 val loss 1.1004226207733154\n",
      "Epoch 5888: train loss 1.1004226207733154 val loss 1.074211597442627\n",
      "Epoch 5889: train loss 1.074211597442627 val loss 1.0971808433532715\n",
      "Epoch 5890: train loss 1.0971808433532715 val loss 1.0858221054077148\n",
      "Epoch 5891: train loss 1.0858221054077148 val loss 1.0727884769439697\n",
      "Epoch 5892: train loss 1.0727884769439697 val loss 1.0779070854187012\n",
      "Epoch 5893: train loss 1.0779070854187012 val loss 1.0768595933914185\n",
      "Epoch 5894: train loss 1.0768595933914185 val loss 1.0718610286712646\n",
      "Epoch 5895: train loss 1.0718610286712646 val loss 1.073270320892334\n",
      "Epoch 5896: train loss 1.073270320892334 val loss 1.0842567682266235\n",
      "Epoch 5897: train loss 1.0842567682266235 val loss 1.09169340133667\n",
      "Epoch 5898: train loss 1.09169340133667 val loss 1.07295823097229\n",
      "Epoch 5899: train loss 1.07295823097229 val loss 1.1030585765838623\n",
      "Epoch 5900: train loss 1.1030585765838623 val loss 1.1006243228912354\n",
      "Epoch 5901: train loss 1.1006243228912354 val loss 1.0735160112380981\n",
      "Epoch 5902: train loss 1.0735160112380981 val loss 1.0978050231933594\n",
      "Epoch 5903: train loss 1.0978050231933594 val loss 1.0720493793487549\n",
      "Epoch 5904: train loss 1.0720493793487549 val loss 1.0917391777038574\n",
      "Epoch 5905: train loss 1.0917391777038574 val loss 1.0892688035964966\n",
      "Epoch 5906: train loss 1.0892688035964966 val loss 1.0716073513031006\n",
      "Epoch 5907: train loss 1.0716073513031006 val loss 1.076796531677246\n",
      "Epoch 5908: train loss 1.076796531677246 val loss 1.0766245126724243\n",
      "Epoch 5909: train loss 1.0766245126724243 val loss 1.0778424739837646\n",
      "Epoch 5910: train loss 1.0778424739837646 val loss 1.0715473890304565\n",
      "Epoch 5911: train loss 1.0715473890304565 val loss 1.085951805114746\n",
      "Epoch 5912: train loss 1.085951805114746 val loss 1.0898330211639404\n",
      "Epoch 5913: train loss 1.0898330211639404 val loss 1.0755094289779663\n",
      "Epoch 5914: train loss 1.0755094289779663 val loss 1.0702567100524902\n",
      "Epoch 5915: train loss 1.0702567100524902 val loss 1.093178629875183\n",
      "Epoch 5916: train loss 1.093178629875183 val loss 1.0880334377288818\n",
      "Epoch 5917: train loss 1.0880334377288818 val loss 1.0693585872650146\n",
      "Epoch 5918: train loss 1.0693585872650146 val loss 1.076404094696045\n",
      "Epoch 5919: train loss 1.076404094696045 val loss 1.091901421546936\n",
      "Epoch 5920: train loss 1.091901421546936 val loss 1.0704376697540283\n",
      "Epoch 5921: train loss 1.0704376697540283 val loss 1.0882809162139893\n",
      "Epoch 5922: train loss 1.0882809162139893 val loss 1.0905717611312866\n",
      "Epoch 5923: train loss 1.0905717611312866 val loss 1.069258451461792\n",
      "Epoch 5924: train loss 1.069258451461792 val loss 1.075840950012207\n",
      "Epoch 5925: train loss 1.075840950012207 val loss 1.0744444131851196\n",
      "Epoch 5926: train loss 1.0744444131851196 val loss 1.0696029663085938\n",
      "Epoch 5927: train loss 1.0696029663085938 val loss 1.071038842201233\n",
      "Epoch 5928: train loss 1.071038842201233 val loss 1.076729655265808\n",
      "Epoch 5929: train loss 1.076729655265808 val loss 1.0923631191253662\n",
      "Epoch 5930: train loss 1.0923631191253662 val loss 1.0683506727218628\n",
      "Epoch 5931: train loss 1.0683506727218628 val loss 1.1024401187896729\n",
      "Epoch 5932: train loss 1.1024401187896729 val loss 1.0819140672683716\n",
      "Epoch 5933: train loss 1.0819140672683716 val loss 1.0691202878952026\n",
      "Epoch 5934: train loss 1.0691202878952026 val loss 1.083626389503479\n",
      "Epoch 5935: train loss 1.083626389503479 val loss 1.085007667541504\n",
      "Epoch 5936: train loss 1.085007667541504 val loss 1.0681838989257812\n",
      "Epoch 5937: train loss 1.0681838989257812 val loss 1.0683908462524414\n",
      "Epoch 5938: train loss 1.0683908462524414 val loss 1.0682569742202759\n",
      "Epoch 5939: train loss 1.0682569742202759 val loss 1.0674571990966797\n",
      "Epoch 5940: train loss 1.0674571990966797 val loss 1.067797064781189\n",
      "Epoch 5941: train loss 1.067797064781189 val loss 1.071276068687439\n",
      "Epoch 5942: train loss 1.071276068687439 val loss 1.0668714046478271\n",
      "Epoch 5943: train loss 1.0668714046478271 val loss 1.067732334136963\n",
      "Epoch 5944: train loss 1.067732334136963 val loss 1.0661952495574951\n",
      "Epoch 5945: train loss 1.0661952495574951 val loss 1.0692962408065796\n",
      "Epoch 5946: train loss 1.0692962408065796 val loss 1.071883201599121\n",
      "Epoch 5947: train loss 1.071883201599121 val loss 1.0672732591629028\n",
      "Epoch 5948: train loss 1.0672732591629028 val loss 1.0673630237579346\n",
      "Epoch 5949: train loss 1.0673630237579346 val loss 1.0676043033599854\n",
      "Epoch 5950: train loss 1.0676043033599854 val loss 1.0655572414398193\n",
      "Epoch 5951: train loss 1.0655572414398193 val loss 1.0691676139831543\n",
      "Epoch 5952: train loss 1.0691676139831543 val loss 1.088918924331665\n",
      "Epoch 5953: train loss 1.088918924331665 val loss 1.0718070268630981\n",
      "Epoch 5954: train loss 1.0718070268630981 val loss 1.0697224140167236\n",
      "Epoch 5955: train loss 1.0697224140167236 val loss 1.0655112266540527\n",
      "Epoch 5956: train loss 1.0655112266540527 val loss 1.0752978324890137\n",
      "Epoch 5957: train loss 1.0752978324890137 val loss 1.104325532913208\n",
      "Epoch 5958: train loss 1.104325532913208 val loss 1.0945994853973389\n",
      "Epoch 5959: train loss 1.0945994853973389 val loss 1.079750657081604\n",
      "Epoch 5960: train loss 1.079750657081604 val loss 1.1642751693725586\n",
      "Epoch 5961: train loss 1.1642751693725586 val loss 1.0900523662567139\n",
      "Epoch 5962: train loss 1.0900523662567139 val loss 1.0806894302368164\n",
      "Epoch 5963: train loss 1.0806894302368164 val loss 1.1479237079620361\n",
      "Epoch 5964: train loss 1.1479237079620361 val loss 1.06894052028656\n",
      "Epoch 5965: train loss 1.06894052028656 val loss 1.0976276397705078\n",
      "Epoch 5966: train loss 1.0976276397705078 val loss 1.0993926525115967\n",
      "Epoch 5967: train loss 1.0993926525115967 val loss 1.0686588287353516\n",
      "Epoch 5968: train loss 1.0686588287353516 val loss 1.071346640586853\n",
      "Epoch 5969: train loss 1.071346640586853 val loss 1.117760181427002\n",
      "Epoch 5970: train loss 1.117760181427002 val loss 1.074150562286377\n",
      "Epoch 5971: train loss 1.074150562286377 val loss 1.0656390190124512\n",
      "Epoch 5972: train loss 1.0656390190124512 val loss 1.0955774784088135\n",
      "Epoch 5973: train loss 1.0955774784088135 val loss 1.0652905702590942\n",
      "Epoch 5974: train loss 1.0652905702590942 val loss 1.082385778427124\n",
      "Epoch 5975: train loss 1.082385778427124 val loss 1.0915318727493286\n",
      "Epoch 5976: train loss 1.0915318727493286 val loss 1.0649559497833252\n",
      "Epoch 5977: train loss 1.0649559497833252 val loss 1.10468590259552\n",
      "Epoch 5978: train loss 1.10468590259552 val loss 1.0800459384918213\n",
      "Epoch 5979: train loss 1.0800459384918213 val loss 1.0669459104537964\n",
      "Epoch 5980: train loss 1.0669459104537964 val loss 1.0780680179595947\n",
      "Epoch 5981: train loss 1.0780680179595947 val loss 1.0735857486724854\n",
      "Epoch 5982: train loss 1.0735857486724854 val loss 1.0674234628677368\n",
      "Epoch 5983: train loss 1.0674234628677368 val loss 1.0641403198242188\n",
      "Epoch 5984: train loss 1.0641403198242188 val loss 1.0661935806274414\n",
      "Epoch 5985: train loss 1.0661935806274414 val loss 1.0805280208587646\n",
      "Epoch 5986: train loss 1.0805280208587646 val loss 1.0706026554107666\n",
      "Epoch 5987: train loss 1.0706026554107666 val loss 1.0647889375686646\n",
      "Epoch 5988: train loss 1.0647889375686646 val loss 1.0655051469802856\n",
      "Epoch 5989: train loss 1.0655051469802856 val loss 1.0643608570098877\n",
      "Epoch 5990: train loss 1.0643608570098877 val loss 1.067481279373169\n",
      "Epoch 5991: train loss 1.067481279373169 val loss 1.0809682607650757\n",
      "Epoch 5992: train loss 1.0809682607650757 val loss 1.0696594715118408\n",
      "Epoch 5993: train loss 1.0696594715118408 val loss 1.0623291730880737\n",
      "Epoch 5994: train loss 1.0623291730880737 val loss 1.082976222038269\n",
      "Epoch 5995: train loss 1.082976222038269 val loss 1.084989070892334\n",
      "Epoch 5996: train loss 1.084989070892334 val loss 1.064143419265747\n",
      "Epoch 5997: train loss 1.064143419265747 val loss 1.09123694896698\n",
      "Epoch 5998: train loss 1.09123694896698 val loss 1.0744178295135498\n",
      "Epoch 5999: train loss 1.0744178295135498 val loss 1.0632107257843018\n",
      "Epoch 6000: train loss 1.0632107257843018 val loss 1.0795042514801025\n",
      "Epoch 6001: train loss 1.0795042514801025 val loss 1.077728509902954\n",
      "Epoch 6002: train loss 1.077728509902954 val loss 1.0644761323928833\n",
      "Epoch 6003: train loss 1.0644761323928833 val loss 1.0618207454681396\n",
      "Epoch 6004: train loss 1.0618207454681396 val loss 1.0614274740219116\n",
      "Epoch 6005: train loss 1.0614274740219116 val loss 1.060779094696045\n",
      "Epoch 6006: train loss 1.060779094696045 val loss 1.0612213611602783\n",
      "Epoch 6007: train loss 1.0612213611602783 val loss 1.0615652799606323\n",
      "Epoch 6008: train loss 1.0615652799606323 val loss 1.0715559720993042\n",
      "Epoch 6009: train loss 1.0715559720993042 val loss 1.080870509147644\n",
      "Epoch 6010: train loss 1.080870509147644 val loss 1.0633481740951538\n",
      "Epoch 6011: train loss 1.0633481740951538 val loss 1.061673641204834\n",
      "Epoch 6012: train loss 1.061673641204834 val loss 1.0605281591415405\n",
      "Epoch 6013: train loss 1.0605281591415405 val loss 1.0619232654571533\n",
      "Epoch 6014: train loss 1.0619232654571533 val loss 1.0852853059768677\n",
      "Epoch 6015: train loss 1.0852853059768677 val loss 1.0751960277557373\n",
      "Epoch 6016: train loss 1.0751960277557373 val loss 1.0620510578155518\n",
      "Epoch 6017: train loss 1.0620510578155518 val loss 1.070605993270874\n",
      "Epoch 6018: train loss 1.070605993270874 val loss 1.08819580078125\n",
      "Epoch 6019: train loss 1.08819580078125 val loss 1.0652309656143188\n",
      "Epoch 6020: train loss 1.0652309656143188 val loss 1.0653645992279053\n",
      "Epoch 6021: train loss 1.0653645992279053 val loss 1.0801010131835938\n",
      "Epoch 6022: train loss 1.0801010131835938 val loss 1.0624955892562866\n",
      "Epoch 6023: train loss 1.0624955892562866 val loss 1.070635437965393\n",
      "Epoch 6024: train loss 1.070635437965393 val loss 1.1031413078308105\n",
      "Epoch 6025: train loss 1.1031413078308105 val loss 1.0606956481933594\n",
      "Epoch 6026: train loss 1.0606956481933594 val loss 1.086823582649231\n",
      "Epoch 6027: train loss 1.086823582649231 val loss 1.0771849155426025\n",
      "Epoch 6028: train loss 1.0771849155426025 val loss 1.059409499168396\n",
      "Epoch 6029: train loss 1.059409499168396 val loss 1.0682337284088135\n",
      "Epoch 6030: train loss 1.0682337284088135 val loss 1.0820231437683105\n",
      "Epoch 6031: train loss 1.0820231437683105 val loss 1.0696827173233032\n",
      "Epoch 6032: train loss 1.0696827173233032 val loss 1.0592206716537476\n",
      "Epoch 6033: train loss 1.0592206716537476 val loss 1.082843542098999\n",
      "Epoch 6034: train loss 1.082843542098999 val loss 1.1059901714324951\n",
      "Epoch 6035: train loss 1.1059901714324951 val loss 1.063981533050537\n",
      "Epoch 6036: train loss 1.063981533050537 val loss 1.0588421821594238\n",
      "Epoch 6037: train loss 1.0588421821594238 val loss 1.0590388774871826\n",
      "Epoch 6038: train loss 1.0590388774871826 val loss 1.0578866004943848\n",
      "Epoch 6039: train loss 1.0578866004943848 val loss 1.0585013628005981\n",
      "Epoch 6040: train loss 1.0585013628005981 val loss 1.058661937713623\n",
      "Epoch 6041: train loss 1.058661937713623 val loss 1.058027982711792\n",
      "Epoch 6042: train loss 1.058027982711792 val loss 1.0579075813293457\n",
      "Epoch 6043: train loss 1.0579075813293457 val loss 1.057819128036499\n",
      "Epoch 6044: train loss 1.057819128036499 val loss 1.056839942932129\n",
      "Epoch 6045: train loss 1.056839942932129 val loss 1.0570563077926636\n",
      "Epoch 6046: train loss 1.0570563077926636 val loss 1.057076096534729\n",
      "Epoch 6047: train loss 1.057076096534729 val loss 1.0565438270568848\n",
      "Epoch 6048: train loss 1.0565438270568848 val loss 1.0565102100372314\n",
      "Epoch 6049: train loss 1.0565102100372314 val loss 1.0593770742416382\n",
      "Epoch 6050: train loss 1.0593770742416382 val loss 1.0578995943069458\n",
      "Epoch 6051: train loss 1.0578995943069458 val loss 1.0643763542175293\n",
      "Epoch 6052: train loss 1.0643763542175293 val loss 1.0708260536193848\n",
      "Epoch 6053: train loss 1.0708260536193848 val loss 1.0702612400054932\n",
      "Epoch 6054: train loss 1.0702612400054932 val loss 1.0582493543624878\n",
      "Epoch 6055: train loss 1.0582493543624878 val loss 1.0667905807495117\n",
      "Epoch 6056: train loss 1.0667905807495117 val loss 1.10543954372406\n",
      "Epoch 6057: train loss 1.10543954372406 val loss 1.0743664503097534\n",
      "Epoch 6058: train loss 1.0743664503097534 val loss 1.0580110549926758\n",
      "Epoch 6059: train loss 1.0580110549926758 val loss 1.1049892902374268\n",
      "Epoch 6060: train loss 1.1049892902374268 val loss 1.085530400276184\n",
      "Epoch 6061: train loss 1.085530400276184 val loss 1.0558452606201172\n",
      "Epoch 6062: train loss 1.0558452606201172 val loss 1.0617730617523193\n",
      "Epoch 6063: train loss 1.0617730617523193 val loss 1.0563249588012695\n",
      "Epoch 6064: train loss 1.0563249588012695 val loss 1.066911220550537\n",
      "Epoch 6065: train loss 1.066911220550537 val loss 1.0969233512878418\n",
      "Epoch 6066: train loss 1.0969233512878418 val loss 1.0670685768127441\n",
      "Epoch 6067: train loss 1.0670685768127441 val loss 1.0556236505508423\n",
      "Epoch 6068: train loss 1.0556236505508423 val loss 1.057686686515808\n",
      "Epoch 6069: train loss 1.057686686515808 val loss 1.0560575723648071\n",
      "Epoch 6070: train loss 1.0560575723648071 val loss 1.056530475616455\n",
      "Epoch 6071: train loss 1.056530475616455 val loss 1.0631487369537354\n",
      "Epoch 6072: train loss 1.0631487369537354 val loss 1.0756891965866089\n",
      "Epoch 6073: train loss 1.0756891965866089 val loss 1.0573488473892212\n",
      "Epoch 6074: train loss 1.0573488473892212 val loss 1.0672496557235718\n",
      "Epoch 6075: train loss 1.0672496557235718 val loss 1.0961384773254395\n",
      "Epoch 6076: train loss 1.0961384773254395 val loss 1.0565227270126343\n",
      "Epoch 6077: train loss 1.0565227270126343 val loss 1.089665412902832\n",
      "Epoch 6078: train loss 1.089665412902832 val loss 1.1187115907669067\n",
      "Epoch 6079: train loss 1.1187115907669067 val loss 1.0552773475646973\n",
      "Epoch 6080: train loss 1.0552773475646973 val loss 1.0959293842315674\n",
      "Epoch 6081: train loss 1.0959293842315674 val loss 1.107808232307434\n",
      "Epoch 6082: train loss 1.107808232307434 val loss 1.0543608665466309\n",
      "Epoch 6083: train loss 1.0543608665466309 val loss 1.0895428657531738\n",
      "Epoch 6084: train loss 1.0895428657531738 val loss 1.0895159244537354\n",
      "Epoch 6085: train loss 1.0895159244537354 val loss 1.0554203987121582\n",
      "Epoch 6086: train loss 1.0554203987121582 val loss 1.0829838514328003\n",
      "Epoch 6087: train loss 1.0829838514328003 val loss 1.0535168647766113\n",
      "Epoch 6088: train loss 1.0535168647766113 val loss 1.088505744934082\n",
      "Epoch 6089: train loss 1.088505744934082 val loss 1.0787572860717773\n",
      "Epoch 6090: train loss 1.0787572860717773 val loss 1.0544381141662598\n",
      "Epoch 6091: train loss 1.0544381141662598 val loss 1.075373888015747\n",
      "Epoch 6092: train loss 1.075373888015747 val loss 1.0869098901748657\n",
      "Epoch 6093: train loss 1.0869098901748657 val loss 1.066429853439331\n",
      "Epoch 6094: train loss 1.066429853439331 val loss 1.053404450416565\n",
      "Epoch 6095: train loss 1.053404450416565 val loss 1.0578185319900513\n",
      "Epoch 6096: train loss 1.0578185319900513 val loss 1.0570741891860962\n",
      "Epoch 6097: train loss 1.0570741891860962 val loss 1.067995309829712\n",
      "Epoch 6098: train loss 1.067995309829712 val loss 1.0575705766677856\n",
      "Epoch 6099: train loss 1.0575705766677856 val loss 1.0566675662994385\n",
      "Epoch 6100: train loss 1.0566675662994385 val loss 1.0525813102722168\n",
      "Epoch 6101: train loss 1.0525813102722168 val loss 1.0545928478240967\n",
      "Epoch 6102: train loss 1.0545928478240967 val loss 1.0632649660110474\n",
      "Epoch 6103: train loss 1.0632649660110474 val loss 1.0685440301895142\n",
      "Epoch 6104: train loss 1.0685440301895142 val loss 1.0559552907943726\n",
      "Epoch 6105: train loss 1.0559552907943726 val loss 1.0760780572891235\n",
      "Epoch 6106: train loss 1.0760780572891235 val loss 1.1490442752838135\n",
      "Epoch 6107: train loss 1.1490442752838135 val loss 1.0547103881835938\n",
      "Epoch 6108: train loss 1.0547103881835938 val loss 1.1460484266281128\n",
      "Epoch 6109: train loss 1.1460484266281128 val loss 1.1489131450653076\n",
      "Epoch 6110: train loss 1.1489131450653076 val loss 1.060347080230713\n",
      "Epoch 6111: train loss 1.060347080230713 val loss 1.2434780597686768\n",
      "Epoch 6112: train loss 1.2434780597686768 val loss 1.069894552230835\n",
      "Epoch 6113: train loss 1.069894552230835 val loss 1.0862058401107788\n",
      "Epoch 6114: train loss 1.0862058401107788 val loss 1.1111952066421509\n",
      "Epoch 6115: train loss 1.1111952066421509 val loss 1.0527887344360352\n",
      "Epoch 6116: train loss 1.0527887344360352 val loss 1.1193791627883911\n",
      "Epoch 6117: train loss 1.1193791627883911 val loss 1.1353849172592163\n",
      "Epoch 6118: train loss 1.1353849172592163 val loss 1.0537006855010986\n",
      "Epoch 6119: train loss 1.0537006855010986 val loss 1.0993582010269165\n",
      "Epoch 6120: train loss 1.0993582010269165 val loss 1.0670843124389648\n",
      "Epoch 6121: train loss 1.0670843124389648 val loss 1.056338906288147\n",
      "Epoch 6122: train loss 1.056338906288147 val loss 1.0910179615020752\n",
      "Epoch 6123: train loss 1.0910179615020752 val loss 1.0527063608169556\n",
      "Epoch 6124: train loss 1.0527063608169556 val loss 1.0843104124069214\n",
      "Epoch 6125: train loss 1.0843104124069214 val loss 1.1096439361572266\n",
      "Epoch 6126: train loss 1.1096439361572266 val loss 1.0519124269485474\n",
      "Epoch 6127: train loss 1.0519124269485474 val loss 1.1054863929748535\n",
      "Epoch 6128: train loss 1.1054863929748535 val loss 1.143526315689087\n",
      "Epoch 6129: train loss 1.143526315689087 val loss 1.0501923561096191\n",
      "Epoch 6130: train loss 1.0501923561096191 val loss 1.1040728092193604\n",
      "Epoch 6131: train loss 1.1040728092193604 val loss 1.1130118370056152\n",
      "Epoch 6132: train loss 1.1130118370056152 val loss 1.0514087677001953\n",
      "Epoch 6133: train loss 1.0514087677001953 val loss 1.098392367362976\n",
      "Epoch 6134: train loss 1.098392367362976 val loss 1.064795732498169\n",
      "Epoch 6135: train loss 1.064795732498169 val loss 1.054121971130371\n",
      "Epoch 6136: train loss 1.054121971130371 val loss 1.1203980445861816\n",
      "Epoch 6137: train loss 1.1203980445861816 val loss 1.0636098384857178\n",
      "Epoch 6138: train loss 1.0636098384857178 val loss 1.0569274425506592\n",
      "Epoch 6139: train loss 1.0569274425506592 val loss 1.1238698959350586\n",
      "Epoch 6140: train loss 1.1238698959350586 val loss 1.0514285564422607\n",
      "Epoch 6141: train loss 1.0514285564422607 val loss 1.133023738861084\n",
      "Epoch 6142: train loss 1.133023738861084 val loss 1.103888988494873\n",
      "Epoch 6143: train loss 1.103888988494873 val loss 1.0624220371246338\n",
      "Epoch 6144: train loss 1.0624220371246338 val loss 1.1887691020965576\n",
      "Epoch 6145: train loss 1.1887691020965576 val loss 1.0541462898254395\n",
      "Epoch 6146: train loss 1.0541462898254395 val loss 1.214911699295044\n",
      "Epoch 6147: train loss 1.214911699295044 val loss 1.1285314559936523\n",
      "Epoch 6148: train loss 1.1285314559936523 val loss 1.07582688331604\n",
      "Epoch 6149: train loss 1.07582688331604 val loss 1.2271196842193604\n",
      "Epoch 6150: train loss 1.2271196842193604 val loss 1.0527923107147217\n",
      "Epoch 6151: train loss 1.0527923107147217 val loss 1.0958255529403687\n",
      "Epoch 6152: train loss 1.0958255529403687 val loss 1.0590626001358032\n",
      "Epoch 6153: train loss 1.0590626001358032 val loss 1.0592155456542969\n",
      "Epoch 6154: train loss 1.0592155456542969 val loss 1.085695505142212\n",
      "Epoch 6155: train loss 1.085695505142212 val loss 1.0502641201019287\n",
      "Epoch 6156: train loss 1.0502641201019287 val loss 1.0720486640930176\n",
      "Epoch 6157: train loss 1.0720486640930176 val loss 1.066412329673767\n",
      "Epoch 6158: train loss 1.066412329673767 val loss 1.048858642578125\n",
      "Epoch 6159: train loss 1.048858642578125 val loss 1.056754469871521\n",
      "Epoch 6160: train loss 1.056754469871521 val loss 1.0653965473175049\n",
      "Epoch 6161: train loss 1.0653965473175049 val loss 1.0509555339813232\n",
      "Epoch 6162: train loss 1.0509555339813232 val loss 1.05068039894104\n",
      "Epoch 6163: train loss 1.05068039894104 val loss 1.0495986938476562\n",
      "Epoch 6164: train loss 1.0495986938476562 val loss 1.0507408380508423\n",
      "Epoch 6165: train loss 1.0507408380508423 val loss 1.0505931377410889\n",
      "Epoch 6166: train loss 1.0505931377410889 val loss 1.051353931427002\n",
      "Epoch 6167: train loss 1.051353931427002 val loss 1.063215732574463\n",
      "Epoch 6168: train loss 1.063215732574463 val loss 1.0591511726379395\n",
      "Epoch 6169: train loss 1.0591511726379395 val loss 1.048525094985962\n",
      "Epoch 6170: train loss 1.048525094985962 val loss 1.0601372718811035\n",
      "Epoch 6171: train loss 1.0601372718811035 val loss 1.0814356803894043\n",
      "Epoch 6172: train loss 1.0814356803894043 val loss 1.0583598613739014\n",
      "Epoch 6173: train loss 1.0583598613739014 val loss 1.0475316047668457\n",
      "Epoch 6174: train loss 1.0475316047668457 val loss 1.0527927875518799\n",
      "Epoch 6175: train loss 1.0527927875518799 val loss 1.0484046936035156\n",
      "Epoch 6176: train loss 1.0484046936035156 val loss 1.0586668252944946\n",
      "Epoch 6177: train loss 1.0586668252944946 val loss 1.092626929283142\n",
      "Epoch 6178: train loss 1.092626929283142 val loss 1.0570482015609741\n",
      "Epoch 6179: train loss 1.0570482015609741 val loss 1.0460608005523682\n",
      "Epoch 6180: train loss 1.0460608005523682 val loss 1.0531131029129028\n",
      "Epoch 6181: train loss 1.0531131029129028 val loss 1.072974443435669\n",
      "Epoch 6182: train loss 1.072974443435669 val loss 1.0539034605026245\n",
      "Epoch 6183: train loss 1.0539034605026245 val loss 1.0499110221862793\n",
      "Epoch 6184: train loss 1.0499110221862793 val loss 1.0505614280700684\n",
      "Epoch 6185: train loss 1.0505614280700684 val loss 1.0502190589904785\n",
      "Epoch 6186: train loss 1.0502190589904785 val loss 1.0531883239746094\n",
      "Epoch 6187: train loss 1.0531883239746094 val loss 1.0740113258361816\n",
      "Epoch 6188: train loss 1.0740113258361816 val loss 1.0525233745574951\n",
      "Epoch 6189: train loss 1.0525233745574951 val loss 1.0486950874328613\n",
      "Epoch 6190: train loss 1.0486950874328613 val loss 1.048250675201416\n",
      "Epoch 6191: train loss 1.048250675201416 val loss 1.0474189519882202\n",
      "Epoch 6192: train loss 1.0474189519882202 val loss 1.046558141708374\n",
      "Epoch 6193: train loss 1.046558141708374 val loss 1.0467214584350586\n",
      "Epoch 6194: train loss 1.0467214584350586 val loss 1.0478882789611816\n",
      "Epoch 6195: train loss 1.0478882789611816 val loss 1.0471596717834473\n",
      "Epoch 6196: train loss 1.0471596717834473 val loss 1.0470895767211914\n",
      "Epoch 6197: train loss 1.0470895767211914 val loss 1.0459492206573486\n",
      "Epoch 6198: train loss 1.0459492206573486 val loss 1.0454182624816895\n",
      "Epoch 6199: train loss 1.0454182624816895 val loss 1.0455435514450073\n",
      "Epoch 6200: train loss 1.0455435514450073 val loss 1.0449422597885132\n",
      "Epoch 6201: train loss 1.0449422597885132 val loss 1.0449855327606201\n",
      "Epoch 6202: train loss 1.0449855327606201 val loss 1.045137882232666\n",
      "Epoch 6203: train loss 1.045137882232666 val loss 1.0472357273101807\n",
      "Epoch 6204: train loss 1.0472357273101807 val loss 1.0453989505767822\n",
      "Epoch 6205: train loss 1.0453989505767822 val loss 1.0455021858215332\n",
      "Epoch 6206: train loss 1.0455021858215332 val loss 1.0454978942871094\n",
      "Epoch 6207: train loss 1.0454978942871094 val loss 1.044253945350647\n",
      "Epoch 6208: train loss 1.044253945350647 val loss 1.0440559387207031\n",
      "Epoch 6209: train loss 1.0440559387207031 val loss 1.0444622039794922\n",
      "Epoch 6210: train loss 1.0444622039794922 val loss 1.0431973934173584\n",
      "Epoch 6211: train loss 1.0431973934173584 val loss 1.0430400371551514\n",
      "Epoch 6212: train loss 1.0430400371551514 val loss 1.0429260730743408\n",
      "Epoch 6213: train loss 1.0429260730743408 val loss 1.0430152416229248\n",
      "Epoch 6214: train loss 1.0430152416229248 val loss 1.048083782196045\n",
      "Epoch 6215: train loss 1.048083782196045 val loss 1.0669593811035156\n",
      "Epoch 6216: train loss 1.0669593811035156 val loss 1.0460572242736816\n",
      "Epoch 6217: train loss 1.0460572242736816 val loss 1.0460823774337769\n",
      "Epoch 6218: train loss 1.0460823774337769 val loss 1.0569353103637695\n",
      "Epoch 6219: train loss 1.0569353103637695 val loss 1.0555647611618042\n",
      "Epoch 6220: train loss 1.0555647611618042 val loss 1.0498214960098267\n",
      "Epoch 6221: train loss 1.0498214960098267 val loss 1.0485111474990845\n",
      "Epoch 6222: train loss 1.0485111474990845 val loss 1.0455443859100342\n",
      "Epoch 6223: train loss 1.0455443859100342 val loss 1.055628776550293\n",
      "Epoch 6224: train loss 1.055628776550293 val loss 1.0971113443374634\n",
      "Epoch 6225: train loss 1.0971113443374634 val loss 1.043687343597412\n",
      "Epoch 6226: train loss 1.043687343597412 val loss 1.0848400592803955\n",
      "Epoch 6227: train loss 1.0848400592803955 val loss 1.1288697719573975\n",
      "Epoch 6228: train loss 1.1288697719573975 val loss 1.0441471338272095\n",
      "Epoch 6229: train loss 1.0441471338272095 val loss 1.1304080486297607\n",
      "Epoch 6230: train loss 1.1304080486297607 val loss 1.103774905204773\n",
      "Epoch 6231: train loss 1.103774905204773 val loss 1.0487123727798462\n",
      "Epoch 6232: train loss 1.0487123727798462 val loss 1.1953997611999512\n",
      "Epoch 6233: train loss 1.1953997611999512 val loss 1.0917692184448242\n",
      "Epoch 6234: train loss 1.0917692184448242 val loss 1.1319469213485718\n",
      "Epoch 6235: train loss 1.1319469213485718 val loss 1.1830651760101318\n",
      "Epoch 6236: train loss 1.1830651760101318 val loss 1.0456018447875977\n",
      "Epoch 6237: train loss 1.0456018447875977 val loss 1.1718275547027588\n",
      "Epoch 6238: train loss 1.1718275547027588 val loss 1.0761350393295288\n",
      "Epoch 6239: train loss 1.0761350393295288 val loss 1.0667064189910889\n",
      "Epoch 6240: train loss 1.0667064189910889 val loss 1.1799027919769287\n",
      "Epoch 6241: train loss 1.1799027919769287 val loss 1.044739007949829\n",
      "Epoch 6242: train loss 1.044739007949829 val loss 1.1613764762878418\n",
      "Epoch 6243: train loss 1.1613764762878418 val loss 1.1546032428741455\n",
      "Epoch 6244: train loss 1.1546032428741455 val loss 1.059420108795166\n",
      "Epoch 6245: train loss 1.059420108795166 val loss 1.2768018245697021\n",
      "Epoch 6246: train loss 1.2768018245697021 val loss 1.0459085702896118\n",
      "Epoch 6247: train loss 1.0459085702896118 val loss 1.194664716720581\n",
      "Epoch 6248: train loss 1.194664716720581 val loss 1.0453286170959473\n",
      "Epoch 6249: train loss 1.0453286170959473 val loss 1.213059663772583\n",
      "Epoch 6250: train loss 1.213059663772583 val loss 1.0952173471450806\n",
      "Epoch 6251: train loss 1.0952173471450806 val loss 1.119402527809143\n",
      "Epoch 6252: train loss 1.119402527809143 val loss 1.2013968229293823\n",
      "Epoch 6253: train loss 1.2013968229293823 val loss 1.0444951057434082\n",
      "Epoch 6254: train loss 1.0444951057434082 val loss 1.1396245956420898\n",
      "Epoch 6255: train loss 1.1396245956420898 val loss 1.0470774173736572\n",
      "Epoch 6256: train loss 1.0470774173736572 val loss 1.084338903427124\n",
      "Epoch 6257: train loss 1.084338903427124 val loss 1.088581919670105\n",
      "Epoch 6258: train loss 1.088581919670105 val loss 1.0661110877990723\n",
      "Epoch 6259: train loss 1.0661110877990723 val loss 1.1667730808258057\n",
      "Epoch 6260: train loss 1.1667730808258057 val loss 1.0672330856323242\n",
      "Epoch 6261: train loss 1.0672330856323242 val loss 1.0729119777679443\n",
      "Epoch 6262: train loss 1.0729119777679443 val loss 1.1094551086425781\n",
      "Epoch 6263: train loss 1.1094551086425781 val loss 1.0415229797363281\n",
      "Epoch 6264: train loss 1.0415229797363281 val loss 1.098753809928894\n",
      "Epoch 6265: train loss 1.098753809928894 val loss 1.0488841533660889\n",
      "Epoch 6266: train loss 1.0488841533660889 val loss 1.0527448654174805\n",
      "Epoch 6267: train loss 1.0527448654174805 val loss 1.0759778022766113\n",
      "Epoch 6268: train loss 1.0759778022766113 val loss 1.0419719219207764\n",
      "Epoch 6269: train loss 1.0419719219207764 val loss 1.0779725313186646\n",
      "Epoch 6270: train loss 1.0779725313186646 val loss 1.0658371448516846\n",
      "Epoch 6271: train loss 1.0658371448516846 val loss 1.0420178174972534\n",
      "Epoch 6272: train loss 1.0420178174972534 val loss 1.0555469989776611\n",
      "Epoch 6273: train loss 1.0555469989776611 val loss 1.0618562698364258\n",
      "Epoch 6274: train loss 1.0618562698364258 val loss 1.0425641536712646\n",
      "Epoch 6275: train loss 1.0425641536712646 val loss 1.0468299388885498\n",
      "Epoch 6276: train loss 1.0468299388885498 val loss 1.0834050178527832\n",
      "Epoch 6277: train loss 1.0834050178527832 val loss 1.040148377418518\n",
      "Epoch 6278: train loss 1.040148377418518 val loss 1.077478289604187\n",
      "Epoch 6279: train loss 1.077478289604187 val loss 1.0584062337875366\n",
      "Epoch 6280: train loss 1.0584062337875366 val loss 1.0416553020477295\n",
      "Epoch 6281: train loss 1.0416553020477295 val loss 1.0601766109466553\n",
      "Epoch 6282: train loss 1.0601766109466553 val loss 1.0454115867614746\n",
      "Epoch 6283: train loss 1.0454115867614746 val loss 1.0414328575134277\n",
      "Epoch 6284: train loss 1.0414328575134277 val loss 1.046814203262329\n",
      "Epoch 6285: train loss 1.046814203262329 val loss 1.0895836353302002\n",
      "Epoch 6286: train loss 1.0895836353302002 val loss 1.0408529043197632\n",
      "Epoch 6287: train loss 1.0408529043197632 val loss 1.0810956954956055\n",
      "Epoch 6288: train loss 1.0810956954956055 val loss 1.0707745552062988\n",
      "Epoch 6289: train loss 1.0707745552062988 val loss 1.0424174070358276\n",
      "Epoch 6290: train loss 1.0424174070358276 val loss 1.0611588954925537\n",
      "Epoch 6291: train loss 1.0611588954925537 val loss 1.0732160806655884\n",
      "Epoch 6292: train loss 1.0732160806655884 val loss 1.0400822162628174\n",
      "Epoch 6293: train loss 1.0400822162628174 val loss 1.0654913187026978\n",
      "Epoch 6294: train loss 1.0654913187026978 val loss 1.0549325942993164\n",
      "Epoch 6295: train loss 1.0549325942993164 val loss 1.041480302810669\n",
      "Epoch 6296: train loss 1.041480302810669 val loss 1.0578465461730957\n",
      "Epoch 6297: train loss 1.0578465461730957 val loss 1.053054690361023\n",
      "Epoch 6298: train loss 1.053054690361023 val loss 1.0394104719161987\n",
      "Epoch 6299: train loss 1.0394104719161987 val loss 1.049180269241333\n",
      "Epoch 6300: train loss 1.049180269241333 val loss 1.0860956907272339\n",
      "Epoch 6301: train loss 1.0860956907272339 val loss 1.0405402183532715\n",
      "Epoch 6302: train loss 1.0405402183532715 val loss 1.0823614597320557\n",
      "Epoch 6303: train loss 1.0823614597320557 val loss 1.0683557987213135\n",
      "Epoch 6304: train loss 1.0683557987213135 val loss 1.0409140586853027\n",
      "Epoch 6305: train loss 1.0409140586853027 val loss 1.0642528533935547\n",
      "Epoch 6306: train loss 1.0642528533935547 val loss 1.0683434009552002\n",
      "Epoch 6307: train loss 1.0683434009552002 val loss 1.0397828817367554\n",
      "Epoch 6308: train loss 1.0397828817367554 val loss 1.0586543083190918\n",
      "Epoch 6309: train loss 1.0586543083190918 val loss 1.0569970607757568\n",
      "Epoch 6310: train loss 1.0569970607757568 val loss 1.0389244556427002\n",
      "Epoch 6311: train loss 1.0389244556427002 val loss 1.0435136556625366\n",
      "Epoch 6312: train loss 1.0435136556625366 val loss 1.0405197143554688\n",
      "Epoch 6313: train loss 1.0405197143554688 val loss 1.0470736026763916\n",
      "Epoch 6314: train loss 1.0470736026763916 val loss 1.0840822458267212\n",
      "Epoch 6315: train loss 1.0840822458267212 val loss 1.043607473373413\n",
      "Epoch 6316: train loss 1.043607473373413 val loss 1.042611837387085\n",
      "Epoch 6317: train loss 1.042611837387085 val loss 1.0406501293182373\n",
      "Epoch 6318: train loss 1.0406501293182373 val loss 1.0445680618286133\n",
      "Epoch 6319: train loss 1.0445680618286133 val loss 1.1103837490081787\n",
      "Epoch 6320: train loss 1.1103837490081787 val loss 1.0385470390319824\n",
      "Epoch 6321: train loss 1.0385470390319824 val loss 1.1017329692840576\n",
      "Epoch 6322: train loss 1.1017329692840576 val loss 1.0848851203918457\n",
      "Epoch 6323: train loss 1.0848851203918457 val loss 1.0752315521240234\n",
      "Epoch 6324: train loss 1.0752315521240234 val loss 1.1243906021118164\n",
      "Epoch 6325: train loss 1.1243906021118164 val loss 1.0362905263900757\n",
      "Epoch 6326: train loss 1.0362905263900757 val loss 1.085946798324585\n",
      "Epoch 6327: train loss 1.085946798324585 val loss 1.0463101863861084\n",
      "Epoch 6328: train loss 1.0463101863861084 val loss 1.03867506980896\n",
      "Epoch 6329: train loss 1.03867506980896 val loss 1.041975975036621\n",
      "Epoch 6330: train loss 1.041975975036621 val loss 1.0376619100570679\n",
      "Epoch 6331: train loss 1.0376619100570679 val loss 1.0406748056411743\n",
      "Epoch 6332: train loss 1.0406748056411743 val loss 1.0377006530761719\n",
      "Epoch 6333: train loss 1.0377006530761719 val loss 1.038161039352417\n",
      "Epoch 6334: train loss 1.038161039352417 val loss 1.0360904932022095\n",
      "Epoch 6335: train loss 1.0360904932022095 val loss 1.0426344871520996\n",
      "Epoch 6336: train loss 1.0426344871520996 val loss 1.0606071949005127\n",
      "Epoch 6337: train loss 1.0606071949005127 val loss 1.0493332147598267\n",
      "Epoch 6338: train loss 1.0493332147598267 val loss 1.0347824096679688\n",
      "Epoch 6339: train loss 1.0347824096679688 val loss 1.044190764427185\n",
      "Epoch 6340: train loss 1.044190764427185 val loss 1.0571088790893555\n",
      "Epoch 6341: train loss 1.0571088790893555 val loss 1.03762686252594\n",
      "Epoch 6342: train loss 1.03762686252594 val loss 1.040099024772644\n",
      "Epoch 6343: train loss 1.040099024772644 val loss 1.0411392450332642\n",
      "Epoch 6344: train loss 1.0411392450332642 val loss 1.0377318859100342\n",
      "Epoch 6345: train loss 1.0377318859100342 val loss 1.0407373905181885\n",
      "Epoch 6346: train loss 1.0407373905181885 val loss 1.0687443017959595\n",
      "Epoch 6347: train loss 1.0687443017959595 val loss 1.0482017993927002\n",
      "Epoch 6348: train loss 1.0482017993927002 val loss 1.0381097793579102\n",
      "Epoch 6349: train loss 1.0381097793579102 val loss 1.0358586311340332\n",
      "Epoch 6350: train loss 1.0358586311340332 val loss 1.0371432304382324\n",
      "Epoch 6351: train loss 1.0371432304382324 val loss 1.0361793041229248\n",
      "Epoch 6352: train loss 1.0361793041229248 val loss 1.0365018844604492\n",
      "Epoch 6353: train loss 1.0365018844604492 val loss 1.0346784591674805\n",
      "Epoch 6354: train loss 1.0346784591674805 val loss 1.0346927642822266\n",
      "Epoch 6355: train loss 1.0346927642822266 val loss 1.0341298580169678\n",
      "Epoch 6356: train loss 1.0341298580169678 val loss 1.039616346359253\n",
      "Epoch 6357: train loss 1.039616346359253 val loss 1.070601463317871\n",
      "Epoch 6358: train loss 1.070601463317871 val loss 1.0339393615722656\n",
      "Epoch 6359: train loss 1.0339393615722656 val loss 1.0764095783233643\n",
      "Epoch 6360: train loss 1.0764095783233643 val loss 1.1148362159729004\n",
      "Epoch 6361: train loss 1.1148362159729004 val loss 1.0342490673065186\n",
      "Epoch 6362: train loss 1.0342490673065186 val loss 1.1220004558563232\n",
      "Epoch 6363: train loss 1.1220004558563232 val loss 1.0593677759170532\n",
      "Epoch 6364: train loss 1.0593677759170532 val loss 1.0433104038238525\n",
      "Epoch 6365: train loss 1.0433104038238525 val loss 1.109991431236267\n",
      "Epoch 6366: train loss 1.109991431236267 val loss 1.0392612218856812\n",
      "Epoch 6367: train loss 1.0392612218856812 val loss 1.083957314491272\n",
      "Epoch 6368: train loss 1.083957314491272 val loss 1.0634839534759521\n",
      "Epoch 6369: train loss 1.0634839534759521 val loss 1.035987138748169\n",
      "Epoch 6370: train loss 1.035987138748169 val loss 1.0569655895233154\n",
      "Epoch 6371: train loss 1.0569655895233154 val loss 1.0694546699523926\n",
      "Epoch 6372: train loss 1.0694546699523926 val loss 1.0363749265670776\n",
      "Epoch 6373: train loss 1.0363749265670776 val loss 1.0452895164489746\n",
      "Epoch 6374: train loss 1.0452895164489746 val loss 1.0725688934326172\n",
      "Epoch 6375: train loss 1.0725688934326172 val loss 1.038327932357788\n",
      "Epoch 6376: train loss 1.038327932357788 val loss 1.048160195350647\n",
      "Epoch 6377: train loss 1.048160195350647 val loss 1.10667085647583\n",
      "Epoch 6378: train loss 1.10667085647583 val loss 1.0353882312774658\n",
      "Epoch 6379: train loss 1.0353882312774658 val loss 1.1055684089660645\n",
      "Epoch 6380: train loss 1.1055684089660645 val loss 1.0573689937591553\n",
      "Epoch 6381: train loss 1.0573689937591553 val loss 1.0348107814788818\n",
      "Epoch 6382: train loss 1.0348107814788818 val loss 1.0508499145507812\n",
      "Epoch 6383: train loss 1.0508499145507812 val loss 1.0678199529647827\n",
      "Epoch 6384: train loss 1.0678199529647827 val loss 1.0360299348831177\n",
      "Epoch 6385: train loss 1.0360299348831177 val loss 1.0619890689849854\n",
      "Epoch 6386: train loss 1.0619890689849854 val loss 1.0795574188232422\n",
      "Epoch 6387: train loss 1.0795574188232422 val loss 1.038811445236206\n",
      "Epoch 6388: train loss 1.038811445236206 val loss 1.0332276821136475\n",
      "Epoch 6389: train loss 1.0332276821136475 val loss 1.0350511074066162\n",
      "Epoch 6390: train loss 1.0350511074066162 val loss 1.0348016023635864\n",
      "Epoch 6391: train loss 1.0348016023635864 val loss 1.033674955368042\n",
      "Epoch 6392: train loss 1.033674955368042 val loss 1.0371654033660889\n",
      "Epoch 6393: train loss 1.0371654033660889 val loss 1.0821150541305542\n",
      "Epoch 6394: train loss 1.0821150541305542 val loss 1.0625855922698975\n",
      "Epoch 6395: train loss 1.0625855922698975 val loss 1.0349419116973877\n",
      "Epoch 6396: train loss 1.0349419116973877 val loss 1.0443484783172607\n",
      "Epoch 6397: train loss 1.0443484783172607 val loss 1.0490758419036865\n",
      "Epoch 6398: train loss 1.0490758419036865 val loss 1.0428001880645752\n",
      "Epoch 6399: train loss 1.0428001880645752 val loss 1.0358628034591675\n",
      "Epoch 6400: train loss 1.0358628034591675 val loss 1.0337377786636353\n",
      "Epoch 6401: train loss 1.0337377786636353 val loss 1.044258713722229\n",
      "Epoch 6402: train loss 1.044258713722229 val loss 1.0483777523040771\n",
      "Epoch 6403: train loss 1.0483777523040771 val loss 1.0426232814788818\n",
      "Epoch 6404: train loss 1.0426232814788818 val loss 1.0352444648742676\n",
      "Epoch 6405: train loss 1.0352444648742676 val loss 1.033243179321289\n",
      "Epoch 6406: train loss 1.033243179321289 val loss 1.0352622270584106\n",
      "Epoch 6407: train loss 1.0352622270584106 val loss 1.0327670574188232\n",
      "Epoch 6408: train loss 1.0327670574188232 val loss 1.043968677520752\n",
      "Epoch 6409: train loss 1.043968677520752 val loss 1.1125181913375854\n",
      "Epoch 6410: train loss 1.1125181913375854 val loss 1.0330486297607422\n",
      "Epoch 6411: train loss 1.0330486297607422 val loss 1.1201797723770142\n",
      "Epoch 6412: train loss 1.1201797723770142 val loss 1.0755860805511475\n",
      "Epoch 6413: train loss 1.0755860805511475 val loss 1.0426994562149048\n",
      "Epoch 6414: train loss 1.0426994562149048 val loss 1.1525276899337769\n",
      "Epoch 6415: train loss 1.1525276899337769 val loss 1.0319936275482178\n",
      "Epoch 6416: train loss 1.0319936275482178 val loss 1.1463217735290527\n",
      "Epoch 6417: train loss 1.1463217735290527 val loss 1.120901107788086\n",
      "Epoch 6418: train loss 1.120901107788086 val loss 1.0602574348449707\n",
      "Epoch 6419: train loss 1.0602574348449707 val loss 1.2018494606018066\n",
      "Epoch 6420: train loss 1.2018494606018066 val loss 1.0318989753723145\n",
      "Epoch 6421: train loss 1.0318989753723145 val loss 1.192130446434021\n",
      "Epoch 6422: train loss 1.192130446434021 val loss 1.03340482711792\n",
      "Epoch 6423: train loss 1.03340482711792 val loss 1.204763412475586\n",
      "Epoch 6424: train loss 1.204763412475586 val loss 1.0825159549713135\n",
      "Epoch 6425: train loss 1.0825159549713135 val loss 1.1182746887207031\n",
      "Epoch 6426: train loss 1.1182746887207031 val loss 1.1495753526687622\n",
      "Epoch 6427: train loss 1.1495753526687622 val loss 1.030139684677124\n",
      "Epoch 6428: train loss 1.030139684677124 val loss 1.1132876873016357\n",
      "Epoch 6429: train loss 1.1132876873016357 val loss 1.05750572681427\n",
      "Epoch 6430: train loss 1.05750572681427 val loss 1.0341054201126099\n",
      "Epoch 6431: train loss 1.0341054201126099 val loss 1.099003553390503\n",
      "Epoch 6432: train loss 1.099003553390503 val loss 1.0303473472595215\n",
      "Epoch 6433: train loss 1.0303473472595215 val loss 1.1082384586334229\n",
      "Epoch 6434: train loss 1.1082384586334229 val loss 1.0594127178192139\n",
      "Epoch 6435: train loss 1.0594127178192139 val loss 1.0458834171295166\n",
      "Epoch 6436: train loss 1.0458834171295166 val loss 1.1332613229751587\n",
      "Epoch 6437: train loss 1.1332613229751587 val loss 1.057084560394287\n",
      "Epoch 6438: train loss 1.057084560394287 val loss 1.055917501449585\n",
      "Epoch 6439: train loss 1.055917501449585 val loss 1.109248161315918\n",
      "Epoch 6440: train loss 1.109248161315918 val loss 1.0318920612335205\n",
      "Epoch 6441: train loss 1.0318920612335205 val loss 1.0953094959259033\n",
      "Epoch 6442: train loss 1.0953094959259033 val loss 1.0454463958740234\n",
      "Epoch 6443: train loss 1.0454463958740234 val loss 1.0301754474639893\n",
      "Epoch 6444: train loss 1.0301754474639893 val loss 1.0405688285827637\n",
      "Epoch 6445: train loss 1.0405688285827637 val loss 1.0390548706054688\n",
      "Epoch 6446: train loss 1.0390548706054688 val loss 1.0293407440185547\n",
      "Epoch 6447: train loss 1.0293407440185547 val loss 1.0347769260406494\n",
      "Epoch 6448: train loss 1.0347769260406494 val loss 1.0309324264526367\n",
      "Epoch 6449: train loss 1.0309324264526367 val loss 1.0376226902008057\n",
      "Epoch 6450: train loss 1.0376226902008057 val loss 1.0590651035308838\n",
      "Epoch 6451: train loss 1.0590651035308838 val loss 1.030048131942749\n",
      "Epoch 6452: train loss 1.030048131942749 val loss 1.0553590059280396\n",
      "Epoch 6453: train loss 1.0553590059280396 val loss 1.0765535831451416\n",
      "Epoch 6454: train loss 1.0765535831451416 val loss 1.0307179689407349\n",
      "Epoch 6455: train loss 1.0307179689407349 val loss 1.0836477279663086\n",
      "Epoch 6456: train loss 1.0836477279663086 val loss 1.0533971786499023\n",
      "Epoch 6457: train loss 1.0533971786499023 val loss 1.0303475856781006\n",
      "Epoch 6458: train loss 1.0303475856781006 val loss 1.0518794059753418\n",
      "Epoch 6459: train loss 1.0518794059753418 val loss 1.05802583694458\n",
      "Epoch 6460: train loss 1.05802583694458 val loss 1.0278489589691162\n",
      "Epoch 6461: train loss 1.0278489589691162 val loss 1.0526608228683472\n",
      "Epoch 6462: train loss 1.0526608228683472 val loss 1.068375587463379\n",
      "Epoch 6463: train loss 1.068375587463379 val loss 1.0331175327301025\n",
      "Epoch 6464: train loss 1.0331175327301025 val loss 1.0841163396835327\n",
      "Epoch 6465: train loss 1.0841163396835327 val loss 1.044114112854004\n",
      "Epoch 6466: train loss 1.044114112854004 val loss 1.0281028747558594\n",
      "Epoch 6467: train loss 1.0281028747558594 val loss 1.0384306907653809\n",
      "Epoch 6468: train loss 1.0384306907653809 val loss 1.0545878410339355\n",
      "Epoch 6469: train loss 1.0545878410339355 val loss 1.028698205947876\n",
      "Epoch 6470: train loss 1.028698205947876 val loss 1.066936731338501\n",
      "Epoch 6471: train loss 1.066936731338501 val loss 1.065629243850708\n",
      "Epoch 6472: train loss 1.065629243850708 val loss 1.0275105237960815\n",
      "Epoch 6473: train loss 1.0275105237960815 val loss 1.060867190361023\n",
      "Epoch 6474: train loss 1.060867190361023 val loss 1.05906081199646\n",
      "Epoch 6475: train loss 1.05906081199646 val loss 1.0283429622650146\n",
      "Epoch 6476: train loss 1.0283429622650146 val loss 1.0449281930923462\n",
      "Epoch 6477: train loss 1.0449281930923462 val loss 1.035656452178955\n",
      "Epoch 6478: train loss 1.035656452178955 val loss 1.0259664058685303\n",
      "Epoch 6479: train loss 1.0259664058685303 val loss 1.0308469533920288\n",
      "Epoch 6480: train loss 1.0308469533920288 val loss 1.059874415397644\n",
      "Epoch 6481: train loss 1.059874415397644 val loss 1.0268776416778564\n",
      "Epoch 6482: train loss 1.0268776416778564 val loss 1.0601961612701416\n",
      "Epoch 6483: train loss 1.0601961612701416 val loss 1.066054344177246\n",
      "Epoch 6484: train loss 1.066054344177246 val loss 1.0328176021575928\n",
      "Epoch 6485: train loss 1.0328176021575928 val loss 1.1152689456939697\n",
      "Epoch 6486: train loss 1.1152689456939697 val loss 1.0378344058990479\n",
      "Epoch 6487: train loss 1.0378344058990479 val loss 1.0284626483917236\n",
      "Epoch 6488: train loss 1.0284626483917236 val loss 1.0275874137878418\n",
      "Epoch 6489: train loss 1.0275874137878418 val loss 1.0275826454162598\n",
      "Epoch 6490: train loss 1.0275826454162598 val loss 1.0261225700378418\n",
      "Epoch 6491: train loss 1.0261225700378418 val loss 1.0273606777191162\n",
      "Epoch 6492: train loss 1.0273606777191162 val loss 1.0256915092468262\n",
      "Epoch 6493: train loss 1.0256915092468262 val loss 1.0374995470046997\n",
      "Epoch 6494: train loss 1.0374995470046997 val loss 1.0487327575683594\n",
      "Epoch 6495: train loss 1.0487327575683594 val loss 1.0302962064743042\n",
      "Epoch 6496: train loss 1.0302962064743042 val loss 1.0256891250610352\n",
      "Epoch 6497: train loss 1.0256891250610352 val loss 1.029491662979126\n",
      "Epoch 6498: train loss 1.029491662979126 val loss 1.0266964435577393\n",
      "Epoch 6499: train loss 1.0266964435577393 val loss 1.0392404794692993\n",
      "Epoch 6500: train loss 1.0392404794692993 val loss 1.0644030570983887\n",
      "Epoch 6501: train loss 1.0644030570983887 val loss 1.026383638381958\n",
      "Epoch 6502: train loss 1.026383638381958 val loss 1.0861613750457764\n",
      "Epoch 6503: train loss 1.0861613750457764 val loss 1.0727577209472656\n",
      "Epoch 6504: train loss 1.0727577209472656 val loss 1.0407989025115967\n",
      "Epoch 6505: train loss 1.0407989025115967 val loss 1.1837494373321533\n",
      "Epoch 6506: train loss 1.1837494373321533 val loss 1.0321215391159058\n",
      "Epoch 6507: train loss 1.0321215391159058 val loss 1.0724327564239502\n",
      "Epoch 6508: train loss 1.0724327564239502 val loss 1.0582914352416992\n",
      "Epoch 6509: train loss 1.0582914352416992 val loss 1.0248157978057861\n",
      "Epoch 6510: train loss 1.0248157978057861 val loss 1.0489575862884521\n",
      "Epoch 6511: train loss 1.0489575862884521 val loss 1.0419628620147705\n",
      "Epoch 6512: train loss 1.0419628620147705 val loss 1.0241655111312866\n",
      "Epoch 6513: train loss 1.0241655111312866 val loss 1.0313708782196045\n",
      "Epoch 6514: train loss 1.0313708782196045 val loss 1.044719934463501\n",
      "Epoch 6515: train loss 1.044719934463501 val loss 1.050096035003662\n",
      "Epoch 6516: train loss 1.050096035003662 val loss 1.023561716079712\n",
      "Epoch 6517: train loss 1.023561716079712 val loss 1.0431276559829712\n",
      "Epoch 6518: train loss 1.0431276559829712 val loss 1.0506759881973267\n",
      "Epoch 6519: train loss 1.0506759881973267 val loss 1.023600697517395\n",
      "Epoch 6520: train loss 1.023600697517395 val loss 1.0653867721557617\n",
      "Epoch 6521: train loss 1.0653867721557617 val loss 1.0879229307174683\n",
      "Epoch 6522: train loss 1.0879229307174683 val loss 1.0257456302642822\n",
      "Epoch 6523: train loss 1.0257456302642822 val loss 1.1027109622955322\n",
      "Epoch 6524: train loss 1.1027109622955322 val loss 1.028435230255127\n",
      "Epoch 6525: train loss 1.028435230255127 val loss 1.0770702362060547\n",
      "Epoch 6526: train loss 1.0770702362060547 val loss 1.052314281463623\n",
      "Epoch 6527: train loss 1.052314281463623 val loss 1.0251564979553223\n",
      "Epoch 6528: train loss 1.0251564979553223 val loss 1.0361213684082031\n",
      "Epoch 6529: train loss 1.0361213684082031 val loss 1.060418725013733\n",
      "Epoch 6530: train loss 1.060418725013733 val loss 1.0278620719909668\n",
      "Epoch 6531: train loss 1.0278620719909668 val loss 1.0442497730255127\n",
      "Epoch 6532: train loss 1.0442497730255127 val loss 1.0885075330734253\n",
      "Epoch 6533: train loss 1.0885075330734253 val loss 1.0261377096176147\n",
      "Epoch 6534: train loss 1.0261377096176147 val loss 1.0862574577331543\n",
      "Epoch 6535: train loss 1.0862574577331543 val loss 1.0511488914489746\n",
      "Epoch 6536: train loss 1.0511488914489746 val loss 1.0240375995635986\n",
      "Epoch 6537: train loss 1.0240375995635986 val loss 1.0384718179702759\n",
      "Epoch 6538: train loss 1.0384718179702759 val loss 1.0475281476974487\n",
      "Epoch 6539: train loss 1.0475281476974487 val loss 1.033107042312622\n",
      "Epoch 6540: train loss 1.033107042312622 val loss 1.0268933773040771\n",
      "Epoch 6541: train loss 1.0268933773040771 val loss 1.0260123014450073\n",
      "Epoch 6542: train loss 1.0260123014450073 val loss 1.0256808996200562\n",
      "Epoch 6543: train loss 1.0256808996200562 val loss 1.023864984512329\n",
      "Epoch 6544: train loss 1.023864984512329 val loss 1.028735637664795\n",
      "Epoch 6545: train loss 1.028735637664795 val loss 1.066941738128662\n",
      "Epoch 6546: train loss 1.066941738128662 val loss 1.0219320058822632\n",
      "Epoch 6547: train loss 1.0219320058822632 val loss 1.095628023147583\n",
      "Epoch 6548: train loss 1.095628023147583 val loss 1.0526225566864014\n",
      "Epoch 6549: train loss 1.0526225566864014 val loss 1.0342621803283691\n",
      "Epoch 6550: train loss 1.0342621803283691 val loss 1.1651718616485596\n",
      "Epoch 6551: train loss 1.1651718616485596 val loss 1.043195128440857\n",
      "Epoch 6552: train loss 1.043195128440857 val loss 1.0523641109466553\n",
      "Epoch 6553: train loss 1.0523641109466553 val loss 1.0865728855133057\n",
      "Epoch 6554: train loss 1.0865728855133057 val loss 1.023749589920044\n",
      "Epoch 6555: train loss 1.023749589920044 val loss 1.0713701248168945\n",
      "Epoch 6556: train loss 1.0713701248168945 val loss 1.0371615886688232\n",
      "Epoch 6557: train loss 1.0371615886688232 val loss 1.0230464935302734\n",
      "Epoch 6558: train loss 1.0230464935302734 val loss 1.0253093242645264\n",
      "Epoch 6559: train loss 1.0253093242645264 val loss 1.0222476720809937\n",
      "Epoch 6560: train loss 1.0222476720809937 val loss 1.021104335784912\n",
      "Epoch 6561: train loss 1.021104335784912 val loss 1.0204014778137207\n",
      "Epoch 6562: train loss 1.0204014778137207 val loss 1.0207757949829102\n",
      "Epoch 6563: train loss 1.0207757949829102 val loss 1.0200520753860474\n",
      "Epoch 6564: train loss 1.0200520753860474 val loss 1.0311970710754395\n",
      "Epoch 6565: train loss 1.0311970710754395 val loss 1.0539577007293701\n",
      "Epoch 6566: train loss 1.0539577007293701 val loss 1.020642638206482\n",
      "Epoch 6567: train loss 1.020642638206482 val loss 1.0743823051452637\n",
      "Epoch 6568: train loss 1.0743823051452637 val loss 1.0834217071533203\n",
      "Epoch 6569: train loss 1.0834217071533203 val loss 1.0254312753677368\n",
      "Epoch 6570: train loss 1.0254312753677368 val loss 1.0875332355499268\n",
      "Epoch 6571: train loss 1.0875332355499268 val loss 1.0332010984420776\n",
      "Epoch 6572: train loss 1.0332010984420776 val loss 1.0434640645980835\n",
      "Epoch 6573: train loss 1.0434640645980835 val loss 1.0883779525756836\n",
      "Epoch 6574: train loss 1.0883779525756836 val loss 1.0224812030792236\n",
      "Epoch 6575: train loss 1.0224812030792236 val loss 1.0821857452392578\n",
      "Epoch 6576: train loss 1.0821857452392578 val loss 1.0459892749786377\n",
      "Epoch 6577: train loss 1.0459892749786377 val loss 1.0204412937164307\n",
      "Epoch 6578: train loss 1.0204412937164307 val loss 1.026573896408081\n",
      "Epoch 6579: train loss 1.026573896408081 val loss 1.020504355430603\n",
      "Epoch 6580: train loss 1.020504355430603 val loss 1.0330395698547363\n",
      "Epoch 6581: train loss 1.0330395698547363 val loss 1.0497713088989258\n",
      "Epoch 6582: train loss 1.0497713088989258 val loss 1.019956111907959\n",
      "Epoch 6583: train loss 1.019956111907959 val loss 1.0578055381774902\n",
      "Epoch 6584: train loss 1.0578055381774902 val loss 1.0721831321716309\n",
      "Epoch 6585: train loss 1.0721831321716309 val loss 1.0210567712783813\n",
      "Epoch 6586: train loss 1.0210567712783813 val loss 1.066392183303833\n",
      "Epoch 6587: train loss 1.066392183303833 val loss 1.0724279880523682\n",
      "Epoch 6588: train loss 1.0724279880523682 val loss 1.0279324054718018\n",
      "Epoch 6589: train loss 1.0279324054718018 val loss 1.0943970680236816\n",
      "Epoch 6590: train loss 1.0943970680236816 val loss 1.0213971138000488\n",
      "Epoch 6591: train loss 1.0213971138000488 val loss 1.0669541358947754\n",
      "Epoch 6592: train loss 1.0669541358947754 val loss 1.0364747047424316\n",
      "Epoch 6593: train loss 1.0364747047424316 val loss 1.019605040550232\n",
      "Epoch 6594: train loss 1.019605040550232 val loss 1.029742956161499\n",
      "Epoch 6595: train loss 1.029742956161499 val loss 1.0263220071792603\n",
      "Epoch 6596: train loss 1.0263220071792603 val loss 1.0214464664459229\n",
      "Epoch 6597: train loss 1.0214464664459229 val loss 1.0220487117767334\n",
      "Epoch 6598: train loss 1.0220487117767334 val loss 1.0206822156906128\n",
      "Epoch 6599: train loss 1.0206822156906128 val loss 1.0215941667556763\n",
      "Epoch 6600: train loss 1.0215941667556763 val loss 1.020506501197815\n",
      "Epoch 6601: train loss 1.020506501197815 val loss 1.0195395946502686\n",
      "Epoch 6602: train loss 1.0195395946502686 val loss 1.0189098119735718\n",
      "Epoch 6603: train loss 1.0189098119735718 val loss 1.0218660831451416\n",
      "Epoch 6604: train loss 1.0218660831451416 val loss 1.0569651126861572\n",
      "Epoch 6605: train loss 1.0569651126861572 val loss 1.0182366371154785\n",
      "Epoch 6606: train loss 1.0182366371154785 val loss 1.0715274810791016\n",
      "Epoch 6607: train loss 1.0715274810791016 val loss 1.0683025121688843\n",
      "Epoch 6608: train loss 1.0683025121688843 val loss 1.0246937274932861\n",
      "Epoch 6609: train loss 1.0246937274932861 val loss 1.074483871459961\n",
      "Epoch 6610: train loss 1.074483871459961 val loss 1.0389173030853271\n",
      "Epoch 6611: train loss 1.0389173030853271 val loss 1.0196372270584106\n",
      "Epoch 6612: train loss 1.0196372270584106 val loss 1.0271780490875244\n",
      "Epoch 6613: train loss 1.0271780490875244 val loss 1.0522229671478271\n",
      "Epoch 6614: train loss 1.0522229671478271 val loss 1.0190426111221313\n",
      "Epoch 6615: train loss 1.0190426111221313 val loss 1.055065393447876\n",
      "Epoch 6616: train loss 1.055065393447876 val loss 1.051783800125122\n",
      "Epoch 6617: train loss 1.051783800125122 val loss 1.021034836769104\n",
      "Epoch 6618: train loss 1.021034836769104 val loss 1.0524342060089111\n",
      "Epoch 6619: train loss 1.0524342060089111 val loss 1.06965970993042\n",
      "Epoch 6620: train loss 1.06965970993042 val loss 1.018031358718872\n",
      "Epoch 6621: train loss 1.018031358718872 val loss 1.0755877494812012\n",
      "Epoch 6622: train loss 1.0755877494812012 val loss 1.0588188171386719\n",
      "Epoch 6623: train loss 1.0588188171386719 val loss 1.0224874019622803\n",
      "Epoch 6624: train loss 1.0224874019622803 val loss 1.044185757637024\n",
      "Epoch 6625: train loss 1.044185757637024 val loss 1.0376917123794556\n",
      "Epoch 6626: train loss 1.0376917123794556 val loss 1.0177322626113892\n",
      "Epoch 6627: train loss 1.0177322626113892 val loss 1.0331860780715942\n",
      "Epoch 6628: train loss 1.0331860780715942 val loss 1.056136965751648\n",
      "Epoch 6629: train loss 1.056136965751648 val loss 1.0191210508346558\n",
      "Epoch 6630: train loss 1.0191210508346558 val loss 1.072981595993042\n",
      "Epoch 6631: train loss 1.072981595993042 val loss 1.0469372272491455\n",
      "Epoch 6632: train loss 1.0469372272491455 val loss 1.021331787109375\n",
      "Epoch 6633: train loss 1.021331787109375 val loss 1.0521169900894165\n",
      "Epoch 6634: train loss 1.0521169900894165 val loss 1.0440605878829956\n",
      "Epoch 6635: train loss 1.0440605878829956 val loss 1.0188500881195068\n",
      "Epoch 6636: train loss 1.0188500881195068 val loss 1.0381238460540771\n",
      "Epoch 6637: train loss 1.0381238460540771 val loss 1.065887689590454\n",
      "Epoch 6638: train loss 1.065887689590454 val loss 1.0183613300323486\n",
      "Epoch 6639: train loss 1.0183613300323486 val loss 1.063063383102417\n",
      "Epoch 6640: train loss 1.063063383102417 val loss 1.0721790790557861\n",
      "Epoch 6641: train loss 1.0721790790557861 val loss 1.0169243812561035\n",
      "Epoch 6642: train loss 1.0169243812561035 val loss 1.0652252435684204\n",
      "Epoch 6643: train loss 1.0652252435684204 val loss 1.0583703517913818\n",
      "Epoch 6644: train loss 1.0583703517913818 val loss 1.0347682237625122\n",
      "Epoch 6645: train loss 1.0347682237625122 val loss 1.1561567783355713\n",
      "Epoch 6646: train loss 1.1561567783355713 val loss 1.0352299213409424\n",
      "Epoch 6647: train loss 1.0352299213409424 val loss 1.0411856174468994\n",
      "Epoch 6648: train loss 1.0411856174468994 val loss 1.0940320491790771\n",
      "Epoch 6649: train loss 1.0940320491790771 val loss 1.0163477659225464\n",
      "Epoch 6650: train loss 1.0163477659225464 val loss 1.0551087856292725\n",
      "Epoch 6651: train loss 1.0551087856292725 val loss 1.016650915145874\n",
      "Epoch 6652: train loss 1.016650915145874 val loss 1.0520548820495605\n",
      "Epoch 6653: train loss 1.0520548820495605 val loss 1.075500249862671\n",
      "Epoch 6654: train loss 1.075500249862671 val loss 1.0167893171310425\n",
      "Epoch 6655: train loss 1.0167893171310425 val loss 1.0679563283920288\n",
      "Epoch 6656: train loss 1.0679563283920288 val loss 1.0509934425354004\n",
      "Epoch 6657: train loss 1.0509934425354004 val loss 1.0254979133605957\n",
      "Epoch 6658: train loss 1.0254979133605957 val loss 1.1559115648269653\n",
      "Epoch 6659: train loss 1.1559115648269653 val loss 1.0378310680389404\n",
      "Epoch 6660: train loss 1.0378310680389404 val loss 1.0488827228546143\n",
      "Epoch 6661: train loss 1.0488827228546143 val loss 1.0709195137023926\n",
      "Epoch 6662: train loss 1.0709195137023926 val loss 1.0182796716690063\n",
      "Epoch 6663: train loss 1.0182796716690063 val loss 1.052107572555542\n",
      "Epoch 6664: train loss 1.052107572555542 val loss 1.0284664630889893\n",
      "Epoch 6665: train loss 1.0284664630889893 val loss 1.02451491355896\n",
      "Epoch 6666: train loss 1.02451491355896 val loss 1.0176489353179932\n",
      "Epoch 6667: train loss 1.0176489353179932 val loss 1.0152814388275146\n",
      "Epoch 6668: train loss 1.0152814388275146 val loss 1.0172357559204102\n",
      "Epoch 6669: train loss 1.0172357559204102 val loss 1.0161333084106445\n",
      "Epoch 6670: train loss 1.0161333084106445 val loss 1.0208009481430054\n",
      "Epoch 6671: train loss 1.0208009481430054 val loss 1.03719162940979\n",
      "Epoch 6672: train loss 1.03719162940979 val loss 1.0279912948608398\n",
      "Epoch 6673: train loss 1.0279912948608398 val loss 1.0158195495605469\n",
      "Epoch 6674: train loss 1.0158195495605469 val loss 1.0181572437286377\n",
      "Epoch 6675: train loss 1.0181572437286377 val loss 1.0164536237716675\n",
      "Epoch 6676: train loss 1.0164536237716675 val loss 1.022894263267517\n",
      "Epoch 6677: train loss 1.022894263267517 val loss 1.0714212656021118\n",
      "Epoch 6678: train loss 1.0714212656021118 val loss 1.0176440477371216\n",
      "Epoch 6679: train loss 1.0176440477371216 val loss 1.060063123703003\n",
      "Epoch 6680: train loss 1.060063123703003 val loss 1.0577141046524048\n",
      "Epoch 6681: train loss 1.0577141046524048 val loss 1.019118070602417\n",
      "Epoch 6682: train loss 1.019118070602417 val loss 1.0485122203826904\n",
      "Epoch 6683: train loss 1.0485122203826904 val loss 1.0399245023727417\n",
      "Epoch 6684: train loss 1.0399245023727417 val loss 1.0154982805252075\n",
      "Epoch 6685: train loss 1.0154982805252075 val loss 1.0321253538131714\n",
      "Epoch 6686: train loss 1.0321253538131714 val loss 1.0646992921829224\n",
      "Epoch 6687: train loss 1.0646992921829224 val loss 1.012956142425537\n",
      "Epoch 6688: train loss 1.012956142425537 val loss 1.1061699390411377\n",
      "Epoch 6689: train loss 1.1061699390411377 val loss 1.0299499034881592\n",
      "Epoch 6690: train loss 1.0299499034881592 val loss 1.0240570306777954\n",
      "Epoch 6691: train loss 1.0240570306777954 val loss 1.076358437538147\n",
      "Epoch 6692: train loss 1.076358437538147 val loss 1.0188482999801636\n",
      "Epoch 6693: train loss 1.0188482999801636 val loss 1.0557653903961182\n",
      "Epoch 6694: train loss 1.0557653903961182 val loss 1.0512913465499878\n",
      "Epoch 6695: train loss 1.0512913465499878 val loss 1.0173149108886719\n",
      "Epoch 6696: train loss 1.0173149108886719 val loss 1.0442278385162354\n",
      "Epoch 6697: train loss 1.0442278385162354 val loss 1.0289065837860107\n",
      "Epoch 6698: train loss 1.0289065837860107 val loss 1.0245590209960938\n",
      "Epoch 6699: train loss 1.0245590209960938 val loss 1.0172103643417358\n",
      "Epoch 6700: train loss 1.0172103643417358 val loss 1.0244839191436768\n",
      "Epoch 6701: train loss 1.0244839191436768 val loss 1.0306129455566406\n",
      "Epoch 6702: train loss 1.0306129455566406 val loss 1.0163486003875732\n",
      "Epoch 6703: train loss 1.0163486003875732 val loss 1.0207921266555786\n",
      "Epoch 6704: train loss 1.0207921266555786 val loss 1.0319466590881348\n",
      "Epoch 6705: train loss 1.0319466590881348 val loss 1.0394291877746582\n",
      "Epoch 6706: train loss 1.0394291877746582 val loss 1.012528657913208\n",
      "Epoch 6707: train loss 1.012528657913208 val loss 1.0612317323684692\n",
      "Epoch 6708: train loss 1.0612317323684692 val loss 1.0772271156311035\n",
      "Epoch 6709: train loss 1.0772271156311035 val loss 1.0166847705841064\n",
      "Epoch 6710: train loss 1.0166847705841064 val loss 1.0735273361206055\n",
      "Epoch 6711: train loss 1.0735273361206055 val loss 1.0280274152755737\n",
      "Epoch 6712: train loss 1.0280274152755737 val loss 1.0148121118545532\n",
      "Epoch 6713: train loss 1.0148121118545532 val loss 1.019081711769104\n",
      "Epoch 6714: train loss 1.019081711769104 val loss 1.0524301528930664\n",
      "Epoch 6715: train loss 1.0524301528930664 val loss 1.013866901397705\n",
      "Epoch 6716: train loss 1.013866901397705 val loss 1.0564546585083008\n",
      "Epoch 6717: train loss 1.0564546585083008 val loss 1.0882337093353271\n",
      "Epoch 6718: train loss 1.0882337093353271 val loss 1.026422381401062\n",
      "Epoch 6719: train loss 1.026422381401062 val loss 1.1978161334991455\n",
      "Epoch 6720: train loss 1.1978161334991455 val loss 1.014052391052246\n",
      "Epoch 6721: train loss 1.014052391052246 val loss 1.1473217010498047\n",
      "Epoch 6722: train loss 1.1473217010498047 val loss 1.0123018026351929\n",
      "Epoch 6723: train loss 1.0123018026351929 val loss 1.1438357830047607\n",
      "Epoch 6724: train loss 1.1438357830047607 val loss 1.0718761682510376\n",
      "Epoch 6725: train loss 1.0718761682510376 val loss 1.0603852272033691\n",
      "Epoch 6726: train loss 1.0603852272033691 val loss 1.1550214290618896\n",
      "Epoch 6727: train loss 1.1550214290618896 val loss 1.0117982625961304\n",
      "Epoch 6728: train loss 1.0117982625961304 val loss 1.097825288772583\n",
      "Epoch 6729: train loss 1.097825288772583 val loss 1.0268845558166504\n",
      "Epoch 6730: train loss 1.0268845558166504 val loss 1.0179152488708496\n",
      "Epoch 6731: train loss 1.0179152488708496 val loss 1.0963144302368164\n",
      "Epoch 6732: train loss 1.0963144302368164 val loss 1.012097954750061\n",
      "Epoch 6733: train loss 1.012097954750061 val loss 1.12217116355896\n",
      "Epoch 6734: train loss 1.12217116355896 val loss 1.0734503269195557\n",
      "Epoch 6735: train loss 1.0734503269195557 val loss 1.0473183393478394\n",
      "Epoch 6736: train loss 1.0473183393478394 val loss 1.1046500205993652\n",
      "Epoch 6737: train loss 1.1046500205993652 val loss 1.0115078687667847\n",
      "Epoch 6738: train loss 1.0115078687667847 val loss 1.054823637008667\n",
      "Epoch 6739: train loss 1.054823637008667 val loss 1.011468529701233\n",
      "Epoch 6740: train loss 1.011468529701233 val loss 1.055785894393921\n",
      "Epoch 6741: train loss 1.055785894393921 val loss 1.066929578781128\n",
      "Epoch 6742: train loss 1.066929578781128 val loss 1.0155200958251953\n",
      "Epoch 6743: train loss 1.0155200958251953 val loss 1.0730459690093994\n",
      "Epoch 6744: train loss 1.0730459690093994 val loss 1.0192190408706665\n",
      "Epoch 6745: train loss 1.0192190408706665 val loss 1.0273451805114746\n",
      "Epoch 6746: train loss 1.0273451805114746 val loss 1.0749340057373047\n",
      "Epoch 6747: train loss 1.0749340057373047 val loss 1.0152500867843628\n",
      "Epoch 6748: train loss 1.0152500867843628 val loss 1.0523532629013062\n",
      "Epoch 6749: train loss 1.0523532629013062 val loss 1.0325639247894287\n",
      "Epoch 6750: train loss 1.0325639247894287 val loss 1.0141665935516357\n",
      "Epoch 6751: train loss 1.0141665935516357 val loss 1.020051121711731\n",
      "Epoch 6752: train loss 1.020051121711731 val loss 1.0127462148666382\n",
      "Epoch 6753: train loss 1.0127462148666382 val loss 1.0122014284133911\n",
      "Epoch 6754: train loss 1.0122014284133911 val loss 1.0612051486968994\n",
      "Epoch 6755: train loss 1.0612051486968994 val loss 1.0209842920303345\n",
      "Epoch 6756: train loss 1.0209842920303345 val loss 1.0139799118041992\n",
      "Epoch 6757: train loss 1.0139799118041992 val loss 1.0474495887756348\n",
      "Epoch 6758: train loss 1.0474495887756348 val loss 1.0105096101760864\n",
      "Epoch 6759: train loss 1.0105096101760864 val loss 1.0719242095947266\n",
      "Epoch 6760: train loss 1.0719242095947266 val loss 1.0467281341552734\n",
      "Epoch 6761: train loss 1.0467281341552734 val loss 1.0174028873443604\n",
      "Epoch 6762: train loss 1.0174028873443604 val loss 1.0613477230072021\n",
      "Epoch 6763: train loss 1.0613477230072021 val loss 1.0180714130401611\n",
      "Epoch 6764: train loss 1.0180714130401611 val loss 1.027395486831665\n",
      "Epoch 6765: train loss 1.027395486831665 val loss 1.072426676750183\n",
      "Epoch 6766: train loss 1.072426676750183 val loss 1.0125725269317627\n",
      "Epoch 6767: train loss 1.0125725269317627 val loss 1.0566874742507935\n",
      "Epoch 6768: train loss 1.0566874742507935 val loss 1.0254261493682861\n",
      "Epoch 6769: train loss 1.0254261493682861 val loss 1.0091146230697632\n",
      "Epoch 6770: train loss 1.0091146230697632 val loss 1.0167665481567383\n",
      "Epoch 6771: train loss 1.0167665481567383 val loss 1.0126404762268066\n",
      "Epoch 6772: train loss 1.0126404762268066 val loss 1.0247595310211182\n",
      "Epoch 6773: train loss 1.0247595310211182 val loss 1.0331149101257324\n",
      "Epoch 6774: train loss 1.0331149101257324 val loss 1.0092955827713013\n",
      "Epoch 6775: train loss 1.0092955827713013 val loss 1.065551519393921\n",
      "Epoch 6776: train loss 1.065551519393921 val loss 1.0528695583343506\n",
      "Epoch 6777: train loss 1.0528695583343506 val loss 1.0189882516860962\n",
      "Epoch 6778: train loss 1.0189882516860962 val loss 1.0873048305511475\n",
      "Epoch 6779: train loss 1.0873048305511475 val loss 1.0102695226669312\n",
      "Epoch 6780: train loss 1.0102695226669312 val loss 1.0661550760269165\n",
      "Epoch 6781: train loss 1.0661550760269165 val loss 1.0323301553726196\n",
      "Epoch 6782: train loss 1.0323301553726196 val loss 1.0085357427597046\n",
      "Epoch 6783: train loss 1.0085357427597046 val loss 1.015979290008545\n",
      "Epoch 6784: train loss 1.015979290008545 val loss 1.0138332843780518\n",
      "Epoch 6785: train loss 1.0138332843780518 val loss 1.0135918855667114\n",
      "Epoch 6786: train loss 1.0135918855667114 val loss 1.012140154838562\n",
      "Epoch 6787: train loss 1.012140154838562 val loss 1.0151081085205078\n",
      "Epoch 6788: train loss 1.0151081085205078 val loss 1.0276546478271484\n",
      "Epoch 6789: train loss 1.0276546478271484 val loss 1.0166772603988647\n",
      "Epoch 6790: train loss 1.0166772603988647 val loss 1.012534499168396\n",
      "Epoch 6791: train loss 1.012534499168396 val loss 1.0204828977584839\n",
      "Epoch 6792: train loss 1.0204828977584839 val loss 1.059135913848877\n",
      "Epoch 6793: train loss 1.059135913848877 val loss 1.0211880207061768\n",
      "Epoch 6794: train loss 1.0211880207061768 val loss 1.0236241817474365\n",
      "Epoch 6795: train loss 1.0236241817474365 val loss 1.0915460586547852\n",
      "Epoch 6796: train loss 1.0915460586547852 val loss 1.0118292570114136\n",
      "Epoch 6797: train loss 1.0118292570114136 val loss 1.0675764083862305\n",
      "Epoch 6798: train loss 1.0675764083862305 val loss 1.0325634479522705\n",
      "Epoch 6799: train loss 1.0325634479522705 val loss 1.009210467338562\n",
      "Epoch 6800: train loss 1.009210467338562 val loss 1.025464415550232\n",
      "Epoch 6801: train loss 1.025464415550232 val loss 1.0388145446777344\n",
      "Epoch 6802: train loss 1.0388145446777344 val loss 1.040464162826538\n",
      "Epoch 6803: train loss 1.040464162826538 val loss 1.010656476020813\n",
      "Epoch 6804: train loss 1.010656476020813 val loss 1.039716124534607\n",
      "Epoch 6805: train loss 1.039716124534607 val loss 1.0392063856124878\n",
      "Epoch 6806: train loss 1.0392063856124878 val loss 1.0120826959609985\n",
      "Epoch 6807: train loss 1.0120826959609985 val loss 1.0289626121520996\n",
      "Epoch 6808: train loss 1.0289626121520996 val loss 1.0383572578430176\n",
      "Epoch 6809: train loss 1.0383572578430176 val loss 1.0095617771148682\n",
      "Epoch 6810: train loss 1.0095617771148682 val loss 1.0306097269058228\n",
      "Epoch 6811: train loss 1.0306097269058228 val loss 1.0466606616973877\n",
      "Epoch 6812: train loss 1.0466606616973877 val loss 1.0076260566711426\n",
      "Epoch 6813: train loss 1.0076260566711426 val loss 1.0549030303955078\n",
      "Epoch 6814: train loss 1.0549030303955078 val loss 1.0533864498138428\n",
      "Epoch 6815: train loss 1.0533864498138428 val loss 1.0074598789215088\n",
      "Epoch 6816: train loss 1.0074598789215088 val loss 1.0860371589660645\n",
      "Epoch 6817: train loss 1.0860371589660645 val loss 1.0140379667282104\n",
      "Epoch 6818: train loss 1.0140379667282104 val loss 1.0621581077575684\n",
      "Epoch 6819: train loss 1.0621581077575684 val loss 1.054542899131775\n",
      "Epoch 6820: train loss 1.054542899131775 val loss 1.0073390007019043\n",
      "Epoch 6821: train loss 1.0073390007019043 val loss 1.0308682918548584\n",
      "Epoch 6822: train loss 1.0308682918548584 val loss 1.0298080444335938\n",
      "Epoch 6823: train loss 1.0298080444335938 val loss 1.0131701231002808\n",
      "Epoch 6824: train loss 1.0131701231002808 val loss 1.015856146812439\n",
      "Epoch 6825: train loss 1.015856146812439 val loss 1.0246899127960205\n",
      "Epoch 6826: train loss 1.0246899127960205 val loss 1.0299432277679443\n",
      "Epoch 6827: train loss 1.0299432277679443 val loss 1.0058019161224365\n",
      "Epoch 6828: train loss 1.0058019161224365 val loss 1.053022027015686\n",
      "Epoch 6829: train loss 1.053022027015686 val loss 1.0709131956100464\n",
      "Epoch 6830: train loss 1.0709131956100464 val loss 1.016289472579956\n",
      "Epoch 6831: train loss 1.016289472579956 val loss 1.1213666200637817\n",
      "Epoch 6832: train loss 1.1213666200637817 val loss 1.022790551185608\n",
      "Epoch 6833: train loss 1.022790551185608 val loss 1.0280171632766724\n",
      "Epoch 6834: train loss 1.0280171632766724 val loss 1.0857176780700684\n",
      "Epoch 6835: train loss 1.0857176780700684 val loss 1.0057693719863892\n",
      "Epoch 6836: train loss 1.0057693719863892 val loss 1.0504193305969238\n",
      "Epoch 6837: train loss 1.0504193305969238 val loss 1.007411241531372\n",
      "Epoch 6838: train loss 1.007411241531372 val loss 1.0406744480133057\n",
      "Epoch 6839: train loss 1.0406744480133057 val loss 1.1013177633285522\n",
      "Epoch 6840: train loss 1.1013177633285522 val loss 1.011722207069397\n",
      "Epoch 6841: train loss 1.011722207069397 val loss 1.0891599655151367\n",
      "Epoch 6842: train loss 1.0891599655151367 val loss 1.0112053155899048\n",
      "Epoch 6843: train loss 1.0112053155899048 val loss 1.043308973312378\n",
      "Epoch 6844: train loss 1.043308973312378 val loss 1.0237423181533813\n",
      "Epoch 6845: train loss 1.0237423181533813 val loss 1.0101912021636963\n",
      "Epoch 6846: train loss 1.0101912021636963 val loss 1.0229363441467285\n",
      "Epoch 6847: train loss 1.0229363441467285 val loss 1.067731499671936\n",
      "Epoch 6848: train loss 1.067731499671936 val loss 1.0096144676208496\n",
      "Epoch 6849: train loss 1.0096144676208496 val loss 1.0497877597808838\n",
      "Epoch 6850: train loss 1.0497877597808838 val loss 1.0262939929962158\n",
      "Epoch 6851: train loss 1.0262939929962158 val loss 1.007635235786438\n",
      "Epoch 6852: train loss 1.007635235786438 val loss 1.0109165906906128\n",
      "Epoch 6853: train loss 1.0109165906906128 val loss 1.0046305656433105\n",
      "Epoch 6854: train loss 1.0046305656433105 val loss 1.0143630504608154\n",
      "Epoch 6855: train loss 1.0143630504608154 val loss 1.0428372621536255\n",
      "Epoch 6856: train loss 1.0428372621536255 val loss 1.0063108205795288\n",
      "Epoch 6857: train loss 1.0063108205795288 val loss 1.0684460401535034\n",
      "Epoch 6858: train loss 1.0684460401535034 val loss 1.0237712860107422\n",
      "Epoch 6859: train loss 1.0237712860107422 val loss 1.0072200298309326\n",
      "Epoch 6860: train loss 1.0072200298309326 val loss 1.0291383266448975\n",
      "Epoch 6861: train loss 1.0291383266448975 val loss 1.045911192893982\n",
      "Epoch 6862: train loss 1.045911192893982 val loss 1.0057413578033447\n",
      "Epoch 6863: train loss 1.0057413578033447 val loss 1.063803791999817\n",
      "Epoch 6864: train loss 1.063803791999817 val loss 1.0328941345214844\n",
      "Epoch 6865: train loss 1.0328941345214844 val loss 1.0130906105041504\n",
      "Epoch 6866: train loss 1.0130906105041504 val loss 1.0565060377120972\n",
      "Epoch 6867: train loss 1.0565060377120972 val loss 1.0149705410003662\n",
      "Epoch 6868: train loss 1.0149705410003662 val loss 1.0069531202316284\n",
      "Epoch 6869: train loss 1.0069531202316284 val loss 1.0173314809799194\n",
      "Epoch 6870: train loss 1.0173314809799194 val loss 1.021856665611267\n",
      "Epoch 6871: train loss 1.021856665611267 val loss 1.0067600011825562\n",
      "Epoch 6872: train loss 1.0067600011825562 val loss 1.007798671722412\n",
      "Epoch 6873: train loss 1.007798671722412 val loss 1.079099416732788\n",
      "Epoch 6874: train loss 1.079099416732788 val loss 1.0048317909240723\n",
      "Epoch 6875: train loss 1.0048317909240723 val loss 1.1154791116714478\n",
      "Epoch 6876: train loss 1.1154791116714478 val loss 1.0457284450531006\n",
      "Epoch 6877: train loss 1.0457284450531006 val loss 1.065711498260498\n",
      "Epoch 6878: train loss 1.065711498260498 val loss 1.0852818489074707\n",
      "Epoch 6879: train loss 1.0852818489074707 val loss 1.0241715908050537\n",
      "Epoch 6880: train loss 1.0241715908050537 val loss 1.192270278930664\n",
      "Epoch 6881: train loss 1.192270278930664 val loss 1.060483694076538\n",
      "Epoch 6882: train loss 1.060483694076538 val loss 1.2622723579406738\n",
      "Epoch 6883: train loss 1.2622723579406738 val loss 1.100587010383606\n",
      "Epoch 6884: train loss 1.100587010383606 val loss 1.356456995010376\n",
      "Epoch 6885: train loss 1.356456995010376 val loss 1.3519940376281738\n",
      "Epoch 6886: train loss 1.3519940376281738 val loss 1.0513356924057007\n",
      "Epoch 6887: train loss 1.0513356924057007 val loss 1.2664053440093994\n",
      "Epoch 6888: train loss 1.2664053440093994 val loss 1.0194311141967773\n",
      "Epoch 6889: train loss 1.0194311141967773 val loss 1.1635394096374512\n",
      "Epoch 6890: train loss 1.1635394096374512 val loss 1.008946180343628\n",
      "Epoch 6891: train loss 1.008946180343628 val loss 1.1798884868621826\n",
      "Epoch 6892: train loss 1.1798884868621826 val loss 1.018042802810669\n",
      "Epoch 6893: train loss 1.018042802810669 val loss 1.0903656482696533\n",
      "Epoch 6894: train loss 1.0903656482696533 val loss 1.0071344375610352\n",
      "Epoch 6895: train loss 1.0071344375610352 val loss 1.0952895879745483\n",
      "Epoch 6896: train loss 1.0952895879745483 val loss 1.0314571857452393\n",
      "Epoch 6897: train loss 1.0314571857452393 val loss 1.039655089378357\n",
      "Epoch 6898: train loss 1.039655089378357 val loss 1.0861068964004517\n",
      "Epoch 6899: train loss 1.0861068964004517 val loss 1.0088366270065308\n",
      "Epoch 6900: train loss 1.0088366270065308 val loss 1.1151599884033203\n",
      "Epoch 6901: train loss 1.1151599884033203 val loss 1.013798713684082\n",
      "Epoch 6902: train loss 1.013798713684082 val loss 1.1560413837432861\n",
      "Epoch 6903: train loss 1.1560413837432861 val loss 1.005457878112793\n",
      "Epoch 6904: train loss 1.005457878112793 val loss 1.081695318222046\n",
      "Epoch 6905: train loss 1.081695318222046 val loss 1.0060373544692993\n",
      "Epoch 6906: train loss 1.0060373544692993 val loss 1.0917047262191772\n",
      "Epoch 6907: train loss 1.0917047262191772 val loss 1.0279985666275024\n",
      "Epoch 6908: train loss 1.0279985666275024 val loss 1.0245554447174072\n",
      "Epoch 6909: train loss 1.0245554447174072 val loss 1.083723545074463\n",
      "Epoch 6910: train loss 1.083723545074463 val loss 1.0037286281585693\n",
      "Epoch 6911: train loss 1.0037286281585693 val loss 1.0379376411437988\n",
      "Epoch 6912: train loss 1.0379376411437988 val loss 1.0051804780960083\n",
      "Epoch 6913: train loss 1.0051804780960083 val loss 1.0013189315795898\n",
      "Epoch 6914: train loss 1.0013189315795898 val loss 1.0028877258300781\n",
      "Epoch 6915: train loss 1.0028877258300781 val loss 1.0032094717025757\n",
      "Epoch 6916: train loss 1.0032094717025757 val loss 1.014632225036621\n",
      "Epoch 6917: train loss 1.014632225036621 val loss 1.0146689414978027\n",
      "Epoch 6918: train loss 1.0146689414978027 val loss 1.0065959692001343\n",
      "Epoch 6919: train loss 1.0065959692001343 val loss 1.0169332027435303\n",
      "Epoch 6920: train loss 1.0169332027435303 val loss 1.0430408716201782\n",
      "Epoch 6921: train loss 1.0430408716201782 val loss 1.0150648355484009\n",
      "Epoch 6922: train loss 1.0150648355484009 val loss 1.005807638168335\n",
      "Epoch 6923: train loss 1.005807638168335 val loss 1.010104775428772\n",
      "Epoch 6924: train loss 1.010104775428772 val loss 1.022352695465088\n",
      "Epoch 6925: train loss 1.022352695465088 val loss 1.0022916793823242\n",
      "Epoch 6926: train loss 1.0022916793823242 val loss 1.0443809032440186\n",
      "Epoch 6927: train loss 1.0443809032440186 val loss 1.0698590278625488\n",
      "Epoch 6928: train loss 1.0698590278625488 val loss 1.011413812637329\n",
      "Epoch 6929: train loss 1.011413812637329 val loss 1.155370831489563\n",
      "Epoch 6930: train loss 1.155370831489563 val loss 1.0017718076705933\n",
      "Epoch 6931: train loss 1.0017718076705933 val loss 1.0970094203948975\n",
      "Epoch 6932: train loss 1.0970094203948975 val loss 1.0004684925079346\n",
      "Epoch 6933: train loss 1.0004684925079346 val loss 1.1356394290924072\n",
      "Epoch 6934: train loss 1.1356394290924072 val loss 1.0322661399841309\n",
      "Epoch 6935: train loss 1.0322661399841309 val loss 1.0599620342254639\n",
      "Epoch 6936: train loss 1.0599620342254639 val loss 1.067517876625061\n",
      "Epoch 6937: train loss 1.067517876625061 val loss 1.0127029418945312\n",
      "Epoch 6938: train loss 1.0127029418945312 val loss 1.1344692707061768\n",
      "Epoch 6939: train loss 1.1344692707061768 val loss 1.009561538696289\n",
      "Epoch 6940: train loss 1.009561538696289 val loss 1.1756291389465332\n",
      "Epoch 6941: train loss 1.1756291389465332 val loss 1.0030114650726318\n",
      "Epoch 6942: train loss 1.0030114650726318 val loss 1.0950603485107422\n",
      "Epoch 6943: train loss 1.0950603485107422 val loss 1.0028544664382935\n",
      "Epoch 6944: train loss 1.0028544664382935 val loss 1.1131446361541748\n",
      "Epoch 6945: train loss 1.1131446361541748 val loss 1.0483782291412354\n",
      "Epoch 6946: train loss 1.0483782291412354 val loss 1.0328080654144287\n",
      "Epoch 6947: train loss 1.0328080654144287 val loss 1.079944372177124\n",
      "Epoch 6948: train loss 1.079944372177124 val loss 1.007900357246399\n",
      "Epoch 6949: train loss 1.007900357246399 val loss 1.1397188901901245\n",
      "Epoch 6950: train loss 1.1397188901901245 val loss 1.0158146619796753\n",
      "Epoch 6951: train loss 1.0158146619796753 val loss 1.23583984375\n",
      "Epoch 6952: train loss 1.23583984375 val loss 1.0420525074005127\n",
      "Epoch 6953: train loss 1.0420525074005127 val loss 1.2577016353607178\n",
      "Epoch 6954: train loss 1.2577016353607178 val loss 1.2043943405151367\n",
      "Epoch 6955: train loss 1.2043943405151367 val loss 1.097329020500183\n",
      "Epoch 6956: train loss 1.097329020500183 val loss 1.087087869644165\n",
      "Epoch 6957: train loss 1.087087869644165 val loss 1.1683698892593384\n",
      "Epoch 6958: train loss 1.1683698892593384 val loss 1.125417709350586\n",
      "Epoch 6959: train loss 1.125417709350586 val loss 1.1570912599563599\n",
      "Epoch 6960: train loss 1.1570912599563599 val loss 1.0959694385528564\n",
      "Epoch 6961: train loss 1.0959694385528564 val loss 1.2162498235702515\n",
      "Epoch 6962: train loss 1.2162498235702515 val loss 1.2004345655441284\n",
      "Epoch 6963: train loss 1.2004345655441284 val loss 1.0663909912109375\n",
      "Epoch 6964: train loss 1.0663909912109375 val loss 1.1546497344970703\n",
      "Epoch 6965: train loss 1.1546497344970703 val loss 1.0905035734176636\n",
      "Epoch 6966: train loss 1.0905035734176636 val loss 1.0955063104629517\n",
      "Epoch 6967: train loss 1.0955063104629517 val loss 1.0906400680541992\n",
      "Epoch 6968: train loss 1.0906400680541992 val loss 1.0211894512176514\n",
      "Epoch 6969: train loss 1.0211894512176514 val loss 1.152963399887085\n",
      "Epoch 6970: train loss 1.152963399887085 val loss 1.0520460605621338\n",
      "Epoch 6971: train loss 1.0520460605621338 val loss 1.1651488542556763\n",
      "Epoch 6972: train loss 1.1651488542556763 val loss 1.0245933532714844\n",
      "Epoch 6973: train loss 1.0245933532714844 val loss 1.2097601890563965\n",
      "Epoch 6974: train loss 1.2097601890563965 val loss 1.1299091577529907\n",
      "Epoch 6975: train loss 1.1299091577529907 val loss 1.181520700454712\n",
      "Epoch 6976: train loss 1.181520700454712 val loss 1.1197993755340576\n",
      "Epoch 6977: train loss 1.1197993755340576 val loss 1.1918010711669922\n",
      "Epoch 6978: train loss 1.1918010711669922 val loss 1.2165323495864868\n",
      "Epoch 6979: train loss 1.2165323495864868 val loss 1.0294185876846313\n",
      "Epoch 6980: train loss 1.0294185876846313 val loss 1.1152167320251465\n",
      "Epoch 6981: train loss 1.1152167320251465 val loss 1.0628360509872437\n",
      "Epoch 6982: train loss 1.0628360509872437 val loss 1.0732808113098145\n",
      "Epoch 6983: train loss 1.0732808113098145 val loss 1.0457216501235962\n",
      "Epoch 6984: train loss 1.0457216501235962 val loss 1.0275495052337646\n",
      "Epoch 6985: train loss 1.0275495052337646 val loss 1.0872066020965576\n",
      "Epoch 6986: train loss 1.0872066020965576 val loss 1.0242626667022705\n",
      "Epoch 6987: train loss 1.0242626667022705 val loss 1.1088881492614746\n",
      "Epoch 6988: train loss 1.1088881492614746 val loss 1.0039499998092651\n",
      "Epoch 6989: train loss 1.0039499998092651 val loss 1.0830960273742676\n",
      "Epoch 6990: train loss 1.0830960273742676 val loss 1.003755807876587\n",
      "Epoch 6991: train loss 1.003755807876587 val loss 1.1161564588546753\n",
      "Epoch 6992: train loss 1.1161564588546753 val loss 1.0065057277679443\n",
      "Epoch 6993: train loss 1.0065057277679443 val loss 1.0680309534072876\n",
      "Epoch 6994: train loss 1.0680309534072876 val loss 1.0010038614273071\n",
      "Epoch 6995: train loss 1.0010038614273071 val loss 1.0604195594787598\n",
      "Epoch 6996: train loss 1.0604195594787598 val loss 1.0226364135742188\n",
      "Epoch 6997: train loss 1.0226364135742188 val loss 1.0169954299926758\n",
      "Epoch 6998: train loss 1.0169954299926758 val loss 1.0658210515975952\n",
      "Epoch 6999: train loss 1.0658210515975952 val loss 1.0032355785369873\n",
      "Epoch 7000: train loss 1.0032355785369873 val loss 1.046536922454834\n",
      "Epoch 7001: train loss 1.046536922454834 val loss 1.0046252012252808\n",
      "Epoch 7002: train loss 1.0046252012252808 val loss 1.0606476068496704\n",
      "Epoch 7003: train loss 1.0606476068496704 val loss 1.0252947807312012\n",
      "Epoch 7004: train loss 1.0252947807312012 val loss 1.017021894454956\n",
      "Epoch 7005: train loss 1.017021894454956 val loss 1.0682883262634277\n",
      "Epoch 7006: train loss 1.0682883262634277 val loss 0.9991209506988525\n",
      "Epoch 7007: train loss 0.9991209506988525 val loss 1.0148944854736328\n",
      "Epoch 7008: train loss 1.0148944854736328 val loss 0.9996747970581055\n",
      "Epoch 7009: train loss 0.9996747970581055 val loss 1.014699935913086\n",
      "Epoch 7010: train loss 1.014699935913086 val loss 1.0384891033172607\n",
      "Epoch 7011: train loss 1.0384891033172607 val loss 1.0022670030593872\n",
      "Epoch 7012: train loss 1.0022670030593872 val loss 1.0443419218063354\n",
      "Epoch 7013: train loss 1.0443419218063354 val loss 1.0158363580703735\n",
      "Epoch 7014: train loss 1.0158363580703735 val loss 1.0049453973770142\n",
      "Epoch 7015: train loss 1.0049453973770142 val loss 1.0059947967529297\n",
      "Epoch 7016: train loss 1.0059947967529297 val loss 1.010295033454895\n",
      "Epoch 7017: train loss 1.010295033454895 val loss 1.0075421333312988\n",
      "Epoch 7018: train loss 1.0075421333312988 val loss 1.009900689125061\n",
      "Epoch 7019: train loss 1.009900689125061 val loss 1.0037935972213745\n",
      "Epoch 7020: train loss 1.0037935972213745 val loss 1.0167335271835327\n",
      "Epoch 7021: train loss 1.0167335271835327 val loss 1.0407545566558838\n",
      "Epoch 7022: train loss 1.0407545566558838 val loss 1.0034310817718506\n",
      "Epoch 7023: train loss 1.0034310817718506 val loss 1.0235803127288818\n",
      "Epoch 7024: train loss 1.0235803127288818 val loss 1.0203378200531006\n",
      "Epoch 7025: train loss 1.0203378200531006 val loss 1.0037213563919067\n",
      "Epoch 7026: train loss 1.0037213563919067 val loss 1.0259894132614136\n",
      "Epoch 7027: train loss 1.0259894132614136 val loss 1.0192322731018066\n",
      "Epoch 7028: train loss 1.0192322731018066 val loss 1.0025819540023804\n",
      "Epoch 7029: train loss 1.0025819540023804 val loss 1.0196064710617065\n",
      "Epoch 7030: train loss 1.0196064710617065 val loss 1.0269749164581299\n",
      "Epoch 7031: train loss 1.0269749164581299 val loss 1.0012826919555664\n",
      "Epoch 7032: train loss 1.0012826919555664 val loss 1.0194029808044434\n",
      "Epoch 7033: train loss 1.0194029808044434 val loss 1.0224740505218506\n",
      "Epoch 7034: train loss 1.0224740505218506 val loss 1.0021892786026\n",
      "Epoch 7035: train loss 1.0021892786026 val loss 1.0256906747817993\n",
      "Epoch 7036: train loss 1.0256906747817993 val loss 1.0385396480560303\n",
      "Epoch 7037: train loss 1.0385396480560303 val loss 1.0007355213165283\n",
      "Epoch 7038: train loss 1.0007355213165283 val loss 1.024620532989502\n",
      "Epoch 7039: train loss 1.024620532989502 val loss 1.0321186780929565\n",
      "Epoch 7040: train loss 1.0321186780929565 val loss 0.9993419647216797\n",
      "Epoch 7041: train loss 0.9993419647216797 val loss 1.0201258659362793\n",
      "Epoch 7042: train loss 1.0201258659362793 val loss 1.0094237327575684\n",
      "Epoch 7043: train loss 1.0094237327575684 val loss 0.9987842440605164\n",
      "Epoch 7044: train loss 0.9987842440605164 val loss 0.9999568462371826\n",
      "Epoch 7045: train loss 0.9999568462371826 val loss 0.9972387552261353\n",
      "Epoch 7046: train loss 0.9972387552261353 val loss 0.9971045851707458\n",
      "Epoch 7047: train loss 0.9971045851707458 val loss 1.000193476676941\n",
      "Epoch 7048: train loss 1.000193476676941 val loss 0.9990081191062927\n",
      "Epoch 7049: train loss 0.9990081191062927 val loss 1.0088645219802856\n",
      "Epoch 7050: train loss 1.0088645219802856 val loss 1.0074694156646729\n",
      "Epoch 7051: train loss 1.0074694156646729 val loss 1.001033902168274\n",
      "Epoch 7052: train loss 1.001033902168274 val loss 1.0016453266143799\n",
      "Epoch 7053: train loss 1.0016453266143799 val loss 1.0161499977111816\n",
      "Epoch 7054: train loss 1.0161499977111816 val loss 0.9979636669158936\n",
      "Epoch 7055: train loss 0.9979636669158936 val loss 1.0405471324920654\n",
      "Epoch 7056: train loss 1.0405471324920654 val loss 1.0461139678955078\n",
      "Epoch 7057: train loss 1.0461139678955078 val loss 1.004786491394043\n",
      "Epoch 7058: train loss 1.004786491394043 val loss 1.0610800981521606\n",
      "Epoch 7059: train loss 1.0610800981521606 val loss 0.9993593692779541\n",
      "Epoch 7060: train loss 0.9993593692779541 val loss 1.0338587760925293\n",
      "Epoch 7061: train loss 1.0338587760925293 val loss 1.006593108177185\n",
      "Epoch 7062: train loss 1.006593108177185 val loss 0.9975042343139648\n",
      "Epoch 7063: train loss 0.9975042343139648 val loss 0.9986547827720642\n",
      "Epoch 7064: train loss 0.9986547827720642 val loss 0.9965987205505371\n",
      "Epoch 7065: train loss 0.9965987205505371 val loss 0.9961815476417542\n",
      "Epoch 7066: train loss 0.9961815476417542 val loss 0.9978290796279907\n",
      "Epoch 7067: train loss 0.9978290796279907 val loss 0.9961724877357483\n",
      "Epoch 7068: train loss 0.9961724877357483 val loss 1.0130523443222046\n",
      "Epoch 7069: train loss 1.0130523443222046 val loss 1.0356038808822632\n",
      "Epoch 7070: train loss 1.0356038808822632 val loss 1.0050230026245117\n",
      "Epoch 7071: train loss 1.0050230026245117 val loss 1.0716239213943481\n",
      "Epoch 7072: train loss 1.0716239213943481 val loss 0.9950848817825317\n",
      "Epoch 7073: train loss 0.9950848817825317 val loss 1.0206491947174072\n",
      "Epoch 7074: train loss 1.0206491947174072 val loss 0.998023509979248\n",
      "Epoch 7075: train loss 0.998023509979248 val loss 1.0221772193908691\n",
      "Epoch 7076: train loss 1.0221772193908691 val loss 1.0552310943603516\n",
      "Epoch 7077: train loss 1.0552310943603516 val loss 1.0032063722610474\n",
      "Epoch 7078: train loss 1.0032063722610474 val loss 1.0759282112121582\n",
      "Epoch 7079: train loss 1.0759282112121582 val loss 0.9947461485862732\n",
      "Epoch 7080: train loss 0.9947461485862732 val loss 1.0284432172775269\n",
      "Epoch 7081: train loss 1.0284432172775269 val loss 0.994537353515625\n",
      "Epoch 7082: train loss 0.994537353515625 val loss 1.0596952438354492\n",
      "Epoch 7083: train loss 1.0596952438354492 val loss 1.0085492134094238\n",
      "Epoch 7084: train loss 1.0085492134094238 val loss 1.0041961669921875\n",
      "Epoch 7085: train loss 1.0041961669921875 val loss 1.0144704580307007\n",
      "Epoch 7086: train loss 1.0144704580307007 val loss 1.014976978302002\n",
      "Epoch 7087: train loss 1.014976978302002 val loss 0.9970098733901978\n",
      "Epoch 7088: train loss 0.9970098733901978 val loss 1.027546763420105\n",
      "Epoch 7089: train loss 1.027546763420105 val loss 1.0409257411956787\n",
      "Epoch 7090: train loss 1.0409257411956787 val loss 0.9986488819122314\n",
      "Epoch 7091: train loss 0.9986488819122314 val loss 1.0455272197723389\n",
      "Epoch 7092: train loss 1.0455272197723389 val loss 1.0082073211669922\n",
      "Epoch 7093: train loss 1.0082073211669922 val loss 1.0033361911773682\n",
      "Epoch 7094: train loss 1.0033361911773682 val loss 1.0044949054718018\n",
      "Epoch 7095: train loss 1.0044949054718018 val loss 1.0011380910873413\n",
      "Epoch 7096: train loss 1.0011380910873413 val loss 0.9978529214859009\n",
      "Epoch 7097: train loss 0.9978529214859009 val loss 1.0004314184188843\n",
      "Epoch 7098: train loss 1.0004314184188843 val loss 1.00923752784729\n",
      "Epoch 7099: train loss 1.00923752784729 val loss 0.9937148094177246\n",
      "Epoch 7100: train loss 0.9937148094177246 val loss 1.0042978525161743\n",
      "Epoch 7101: train loss 1.0042978525161743 val loss 1.0514590740203857\n",
      "Epoch 7102: train loss 1.0514590740203857 val loss 0.9962058067321777\n",
      "Epoch 7103: train loss 0.9962058067321777 val loss 1.0722167491912842\n",
      "Epoch 7104: train loss 1.0722167491912842 val loss 1.0062534809112549\n",
      "Epoch 7105: train loss 1.0062534809112549 val loss 1.0370701551437378\n",
      "Epoch 7106: train loss 1.0370701551437378 val loss 1.025450587272644\n",
      "Epoch 7107: train loss 1.025450587272644 val loss 0.9985471963882446\n",
      "Epoch 7108: train loss 0.9985471963882446 val loss 1.021036982536316\n",
      "Epoch 7109: train loss 1.021036982536316 val loss 1.012397050857544\n",
      "Epoch 7110: train loss 1.012397050857544 val loss 0.9985424280166626\n",
      "Epoch 7111: train loss 0.9985424280166626 val loss 1.015541672706604\n",
      "Epoch 7112: train loss 1.015541672706604 val loss 1.0251330137252808\n",
      "Epoch 7113: train loss 1.0251330137252808 val loss 0.9980154633522034\n",
      "Epoch 7114: train loss 0.9980154633522034 val loss 1.0253671407699585\n",
      "Epoch 7115: train loss 1.0253671407699585 val loss 1.0048576593399048\n",
      "Epoch 7116: train loss 1.0048576593399048 val loss 0.9953649044036865\n",
      "Epoch 7117: train loss 0.9953649044036865 val loss 0.997534990310669\n",
      "Epoch 7118: train loss 0.997534990310669 val loss 0.9938051104545593\n",
      "Epoch 7119: train loss 0.9938051104545593 val loss 1.0008652210235596\n",
      "Epoch 7120: train loss 1.0008652210235596 val loss 1.0400705337524414\n",
      "Epoch 7121: train loss 1.0400705337524414 val loss 0.993018627166748\n",
      "Epoch 7122: train loss 0.993018627166748 val loss 1.0652927160263062\n",
      "Epoch 7123: train loss 1.0652927160263062 val loss 1.0073623657226562\n",
      "Epoch 7124: train loss 1.0073623657226562 val loss 1.0021790266036987\n",
      "Epoch 7125: train loss 1.0021790266036987 val loss 1.0181522369384766\n",
      "Epoch 7126: train loss 1.0181522369384766 val loss 1.0068073272705078\n",
      "Epoch 7127: train loss 1.0068073272705078 val loss 0.9955116510391235\n",
      "Epoch 7128: train loss 0.9955116510391235 val loss 1.0218193531036377\n",
      "Epoch 7129: train loss 1.0218193531036377 val loss 1.0434376001358032\n",
      "Epoch 7130: train loss 1.0434376001358032 val loss 0.9970503449440002\n",
      "Epoch 7131: train loss 0.9970503449440002 val loss 1.0439026355743408\n",
      "Epoch 7132: train loss 1.0439026355743408 val loss 1.007175326347351\n",
      "Epoch 7133: train loss 1.007175326347351 val loss 1.0142468214035034\n",
      "Epoch 7134: train loss 1.0142468214035034 val loss 1.0485806465148926\n",
      "Epoch 7135: train loss 1.0485806465148926 val loss 0.9975743293762207\n",
      "Epoch 7136: train loss 0.9975743293762207 val loss 1.024381399154663\n",
      "Epoch 7137: train loss 1.024381399154663 val loss 0.9981942176818848\n",
      "Epoch 7138: train loss 0.9981942176818848 val loss 1.0298981666564941\n",
      "Epoch 7139: train loss 1.0298981666564941 val loss 1.042269229888916\n",
      "Epoch 7140: train loss 1.042269229888916 val loss 1.010765790939331\n",
      "Epoch 7141: train loss 1.010765790939331 val loss 1.113069772720337\n",
      "Epoch 7142: train loss 1.113069772720337 val loss 0.9958518743515015\n",
      "Epoch 7143: train loss 0.9958518743515015 val loss 1.0534313917160034\n",
      "Epoch 7144: train loss 1.0534313917160034 val loss 0.9956480264663696\n",
      "Epoch 7145: train loss 0.9956480264663696 val loss 1.0018352270126343\n",
      "Epoch 7146: train loss 1.0018352270126343 val loss 1.0409090518951416\n",
      "Epoch 7147: train loss 1.0409090518951416 val loss 0.9983503222465515\n",
      "Epoch 7148: train loss 0.9983503222465515 val loss 1.0325932502746582\n",
      "Epoch 7149: train loss 1.0325932502746582 val loss 1.004063606262207\n",
      "Epoch 7150: train loss 1.004063606262207 val loss 1.0146746635437012\n",
      "Epoch 7151: train loss 1.0146746635437012 val loss 1.0244636535644531\n",
      "Epoch 7152: train loss 1.0244636535644531 val loss 0.9981036186218262\n",
      "Epoch 7153: train loss 0.9981036186218262 val loss 1.001465916633606\n",
      "Epoch 7154: train loss 1.001465916633606 val loss 0.9971432685852051\n",
      "Epoch 7155: train loss 0.9971432685852051 val loss 1.0010930299758911\n",
      "Epoch 7156: train loss 1.0010930299758911 val loss 0.9956860542297363\n",
      "Epoch 7157: train loss 0.9956860542297363 val loss 1.0048247575759888\n",
      "Epoch 7158: train loss 1.0048247575759888 val loss 1.0050251483917236\n",
      "Epoch 7159: train loss 1.0050251483917236 val loss 0.9912874102592468\n",
      "Epoch 7160: train loss 0.9912874102592468 val loss 1.0090563297271729\n",
      "Epoch 7161: train loss 1.0090563297271729 val loss 1.070085048675537\n",
      "Epoch 7162: train loss 1.070085048675537 val loss 1.005947470664978\n",
      "Epoch 7163: train loss 1.005947470664978 val loss 1.1469461917877197\n",
      "Epoch 7164: train loss 1.1469461917877197 val loss 0.9934122562408447\n",
      "Epoch 7165: train loss 0.9934122562408447 val loss 1.0905728340148926\n",
      "Epoch 7166: train loss 1.0905728340148926 val loss 0.9946523904800415\n",
      "Epoch 7167: train loss 0.9946523904800415 val loss 1.090358018875122\n",
      "Epoch 7168: train loss 1.090358018875122 val loss 1.001712441444397\n",
      "Epoch 7169: train loss 1.001712441444397 val loss 1.0447206497192383\n",
      "Epoch 7170: train loss 1.0447206497192383 val loss 1.0319163799285889\n",
      "Epoch 7171: train loss 1.0319163799285889 val loss 0.9918985366821289\n",
      "Epoch 7172: train loss 0.9918985366821289 val loss 0.9982271194458008\n",
      "Epoch 7173: train loss 0.9982271194458008 val loss 0.9931638240814209\n",
      "Epoch 7174: train loss 0.9931638240814209 val loss 1.0098295211791992\n",
      "Epoch 7175: train loss 1.0098295211791992 val loss 1.0356897115707397\n",
      "Epoch 7176: train loss 1.0356897115707397 val loss 0.9914364814758301\n",
      "Epoch 7177: train loss 0.9914364814758301 val loss 1.077505350112915\n",
      "Epoch 7178: train loss 1.077505350112915 val loss 1.003975749015808\n",
      "Epoch 7179: train loss 1.003975749015808 val loss 1.0121550559997559\n",
      "Epoch 7180: train loss 1.0121550559997559 val loss 1.0468857288360596\n",
      "Epoch 7181: train loss 1.0468857288360596 val loss 0.9922041893005371\n",
      "Epoch 7182: train loss 0.9922041893005371 val loss 1.0085115432739258\n",
      "Epoch 7183: train loss 1.0085115432739258 val loss 1.014627456665039\n",
      "Epoch 7184: train loss 1.014627456665039 val loss 0.9997632503509521\n",
      "Epoch 7185: train loss 0.9997632503509521 val loss 1.0016604661941528\n",
      "Epoch 7186: train loss 1.0016604661941528 val loss 1.007145643234253\n",
      "Epoch 7187: train loss 1.007145643234253 val loss 1.0149822235107422\n",
      "Epoch 7188: train loss 1.0149822235107422 val loss 0.9913522005081177\n",
      "Epoch 7189: train loss 0.9913522005081177 val loss 1.0370817184448242\n",
      "Epoch 7190: train loss 1.0370817184448242 val loss 1.0210065841674805\n",
      "Epoch 7191: train loss 1.0210065841674805 val loss 1.0010733604431152\n",
      "Epoch 7192: train loss 1.0010733604431152 val loss 1.0384109020233154\n",
      "Epoch 7193: train loss 1.0384109020233154 val loss 0.9966594576835632\n",
      "Epoch 7194: train loss 0.9966594576835632 val loss 1.0089081525802612\n",
      "Epoch 7195: train loss 1.0089081525802612 val loss 1.0331482887268066\n",
      "Epoch 7196: train loss 1.0331482887268066 val loss 0.9967057108879089\n",
      "Epoch 7197: train loss 0.9967057108879089 val loss 1.0054446458816528\n",
      "Epoch 7198: train loss 1.0054446458816528 val loss 1.0386037826538086\n",
      "Epoch 7199: train loss 1.0386037826538086 val loss 0.9942059516906738\n",
      "Epoch 7200: train loss 0.9942059516906738 val loss 1.0307163000106812\n",
      "Epoch 7201: train loss 1.0307163000106812 val loss 1.0078896284103394\n",
      "Epoch 7202: train loss 1.0078896284103394 val loss 0.9929550886154175\n",
      "Epoch 7203: train loss 0.9929550886154175 val loss 0.9988449811935425\n",
      "Epoch 7204: train loss 0.9988449811935425 val loss 0.9906428456306458\n",
      "Epoch 7205: train loss 0.9906428456306458 val loss 1.0030324459075928\n",
      "Epoch 7206: train loss 1.0030324459075928 val loss 1.0399225950241089\n",
      "Epoch 7207: train loss 1.0399225950241089 val loss 0.9901503324508667\n",
      "Epoch 7208: train loss 0.9901503324508667 val loss 1.0894466638565063\n",
      "Epoch 7209: train loss 1.0894466638565063 val loss 0.999269425868988\n",
      "Epoch 7210: train loss 0.999269425868988 val loss 1.0631892681121826\n",
      "Epoch 7211: train loss 1.0631892681121826 val loss 1.0031050443649292\n",
      "Epoch 7212: train loss 1.0031050443649292 val loss 1.0030485391616821\n",
      "Epoch 7213: train loss 1.0030485391616821 val loss 1.0576385259628296\n",
      "Epoch 7214: train loss 1.0576385259628296 val loss 0.9951906204223633\n",
      "Epoch 7215: train loss 0.9951906204223633 val loss 1.0732076168060303\n",
      "Epoch 7216: train loss 1.0732076168060303 val loss 0.9966193437576294\n",
      "Epoch 7217: train loss 0.9966193437576294 val loss 1.0323565006256104\n",
      "Epoch 7218: train loss 1.0323565006256104 val loss 1.0024468898773193\n",
      "Epoch 7219: train loss 1.0024468898773193 val loss 0.9946317672729492\n",
      "Epoch 7220: train loss 0.9946317672729492 val loss 0.9929269552230835\n",
      "Epoch 7221: train loss 0.9929269552230835 val loss 0.9881603717803955\n",
      "Epoch 7222: train loss 0.9881603717803955 val loss 1.010958194732666\n",
      "Epoch 7223: train loss 1.010958194732666 val loss 1.0826609134674072\n",
      "Epoch 7224: train loss 1.0826609134674072 val loss 0.9993712306022644\n",
      "Epoch 7225: train loss 0.9993712306022644 val loss 1.1283361911773682\n",
      "Epoch 7226: train loss 1.1283361911773682 val loss 0.9905116558074951\n",
      "Epoch 7227: train loss 0.9905116558074951 val loss 1.0667153596878052\n",
      "Epoch 7228: train loss 1.0667153596878052 val loss 0.9994590282440186\n",
      "Epoch 7229: train loss 0.9994590282440186 val loss 1.0012669563293457\n",
      "Epoch 7230: train loss 1.0012669563293457 val loss 1.0542575120925903\n",
      "Epoch 7231: train loss 1.0542575120925903 val loss 0.9959635734558105\n",
      "Epoch 7232: train loss 0.9959635734558105 val loss 1.0833207368850708\n",
      "Epoch 7233: train loss 1.0833207368850708 val loss 1.008486270904541\n",
      "Epoch 7234: train loss 1.008486270904541 val loss 1.0460259914398193\n",
      "Epoch 7235: train loss 1.0460259914398193 val loss 1.0216083526611328\n",
      "Epoch 7236: train loss 1.0216083526611328 val loss 0.9965473413467407\n",
      "Epoch 7237: train loss 0.9965473413467407 val loss 1.0031931400299072\n",
      "Epoch 7238: train loss 1.0031931400299072 val loss 0.991858959197998\n",
      "Epoch 7239: train loss 0.991858959197998 val loss 0.99934983253479\n",
      "Epoch 7240: train loss 0.99934983253479 val loss 1.0099163055419922\n",
      "Epoch 7241: train loss 1.0099163055419922 val loss 0.9906662702560425\n",
      "Epoch 7242: train loss 0.9906662702560425 val loss 1.0241607427597046\n",
      "Epoch 7243: train loss 1.0241607427597046 val loss 1.0362850427627563\n",
      "Epoch 7244: train loss 1.0362850427627563 val loss 0.9973069429397583\n",
      "Epoch 7245: train loss 0.9973069429397583 val loss 1.038610577583313\n",
      "Epoch 7246: train loss 1.038610577583313 val loss 0.9959019422531128\n",
      "Epoch 7247: train loss 0.9959019422531128 val loss 1.0074257850646973\n",
      "Epoch 7248: train loss 1.0074257850646973 val loss 1.0309524536132812\n",
      "Epoch 7249: train loss 1.0309524536132812 val loss 0.9953678846359253\n",
      "Epoch 7250: train loss 0.9953678846359253 val loss 1.0052433013916016\n",
      "Epoch 7251: train loss 1.0052433013916016 val loss 1.0329902172088623\n",
      "Epoch 7252: train loss 1.0329902172088623 val loss 0.9937268495559692\n",
      "Epoch 7253: train loss 0.9937268495559692 val loss 0.9998000860214233\n",
      "Epoch 7254: train loss 0.9998000860214233 val loss 0.999065637588501\n",
      "Epoch 7255: train loss 0.999065637588501 val loss 1.001102089881897\n",
      "Epoch 7256: train loss 1.001102089881897 val loss 0.9888994693756104\n",
      "Epoch 7257: train loss 0.9888994693756104 val loss 1.0268222093582153\n",
      "Epoch 7258: train loss 1.0268222093582153 val loss 1.037076711654663\n",
      "Epoch 7259: train loss 1.037076711654663 val loss 0.9932260513305664\n",
      "Epoch 7260: train loss 0.9932260513305664 val loss 1.0507698059082031\n",
      "Epoch 7261: train loss 1.0507698059082031 val loss 0.9925107955932617\n",
      "Epoch 7262: train loss 0.9925107955932617 val loss 1.0273497104644775\n",
      "Epoch 7263: train loss 1.0273497104644775 val loss 1.0363097190856934\n",
      "Epoch 7264: train loss 1.0363097190856934 val loss 0.9874095916748047\n",
      "Epoch 7265: train loss 0.9874095916748047 val loss 0.9981342554092407\n",
      "Epoch 7266: train loss 0.9981342554092407 val loss 0.9876693487167358\n",
      "Epoch 7267: train loss 0.9876693487167358 val loss 1.0131760835647583\n",
      "Epoch 7268: train loss 1.0131760835647583 val loss 1.0309607982635498\n",
      "Epoch 7269: train loss 1.0309607982635498 val loss 0.9877841472625732\n",
      "Epoch 7270: train loss 0.9877841472625732 val loss 1.0624516010284424\n",
      "Epoch 7271: train loss 1.0624516010284424 val loss 1.0083345174789429\n",
      "Epoch 7272: train loss 1.0083345174789429 val loss 1.0007065534591675\n",
      "Epoch 7273: train loss 1.0007065534591675 val loss 1.065826416015625\n",
      "Epoch 7274: train loss 1.065826416015625 val loss 0.9888688921928406\n",
      "Epoch 7275: train loss 0.9888688921928406 val loss 1.0298528671264648\n",
      "Epoch 7276: train loss 1.0298528671264648 val loss 1.0158536434173584\n",
      "Epoch 7277: train loss 1.0158536434173584 val loss 0.9905300736427307\n",
      "Epoch 7278: train loss 0.9905300736427307 val loss 1.010316014289856\n",
      "Epoch 7279: train loss 1.010316014289856 val loss 1.0103394985198975\n",
      "Epoch 7280: train loss 1.0103394985198975 val loss 0.9942470788955688\n",
      "Epoch 7281: train loss 0.9942470788955688 val loss 1.000282883644104\n",
      "Epoch 7282: train loss 1.000282883644104 val loss 1.0238988399505615\n",
      "Epoch 7283: train loss 1.0238988399505615 val loss 0.9997488260269165\n",
      "Epoch 7284: train loss 0.9997488260269165 val loss 1.0037429332733154\n",
      "Epoch 7285: train loss 1.0037429332733154 val loss 1.0531353950500488\n",
      "Epoch 7286: train loss 1.0531353950500488 val loss 0.9865404367446899\n",
      "Epoch 7287: train loss 0.9865404367446899 val loss 1.0024346113204956\n",
      "Epoch 7288: train loss 1.0024346113204956 val loss 0.9934930205345154\n",
      "Epoch 7289: train loss 0.9934930205345154 val loss 0.9903389811515808\n",
      "Epoch 7290: train loss 0.9903389811515808 val loss 0.997990071773529\n",
      "Epoch 7291: train loss 0.997990071773529 val loss 0.9903447031974792\n",
      "Epoch 7292: train loss 0.9903447031974792 val loss 0.9996987581253052\n",
      "Epoch 7293: train loss 0.9996987581253052 val loss 0.9851324558258057\n",
      "Epoch 7294: train loss 0.9851324558258057 val loss 1.0260006189346313\n",
      "Epoch 7295: train loss 1.0260006189346313 val loss 1.0450986623764038\n",
      "Epoch 7296: train loss 1.0450986623764038 val loss 0.9919756650924683\n",
      "Epoch 7297: train loss 0.9919756650924683 val loss 1.0763816833496094\n",
      "Epoch 7298: train loss 1.0763816833496094 val loss 0.9872463941574097\n",
      "Epoch 7299: train loss 0.9872463941574097 val loss 1.0362462997436523\n",
      "Epoch 7300: train loss 1.0362462997436523 val loss 1.0010713338851929\n",
      "Epoch 7301: train loss 1.0010713338851929 val loss 0.989128828048706\n",
      "Epoch 7302: train loss 0.989128828048706 val loss 0.9968655705451965\n",
      "Epoch 7303: train loss 0.9968655705451965 val loss 0.989912211894989\n",
      "Epoch 7304: train loss 0.989912211894989 val loss 0.9952186346054077\n",
      "Epoch 7305: train loss 0.9952186346054077 val loss 0.9892140626907349\n",
      "Epoch 7306: train loss 0.9892140626907349 val loss 0.9903849959373474\n",
      "Epoch 7307: train loss 0.9903849959373474 val loss 0.9961851835250854\n",
      "Epoch 7308: train loss 0.9961851835250854 val loss 0.9877339601516724\n",
      "Epoch 7309: train loss 0.9877339601516724 val loss 0.9988868236541748\n",
      "Epoch 7310: train loss 0.9988868236541748 val loss 1.0395658016204834\n",
      "Epoch 7311: train loss 1.0395658016204834 val loss 0.9865334033966064\n",
      "Epoch 7312: train loss 0.9865334033966064 val loss 1.05647611618042\n",
      "Epoch 7313: train loss 1.05647611618042 val loss 0.9918553829193115\n",
      "Epoch 7314: train loss 0.9918553829193115 val loss 1.012192964553833\n",
      "Epoch 7315: train loss 1.012192964553833 val loss 1.015067219734192\n",
      "Epoch 7316: train loss 1.015067219734192 val loss 0.9942906498908997\n",
      "Epoch 7317: train loss 0.9942906498908997 val loss 0.9931372404098511\n",
      "Epoch 7318: train loss 0.9931372404098511 val loss 0.9977936744689941\n",
      "Epoch 7319: train loss 0.9977936744689941 val loss 0.9959256052970886\n",
      "Epoch 7320: train loss 0.9959256052970886 val loss 0.9889432191848755\n",
      "Epoch 7321: train loss 0.9889432191848755 val loss 0.9881092309951782\n",
      "Epoch 7322: train loss 0.9881092309951782 val loss 0.9927990436553955\n",
      "Epoch 7323: train loss 0.9927990436553955 val loss 0.9871786832809448\n",
      "Epoch 7324: train loss 0.9871786832809448 val loss 0.99871826171875\n",
      "Epoch 7325: train loss 0.99871826171875 val loss 1.0267319679260254\n",
      "Epoch 7326: train loss 1.0267319679260254 val loss 0.9837164282798767\n",
      "Epoch 7327: train loss 0.9837164282798767 val loss 1.028350830078125\n",
      "Epoch 7328: train loss 1.028350830078125 val loss 1.0387914180755615\n",
      "Epoch 7329: train loss 1.0387914180755615 val loss 0.9920068383216858\n",
      "Epoch 7330: train loss 0.9920068383216858 val loss 1.05995774269104\n",
      "Epoch 7331: train loss 1.05995774269104 val loss 0.9922518730163574\n",
      "Epoch 7332: train loss 0.9922518730163574 val loss 1.0297396183013916\n",
      "Epoch 7333: train loss 1.0297396183013916 val loss 1.030569314956665\n",
      "Epoch 7334: train loss 1.030569314956665 val loss 0.9859085083007812\n",
      "Epoch 7335: train loss 0.9859085083007812 val loss 1.0168954133987427\n",
      "Epoch 7336: train loss 1.0168954133987427 val loss 0.9843084216117859\n",
      "Epoch 7337: train loss 0.9843084216117859 val loss 1.0331897735595703\n",
      "Epoch 7338: train loss 1.0331897735595703 val loss 1.0356141328811646\n",
      "Epoch 7339: train loss 1.0356141328811646 val loss 0.9915182590484619\n",
      "Epoch 7340: train loss 0.9915182590484619 val loss 1.0264835357666016\n",
      "Epoch 7341: train loss 1.0264835357666016 val loss 1.0074796676635742\n",
      "Epoch 7342: train loss 1.0074796676635742 val loss 0.9887266159057617\n",
      "Epoch 7343: train loss 0.9887266159057617 val loss 1.0144550800323486\n",
      "Epoch 7344: train loss 1.0144550800323486 val loss 1.0230653285980225\n",
      "Epoch 7345: train loss 1.0230653285980225 val loss 0.9843646287918091\n",
      "Epoch 7346: train loss 0.9843646287918091 val loss 1.0598211288452148\n",
      "Epoch 7347: train loss 1.0598211288452148 val loss 1.0128871202468872\n",
      "Epoch 7348: train loss 1.0128871202468872 val loss 1.0120035409927368\n",
      "Epoch 7349: train loss 1.0120035409927368 val loss 1.0442469120025635\n",
      "Epoch 7350: train loss 1.0442469120025635 val loss 0.9893172979354858\n",
      "Epoch 7351: train loss 0.9893172979354858 val loss 1.0052235126495361\n",
      "Epoch 7352: train loss 1.0052235126495361 val loss 1.0116499662399292\n",
      "Epoch 7353: train loss 1.0116499662399292 val loss 0.9887074828147888\n",
      "Epoch 7354: train loss 0.9887074828147888 val loss 0.9911050796508789\n",
      "Epoch 7355: train loss 0.9911050796508789 val loss 0.9947011470794678\n",
      "Epoch 7356: train loss 0.9947011470794678 val loss 0.9952791929244995\n",
      "Epoch 7357: train loss 0.9952791929244995 val loss 0.9816716909408569\n",
      "Epoch 7358: train loss 0.9816716909408569 val loss 0.9829615354537964\n",
      "Epoch 7359: train loss 0.9829615354537964 val loss 0.9865310192108154\n",
      "Epoch 7360: train loss 0.9865310192108154 val loss 0.9863773584365845\n",
      "Epoch 7361: train loss 0.9863773584365845 val loss 0.98187655210495\n",
      "Epoch 7362: train loss 0.98187655210495 val loss 0.9820115566253662\n",
      "Epoch 7363: train loss 0.9820115566253662 val loss 0.9900608062744141\n",
      "Epoch 7364: train loss 0.9900608062744141 val loss 0.9822501540184021\n",
      "Epoch 7365: train loss 0.9822501540184021 val loss 1.0262038707733154\n",
      "Epoch 7366: train loss 1.0262038707733154 val loss 1.028854489326477\n",
      "Epoch 7367: train loss 1.028854489326477 val loss 0.9852603673934937\n",
      "Epoch 7368: train loss 0.9852603673934937 val loss 1.0423825979232788\n",
      "Epoch 7369: train loss 1.0423825979232788 val loss 0.9930727481842041\n",
      "Epoch 7370: train loss 0.9930727481842041 val loss 1.0052167177200317\n",
      "Epoch 7371: train loss 1.0052167177200317 val loss 1.0304831266403198\n",
      "Epoch 7372: train loss 1.0304831266403198 val loss 0.9877516031265259\n",
      "Epoch 7373: train loss 0.9877516031265259 val loss 0.9977221488952637\n",
      "Epoch 7374: train loss 0.9977221488952637 val loss 1.0133607387542725\n",
      "Epoch 7375: train loss 1.0133607387542725 val loss 1.0002872943878174\n",
      "Epoch 7376: train loss 1.0002872943878174 val loss 0.9862374067306519\n",
      "Epoch 7377: train loss 0.9862374067306519 val loss 0.9980316758155823\n",
      "Epoch 7378: train loss 0.9980316758155823 val loss 1.0284556150436401\n",
      "Epoch 7379: train loss 1.0284556150436401 val loss 0.9814842343330383\n",
      "Epoch 7380: train loss 0.9814842343330383 val loss 1.0661602020263672\n",
      "Epoch 7381: train loss 1.0661602020263672 val loss 1.0034443140029907\n",
      "Epoch 7382: train loss 1.0034443140029907 val loss 0.997896671295166\n",
      "Epoch 7383: train loss 0.997896671295166 val loss 1.0597366094589233\n",
      "Epoch 7384: train loss 1.0597366094589233 val loss 0.9815230369567871\n",
      "Epoch 7385: train loss 0.9815230369567871 val loss 1.0835630893707275\n",
      "Epoch 7386: train loss 1.0835630893707275 val loss 0.9859753847122192\n",
      "Epoch 7387: train loss 0.9859753847122192 val loss 1.0770618915557861\n",
      "Epoch 7388: train loss 1.0770618915557861 val loss 0.9960967898368835\n",
      "Epoch 7389: train loss 0.9960967898368835 val loss 1.0418314933776855\n",
      "Epoch 7390: train loss 1.0418314933776855 val loss 1.0133016109466553\n",
      "Epoch 7391: train loss 1.0133016109466553 val loss 0.983578085899353\n",
      "Epoch 7392: train loss 0.983578085899353 val loss 0.9995584487915039\n",
      "Epoch 7393: train loss 0.9995584487915039 val loss 0.9920685291290283\n",
      "Epoch 7394: train loss 0.9920685291290283 val loss 0.9885523319244385\n",
      "Epoch 7395: train loss 0.9885523319244385 val loss 0.9845664501190186\n",
      "Epoch 7396: train loss 0.9845664501190186 val loss 0.9830670952796936\n",
      "Epoch 7397: train loss 0.9830670952796936 val loss 0.9843345284461975\n",
      "Epoch 7398: train loss 0.9843345284461975 val loss 0.9845240116119385\n",
      "Epoch 7399: train loss 0.9845240116119385 val loss 0.9810787439346313\n",
      "Epoch 7400: train loss 0.9810787439346313 val loss 0.9796390533447266\n",
      "Epoch 7401: train loss 0.9796390533447266 val loss 0.9885662794113159\n",
      "Epoch 7402: train loss 0.9885662794113159 val loss 1.0229287147521973\n",
      "Epoch 7403: train loss 1.0229287147521973 val loss 0.9836879968643188\n",
      "Epoch 7404: train loss 0.9836879968643188 val loss 0.9827920198440552\n",
      "Epoch 7405: train loss 0.9827920198440552 val loss 0.9879765510559082\n",
      "Epoch 7406: train loss 0.9879765510559082 val loss 0.9823364019393921\n",
      "Epoch 7407: train loss 0.9823364019393921 val loss 0.9959276914596558\n",
      "Epoch 7408: train loss 0.9959276914596558 val loss 1.0414294004440308\n",
      "Epoch 7409: train loss 1.0414294004440308 val loss 0.9818867444992065\n",
      "Epoch 7410: train loss 0.9818867444992065 val loss 1.0699042081832886\n",
      "Epoch 7411: train loss 1.0699042081832886 val loss 0.9892373085021973\n",
      "Epoch 7412: train loss 0.9892373085021973 val loss 1.0239595174789429\n",
      "Epoch 7413: train loss 1.0239595174789429 val loss 1.0413002967834473\n",
      "Epoch 7414: train loss 1.0413002967834473 val loss 0.9804703593254089\n",
      "Epoch 7415: train loss 0.9804703593254089 val loss 1.0443824529647827\n",
      "Epoch 7416: train loss 1.0443824529647827 val loss 0.980201780796051\n",
      "Epoch 7417: train loss 0.980201780796051 val loss 1.1007790565490723\n",
      "Epoch 7418: train loss 1.1007790565490723 val loss 0.9974098205566406\n",
      "Epoch 7419: train loss 0.9974098205566406 val loss 1.0149624347686768\n",
      "Epoch 7420: train loss 1.0149624347686768 val loss 1.0091183185577393\n",
      "Epoch 7421: train loss 1.0091183185577393 val loss 0.9920758008956909\n",
      "Epoch 7422: train loss 0.9920758008956909 val loss 0.9949392080307007\n",
      "Epoch 7423: train loss 0.9949392080307007 val loss 1.017796516418457\n",
      "Epoch 7424: train loss 1.017796516418457 val loss 0.9954698085784912\n",
      "Epoch 7425: train loss 0.9954698085784912 val loss 1.0050866603851318\n",
      "Epoch 7426: train loss 1.0050866603851318 val loss 1.0330044031143188\n",
      "Epoch 7427: train loss 1.0330044031143188 val loss 0.9824650287628174\n",
      "Epoch 7428: train loss 0.9824650287628174 val loss 1.0029351711273193\n",
      "Epoch 7429: train loss 1.0029351711273193 val loss 0.9909656047821045\n",
      "Epoch 7430: train loss 0.9909656047821045 val loss 0.9935427308082581\n",
      "Epoch 7431: train loss 0.9935427308082581 val loss 0.982979953289032\n",
      "Epoch 7432: train loss 0.982979953289032 val loss 0.9931408166885376\n",
      "Epoch 7433: train loss 0.9931408166885376 val loss 1.0013355016708374\n",
      "Epoch 7434: train loss 1.0013355016708374 val loss 0.9792699813842773\n",
      "Epoch 7435: train loss 0.9792699813842773 val loss 1.0371861457824707\n",
      "Epoch 7436: train loss 1.0371861457824707 val loss 1.0036202669143677\n",
      "Epoch 7437: train loss 1.0036202669143677 val loss 0.9864146709442139\n",
      "Epoch 7438: train loss 0.9864146709442139 val loss 1.0318992137908936\n",
      "Epoch 7439: train loss 1.0318992137908936 val loss 0.9893254041671753\n",
      "Epoch 7440: train loss 0.9893254041671753 val loss 1.0040395259857178\n",
      "Epoch 7441: train loss 1.0040395259857178 val loss 1.0164719820022583\n",
      "Epoch 7442: train loss 1.0164719820022583 val loss 0.9876383543014526\n",
      "Epoch 7443: train loss 0.9876383543014526 val loss 0.9916603565216064\n",
      "Epoch 7444: train loss 0.9916603565216064 val loss 1.0045510530471802\n",
      "Epoch 7445: train loss 1.0045510530471802 val loss 1.0120500326156616\n",
      "Epoch 7446: train loss 1.0120500326156616 val loss 0.9800758361816406\n",
      "Epoch 7447: train loss 0.9800758361816406 val loss 1.032304048538208\n",
      "Epoch 7448: train loss 1.032304048538208 val loss 1.00627601146698\n",
      "Epoch 7449: train loss 1.00627601146698 val loss 0.9912995100021362\n",
      "Epoch 7450: train loss 0.9912995100021362 val loss 1.0348016023635864\n",
      "Epoch 7451: train loss 1.0348016023635864 val loss 0.9828239679336548\n",
      "Epoch 7452: train loss 0.9828239679336548 val loss 1.0269546508789062\n",
      "Epoch 7453: train loss 1.0269546508789062 val loss 1.0144727230072021\n",
      "Epoch 7454: train loss 1.0144727230072021 val loss 0.9816648960113525\n",
      "Epoch 7455: train loss 0.9816648960113525 val loss 0.9949852228164673\n",
      "Epoch 7456: train loss 0.9949852228164673 val loss 0.9802577495574951\n",
      "Epoch 7457: train loss 0.9802577495574951 val loss 1.0049172639846802\n",
      "Epoch 7458: train loss 1.0049172639846802 val loss 1.068747878074646\n",
      "Epoch 7459: train loss 1.068747878074646 val loss 0.9868736267089844\n",
      "Epoch 7460: train loss 0.9868736267089844 val loss 1.0902490615844727\n",
      "Epoch 7461: train loss 1.0902490615844727 val loss 0.9849773645401001\n",
      "Epoch 7462: train loss 0.9849773645401001 val loss 1.053445816040039\n",
      "Epoch 7463: train loss 1.053445816040039 val loss 0.9847009181976318\n",
      "Epoch 7464: train loss 0.9847009181976318 val loss 0.9792760610580444\n",
      "Epoch 7465: train loss 0.9792760610580444 val loss 0.9897645711898804\n",
      "Epoch 7466: train loss 0.9897645711898804 val loss 0.979604959487915\n",
      "Epoch 7467: train loss 0.979604959487915 val loss 0.9784776568412781\n",
      "Epoch 7468: train loss 0.9784776568412781 val loss 0.981277346611023\n",
      "Epoch 7469: train loss 0.981277346611023 val loss 0.9807448387145996\n",
      "Epoch 7470: train loss 0.9807448387145996 val loss 0.9807872772216797\n",
      "Epoch 7471: train loss 0.9807872772216797 val loss 0.994080662727356\n",
      "Epoch 7472: train loss 0.994080662727356 val loss 0.9861324429512024\n",
      "Epoch 7473: train loss 0.9861324429512024 val loss 0.9833182096481323\n",
      "Epoch 7474: train loss 0.9833182096481323 val loss 0.9880676865577698\n",
      "Epoch 7475: train loss 0.9880676865577698 val loss 0.9846481084823608\n",
      "Epoch 7476: train loss 0.9846481084823608 val loss 0.9829471111297607\n",
      "Epoch 7477: train loss 0.9829471111297607 val loss 0.9874081611633301\n",
      "Epoch 7478: train loss 0.9874081611633301 val loss 0.9959570169448853\n",
      "Epoch 7479: train loss 0.9959570169448853 val loss 1.0281519889831543\n",
      "Epoch 7480: train loss 1.0281519889831543 val loss 0.9782689809799194\n",
      "Epoch 7481: train loss 0.9782689809799194 val loss 1.04643976688385\n",
      "Epoch 7482: train loss 1.04643976688385 val loss 0.9915173649787903\n",
      "Epoch 7483: train loss 0.9915173649787903 val loss 1.0053331851959229\n",
      "Epoch 7484: train loss 1.0053331851959229 val loss 1.0203908681869507\n",
      "Epoch 7485: train loss 1.0203908681869507 val loss 0.9811848402023315\n",
      "Epoch 7486: train loss 0.9811848402023315 val loss 1.0095345973968506\n",
      "Epoch 7487: train loss 1.0095345973968506 val loss 0.9880719184875488\n",
      "Epoch 7488: train loss 0.9880719184875488 val loss 0.992908239364624\n",
      "Epoch 7489: train loss 0.992908239364624 val loss 0.9785782098770142\n",
      "Epoch 7490: train loss 0.9785782098770142 val loss 0.996837854385376\n",
      "Epoch 7491: train loss 0.996837854385376 val loss 1.0484107732772827\n",
      "Epoch 7492: train loss 1.0484107732772827 val loss 0.9847080707550049\n",
      "Epoch 7493: train loss 0.9847080707550049 val loss 1.0756312608718872\n",
      "Epoch 7494: train loss 1.0756312608718872 val loss 0.986936092376709\n",
      "Epoch 7495: train loss 0.986936092376709 val loss 1.027928113937378\n",
      "Epoch 7496: train loss 1.027928113937378 val loss 1.0276589393615723\n",
      "Epoch 7497: train loss 1.0276589393615723 val loss 0.9762230515480042\n",
      "Epoch 7498: train loss 0.9762230515480042 val loss 0.9847413897514343\n",
      "Epoch 7499: train loss 0.9847413897514343 val loss 0.9751033186912537\n",
      "Epoch 7500: train loss 0.9751033186912537 val loss 1.013871192932129\n",
      "Epoch 7501: train loss 1.013871192932129 val loss 1.0458996295928955\n",
      "Epoch 7502: train loss 1.0458996295928955 val loss 0.9885554313659668\n",
      "Epoch 7503: train loss 0.9885554313659668 val loss 1.0558085441589355\n",
      "Epoch 7504: train loss 1.0558085441589355 val loss 0.9816749095916748\n",
      "Epoch 7505: train loss 0.9816749095916748 val loss 1.0089092254638672\n",
      "Epoch 7506: train loss 1.0089092254638672 val loss 0.986782431602478\n",
      "Epoch 7507: train loss 0.986782431602478 val loss 0.9945399761199951\n",
      "Epoch 7508: train loss 0.9945399761199951 val loss 0.9763984680175781\n",
      "Epoch 7509: train loss 0.9763984680175781 val loss 1.0090234279632568\n",
      "Epoch 7510: train loss 1.0090234279632568 val loss 1.0416046380996704\n",
      "Epoch 7511: train loss 1.0416046380996704 val loss 0.9822406768798828\n",
      "Epoch 7512: train loss 0.9822406768798828 val loss 1.0795037746429443\n",
      "Epoch 7513: train loss 1.0795037746429443 val loss 0.9825814962387085\n",
      "Epoch 7514: train loss 0.9825814962387085 val loss 1.0195996761322021\n",
      "Epoch 7515: train loss 1.0195996761322021 val loss 1.0351486206054688\n",
      "Epoch 7516: train loss 1.0351486206054688 val loss 0.9756687879562378\n",
      "Epoch 7517: train loss 0.9756687879562378 val loss 0.9988318681716919\n",
      "Epoch 7518: train loss 0.9988318681716919 val loss 0.9836311340332031\n",
      "Epoch 7519: train loss 0.9836311340332031 val loss 0.9811433553695679\n",
      "Epoch 7520: train loss 0.9811433553695679 val loss 0.9796555042266846\n",
      "Epoch 7521: train loss 0.9796555042266846 val loss 0.9813995361328125\n",
      "Epoch 7522: train loss 0.9813995361328125 val loss 0.9793556332588196\n",
      "Epoch 7523: train loss 0.9793556332588196 val loss 0.9797263145446777\n",
      "Epoch 7524: train loss 0.9797263145446777 val loss 0.9750684499740601\n",
      "Epoch 7525: train loss 0.9750684499740601 val loss 1.0117747783660889\n",
      "Epoch 7526: train loss 1.0117747783660889 val loss 1.036617636680603\n",
      "Epoch 7527: train loss 1.036617636680603 val loss 0.9800020456314087\n",
      "Epoch 7528: train loss 0.9800020456314087 val loss 1.0455683469772339\n",
      "Epoch 7529: train loss 1.0455683469772339 val loss 0.9796010255813599\n",
      "Epoch 7530: train loss 0.9796010255813599 val loss 1.0255041122436523\n",
      "Epoch 7531: train loss 1.0255041122436523 val loss 1.0102382898330688\n",
      "Epoch 7532: train loss 1.0102382898330688 val loss 0.9785417914390564\n",
      "Epoch 7533: train loss 0.9785417914390564 val loss 0.9931849241256714\n",
      "Epoch 7534: train loss 0.9931849241256714 val loss 1.002065658569336\n",
      "Epoch 7535: train loss 1.002065658569336 val loss 0.9868552684783936\n",
      "Epoch 7536: train loss 0.9868552684783936 val loss 0.9960371255874634\n",
      "Epoch 7537: train loss 0.9960371255874634 val loss 1.0155181884765625\n",
      "Epoch 7538: train loss 1.0155181884765625 val loss 0.9867329597473145\n",
      "Epoch 7539: train loss 0.9867329597473145 val loss 0.9970327615737915\n",
      "Epoch 7540: train loss 0.9970327615737915 val loss 1.028172492980957\n",
      "Epoch 7541: train loss 1.028172492980957 val loss 0.9778931736946106\n",
      "Epoch 7542: train loss 0.9778931736946106 val loss 1.0027313232421875\n",
      "Epoch 7543: train loss 1.0027313232421875 val loss 0.9913017153739929\n",
      "Epoch 7544: train loss 0.9913017153739929 val loss 0.9874124526977539\n",
      "Epoch 7545: train loss 0.9874124526977539 val loss 0.9806567430496216\n",
      "Epoch 7546: train loss 0.9806567430496216 val loss 1.0086421966552734\n",
      "Epoch 7547: train loss 1.0086421966552734 val loss 1.0067760944366455\n",
      "Epoch 7548: train loss 1.0067760944366455 val loss 0.9760594964027405\n",
      "Epoch 7549: train loss 0.9760594964027405 val loss 1.0205967426300049\n",
      "Epoch 7550: train loss 1.0205967426300049 val loss 1.0048763751983643\n",
      "Epoch 7551: train loss 1.0048763751983643 val loss 0.977023720741272\n",
      "Epoch 7552: train loss 0.977023720741272 val loss 1.0168086290359497\n",
      "Epoch 7553: train loss 1.0168086290359497 val loss 1.005001187324524\n",
      "Epoch 7554: train loss 1.005001187324524 val loss 0.9848114252090454\n",
      "Epoch 7555: train loss 0.9848114252090454 val loss 1.0070217847824097\n",
      "Epoch 7556: train loss 1.0070217847824097 val loss 0.9996185302734375\n",
      "Epoch 7557: train loss 0.9996185302734375 val loss 0.9816098213195801\n",
      "Epoch 7558: train loss 0.9816098213195801 val loss 1.018485188484192\n",
      "Epoch 7559: train loss 1.018485188484192 val loss 0.9948608875274658\n",
      "Epoch 7560: train loss 0.9948608875274658 val loss 0.9808334112167358\n",
      "Epoch 7561: train loss 0.9808334112167358 val loss 1.011676549911499\n",
      "Epoch 7562: train loss 1.011676549911499 val loss 0.9998946189880371\n",
      "Epoch 7563: train loss 0.9998946189880371 val loss 0.9787591695785522\n",
      "Epoch 7564: train loss 0.9787591695785522 val loss 1.0198643207550049\n",
      "Epoch 7565: train loss 1.0198643207550049 val loss 0.9969782829284668\n",
      "Epoch 7566: train loss 0.9969782829284668 val loss 0.978050947189331\n",
      "Epoch 7567: train loss 0.978050947189331 val loss 0.9930267333984375\n",
      "Epoch 7568: train loss 0.9930267333984375 val loss 1.0368854999542236\n",
      "Epoch 7569: train loss 1.0368854999542236 val loss 0.9768615961074829\n",
      "Epoch 7570: train loss 0.9768615961074829 val loss 1.0426716804504395\n",
      "Epoch 7571: train loss 1.0426716804504395 val loss 1.0003242492675781\n",
      "Epoch 7572: train loss 1.0003242492675781 val loss 0.9899793863296509\n",
      "Epoch 7573: train loss 0.9899793863296509 val loss 1.0497658252716064\n",
      "Epoch 7574: train loss 1.0497658252716064 val loss 0.9778224229812622\n",
      "Epoch 7575: train loss 0.9778224229812622 val loss 1.002113938331604\n",
      "Epoch 7576: train loss 1.002113938331604 val loss 0.9931776523590088\n",
      "Epoch 7577: train loss 0.9931776523590088 val loss 0.9970294237136841\n",
      "Epoch 7578: train loss 0.9970294237136841 val loss 0.9741630554199219\n",
      "Epoch 7579: train loss 0.9741630554199219 val loss 1.0250605344772339\n",
      "Epoch 7580: train loss 1.0250605344772339 val loss 1.004311203956604\n",
      "Epoch 7581: train loss 1.004311203956604 val loss 0.9763688445091248\n",
      "Epoch 7582: train loss 0.9763688445091248 val loss 1.012401819229126\n",
      "Epoch 7583: train loss 1.012401819229126 val loss 1.0036585330963135\n",
      "Epoch 7584: train loss 1.0036585330963135 val loss 0.9757165908813477\n",
      "Epoch 7585: train loss 0.9757165908813477 val loss 1.0143115520477295\n",
      "Epoch 7586: train loss 1.0143115520477295 val loss 1.0149426460266113\n",
      "Epoch 7587: train loss 1.0149426460266113 val loss 0.9772604703903198\n",
      "Epoch 7588: train loss 0.9772604703903198 val loss 1.0074766874313354\n",
      "Epoch 7589: train loss 1.0074766874313354 val loss 1.019008755683899\n",
      "Epoch 7590: train loss 1.019008755683899 val loss 0.9770736694335938\n",
      "Epoch 7591: train loss 0.9770736694335938 val loss 1.017311930656433\n",
      "Epoch 7592: train loss 1.017311930656433 val loss 1.0048635005950928\n",
      "Epoch 7593: train loss 1.0048635005950928 val loss 0.9871786832809448\n",
      "Epoch 7594: train loss 0.9871786832809448 val loss 1.0024535655975342\n",
      "Epoch 7595: train loss 1.0024535655975342 val loss 0.9837692379951477\n",
      "Epoch 7596: train loss 0.9837692379951477 val loss 0.9911174774169922\n",
      "Epoch 7597: train loss 0.9911174774169922 val loss 1.0017476081848145\n",
      "Epoch 7598: train loss 1.0017476081848145 val loss 0.9929819107055664\n",
      "Epoch 7599: train loss 0.9929819107055664 val loss 0.9753745198249817\n",
      "Epoch 7600: train loss 0.9753745198249817 val loss 1.001679539680481\n",
      "Epoch 7601: train loss 1.001679539680481 val loss 1.0260602235794067\n",
      "Epoch 7602: train loss 1.0260602235794067 val loss 0.9754890203475952\n",
      "Epoch 7603: train loss 0.9754890203475952 val loss 1.0311980247497559\n",
      "Epoch 7604: train loss 1.0311980247497559 val loss 0.9850553274154663\n",
      "Epoch 7605: train loss 0.9850553274154663 val loss 1.00882887840271\n",
      "Epoch 7606: train loss 1.00882887840271 val loss 1.0587271451950073\n",
      "Epoch 7607: train loss 1.0587271451950073 val loss 0.9741040468215942\n",
      "Epoch 7608: train loss 0.9741040468215942 val loss 1.0155532360076904\n",
      "Epoch 7609: train loss 1.0155532360076904 val loss 0.9829119443893433\n",
      "Epoch 7610: train loss 0.9829119443893433 val loss 0.9727975130081177\n",
      "Epoch 7611: train loss 0.9727975130081177 val loss 0.9733891487121582\n",
      "Epoch 7612: train loss 0.9733891487121582 val loss 0.975865364074707\n",
      "Epoch 7613: train loss 0.975865364074707 val loss 0.9905344247817993\n",
      "Epoch 7614: train loss 0.9905344247817993 val loss 0.9726742506027222\n",
      "Epoch 7615: train loss 0.9726742506027222 val loss 0.9759875535964966\n",
      "Epoch 7616: train loss 0.9759875535964966 val loss 1.031253695487976\n",
      "Epoch 7617: train loss 1.031253695487976 val loss 0.9731982946395874\n",
      "Epoch 7618: train loss 0.9731982946395874 val loss 1.0601553916931152\n",
      "Epoch 7619: train loss 1.0601553916931152 val loss 0.9860988855361938\n",
      "Epoch 7620: train loss 0.9860988855361938 val loss 1.0349206924438477\n",
      "Epoch 7621: train loss 1.0349206924438477 val loss 1.0026534795761108\n",
      "Epoch 7622: train loss 1.0026534795761108 val loss 0.9746342897415161\n",
      "Epoch 7623: train loss 0.9746342897415161 val loss 0.9838533997535706\n",
      "Epoch 7624: train loss 0.9838533997535706 val loss 0.972292423248291\n",
      "Epoch 7625: train loss 0.972292423248291 val loss 1.0010117292404175\n",
      "Epoch 7626: train loss 1.0010117292404175 val loss 1.0502760410308838\n",
      "Epoch 7627: train loss 1.0502760410308838 val loss 0.9890227913856506\n",
      "Epoch 7628: train loss 0.9890227913856506 val loss 1.0915908813476562\n",
      "Epoch 7629: train loss 1.0915908813476562 val loss 0.972974419593811\n",
      "Epoch 7630: train loss 0.972974419593811 val loss 1.032665729522705\n",
      "Epoch 7631: train loss 1.032665729522705 val loss 0.9773611426353455\n",
      "Epoch 7632: train loss 0.9773611426353455 val loss 0.970962643623352\n",
      "Epoch 7633: train loss 0.970962643623352 val loss 0.9770926237106323\n",
      "Epoch 7634: train loss 0.9770926237106323 val loss 0.9717098474502563\n",
      "Epoch 7635: train loss 0.9717098474502563 val loss 0.9903924465179443\n",
      "Epoch 7636: train loss 0.9903924465179443 val loss 1.044449806213379\n",
      "Epoch 7637: train loss 1.044449806213379 val loss 0.9799480438232422\n",
      "Epoch 7638: train loss 0.9799480438232422 val loss 1.0537551641464233\n",
      "Epoch 7639: train loss 1.0537551641464233 val loss 0.976531982421875\n",
      "Epoch 7640: train loss 0.976531982421875 val loss 1.0044000148773193\n",
      "Epoch 7641: train loss 1.0044000148773193 val loss 1.013215184211731\n",
      "Epoch 7642: train loss 1.013215184211731 val loss 0.9761936664581299\n",
      "Epoch 7643: train loss 0.9761936664581299 val loss 0.9818379878997803\n",
      "Epoch 7644: train loss 0.9818379878997803 val loss 0.9794493913650513\n",
      "Epoch 7645: train loss 0.9794493913650513 val loss 0.9815033674240112\n",
      "Epoch 7646: train loss 0.9815033674240112 val loss 0.975088357925415\n",
      "Epoch 7647: train loss 0.975088357925415 val loss 0.9770855903625488\n",
      "Epoch 7648: train loss 0.9770855903625488 val loss 0.9818462133407593\n",
      "Epoch 7649: train loss 0.9818462133407593 val loss 0.9741101264953613\n",
      "Epoch 7650: train loss 0.9741101264953613 val loss 0.9885947108268738\n",
      "Epoch 7651: train loss 0.9885947108268738 val loss 1.0447454452514648\n",
      "Epoch 7652: train loss 1.0447454452514648 val loss 0.9785676002502441\n",
      "Epoch 7653: train loss 0.9785676002502441 val loss 1.0710009336471558\n",
      "Epoch 7654: train loss 1.0710009336471558 val loss 0.9806742668151855\n",
      "Epoch 7655: train loss 0.9806742668151855 val loss 1.0343502759933472\n",
      "Epoch 7656: train loss 1.0343502759933472 val loss 1.0062199831008911\n",
      "Epoch 7657: train loss 1.0062199831008911 val loss 0.9712446928024292\n",
      "Epoch 7658: train loss 0.9712446928024292 val loss 0.9800180196762085\n",
      "Epoch 7659: train loss 0.9800180196762085 val loss 0.9709644317626953\n",
      "Epoch 7660: train loss 0.9709644317626953 val loss 0.9725074172019958\n",
      "Epoch 7661: train loss 0.9725074172019958 val loss 0.9862896203994751\n",
      "Epoch 7662: train loss 0.9862896203994751 val loss 0.9693204164505005\n",
      "Epoch 7663: train loss 0.9693204164505005 val loss 1.0104483366012573\n",
      "Epoch 7664: train loss 1.0104483366012573 val loss 1.0333812236785889\n",
      "Epoch 7665: train loss 1.0333812236785889 val loss 0.981890082359314\n",
      "Epoch 7666: train loss 0.981890082359314 val loss 1.048609733581543\n",
      "Epoch 7667: train loss 1.048609733581543 val loss 0.976603627204895\n",
      "Epoch 7668: train loss 0.976603627204895 val loss 1.0088809728622437\n",
      "Epoch 7669: train loss 1.0088809728622437 val loss 1.0004246234893799\n",
      "Epoch 7670: train loss 1.0004246234893799 val loss 0.9784297943115234\n",
      "Epoch 7671: train loss 0.9784297943115234 val loss 0.9769384264945984\n",
      "Epoch 7672: train loss 0.9769384264945984 val loss 0.9918452501296997\n",
      "Epoch 7673: train loss 0.9918452501296997 val loss 1.0179574489593506\n",
      "Epoch 7674: train loss 1.0179574489593506 val loss 0.9704952239990234\n",
      "Epoch 7675: train loss 0.9704952239990234 val loss 1.047691822052002\n",
      "Epoch 7676: train loss 1.047691822052002 val loss 0.9974932670593262\n",
      "Epoch 7677: train loss 0.9974932670593262 val loss 0.9920048713684082\n",
      "Epoch 7678: train loss 0.9920048713684082 val loss 1.050990343093872\n",
      "Epoch 7679: train loss 1.050990343093872 val loss 0.9711613059043884\n",
      "Epoch 7680: train loss 0.9711613059043884 val loss 1.0050967931747437\n",
      "Epoch 7681: train loss 1.0050967931747437 val loss 0.9896969795227051\n",
      "Epoch 7682: train loss 0.9896969795227051 val loss 0.9850738048553467\n",
      "Epoch 7683: train loss 0.9850738048553467 val loss 0.9712239503860474\n",
      "Epoch 7684: train loss 0.9712239503860474 val loss 1.0002672672271729\n",
      "Epoch 7685: train loss 1.0002672672271729 val loss 1.0296525955200195\n",
      "Epoch 7686: train loss 1.0296525955200195 val loss 0.9747691750526428\n",
      "Epoch 7687: train loss 0.9747691750526428 val loss 1.0628952980041504\n",
      "Epoch 7688: train loss 1.0628952980041504 val loss 0.9891933798789978\n",
      "Epoch 7689: train loss 0.9891933798789978 val loss 0.9939111471176147\n",
      "Epoch 7690: train loss 0.9939111471176147 val loss 1.0310688018798828\n",
      "Epoch 7691: train loss 1.0310688018798828 val loss 0.9699369668960571\n",
      "Epoch 7692: train loss 0.9699369668960571 val loss 0.987470269203186\n",
      "Epoch 7693: train loss 0.987470269203186 val loss 0.9743415713310242\n",
      "Epoch 7694: train loss 0.9743415713310242 val loss 0.9756661653518677\n",
      "Epoch 7695: train loss 0.9756661653518677 val loss 0.9714080095291138\n",
      "Epoch 7696: train loss 0.9714080095291138 val loss 0.9766492247581482\n",
      "Epoch 7697: train loss 0.9766492247581482 val loss 0.9986770153045654\n",
      "Epoch 7698: train loss 0.9986770153045654 val loss 0.970077633857727\n",
      "Epoch 7699: train loss 0.970077633857727 val loss 1.0203406810760498\n",
      "Epoch 7700: train loss 1.0203406810760498 val loss 1.0005409717559814\n",
      "Epoch 7701: train loss 1.0005409717559814 val loss 0.9723058938980103\n",
      "Epoch 7702: train loss 0.9723058938980103 val loss 0.9985545873641968\n",
      "Epoch 7703: train loss 0.9985545873641968 val loss 1.0177404880523682\n",
      "Epoch 7704: train loss 1.0177404880523682 val loss 0.9709339737892151\n",
      "Epoch 7705: train loss 0.9709339737892151 val loss 1.029111623764038\n",
      "Epoch 7706: train loss 1.029111623764038 val loss 0.9898636341094971\n",
      "Epoch 7707: train loss 0.9898636341094971 val loss 0.981354832649231\n",
      "Epoch 7708: train loss 0.981354832649231 val loss 0.9890768527984619\n",
      "Epoch 7709: train loss 0.9890768527984619 val loss 1.0125514268875122\n",
      "Epoch 7710: train loss 1.0125514268875122 val loss 0.9694557785987854\n",
      "Epoch 7711: train loss 0.9694557785987854 val loss 1.039778709411621\n",
      "Epoch 7712: train loss 1.039778709411621 val loss 1.005903959274292\n",
      "Epoch 7713: train loss 1.005903959274292 val loss 0.9724348783493042\n",
      "Epoch 7714: train loss 0.9724348783493042 val loss 1.0135574340820312\n",
      "Epoch 7715: train loss 1.0135574340820312 val loss 0.9977575540542603\n",
      "Epoch 7716: train loss 0.9977575540542603 val loss 0.9778210520744324\n",
      "Epoch 7717: train loss 0.9778210520744324 val loss 1.0222705602645874\n",
      "Epoch 7718: train loss 1.0222705602645874 val loss 0.9779833555221558\n",
      "Epoch 7719: train loss 0.9779833555221558 val loss 0.9871035814285278\n",
      "Epoch 7720: train loss 0.9871035814285278 val loss 1.0147011280059814\n",
      "Epoch 7721: train loss 1.0147011280059814 val loss 0.9770796895027161\n",
      "Epoch 7722: train loss 0.9770796895027161 val loss 0.9883079528808594\n",
      "Epoch 7723: train loss 0.9883079528808594 val loss 1.0053024291992188\n",
      "Epoch 7724: train loss 1.0053024291992188 val loss 0.9796431064605713\n",
      "Epoch 7725: train loss 0.9796431064605713 val loss 0.9889575242996216\n",
      "Epoch 7726: train loss 0.9889575242996216 val loss 1.0141432285308838\n",
      "Epoch 7727: train loss 1.0141432285308838 val loss 0.9754911661148071\n",
      "Epoch 7728: train loss 0.9754911661148071 val loss 0.9850559234619141\n",
      "Epoch 7729: train loss 0.9850559234619141 val loss 1.0167222023010254\n",
      "Epoch 7730: train loss 1.0167222023010254 val loss 0.9748920202255249\n",
      "Epoch 7731: train loss 0.9748920202255249 val loss 0.9826754331588745\n",
      "Epoch 7732: train loss 0.9826754331588745 val loss 1.0120418071746826\n",
      "Epoch 7733: train loss 1.0120418071746826 val loss 0.9767422676086426\n",
      "Epoch 7734: train loss 0.9767422676086426 val loss 0.9852988719940186\n",
      "Epoch 7735: train loss 0.9852988719940186 val loss 1.0199240446090698\n",
      "Epoch 7736: train loss 1.0199240446090698 val loss 0.9731838703155518\n",
      "Epoch 7737: train loss 0.9731838703155518 val loss 0.9853920936584473\n",
      "Epoch 7738: train loss 0.9853920936584473 val loss 1.0061553716659546\n",
      "Epoch 7739: train loss 1.0061553716659546 val loss 0.9789382815361023\n",
      "Epoch 7740: train loss 0.9789382815361023 val loss 0.9939486384391785\n",
      "Epoch 7741: train loss 0.9939486384391785 val loss 1.000605583190918\n",
      "Epoch 7742: train loss 1.000605583190918 val loss 0.9791011214256287\n",
      "Epoch 7743: train loss 0.9791011214256287 val loss 0.9878659248352051\n",
      "Epoch 7744: train loss 0.9878659248352051 val loss 1.0155285596847534\n",
      "Epoch 7745: train loss 1.0155285596847534 val loss 0.9735239744186401\n",
      "Epoch 7746: train loss 0.9735239744186401 val loss 0.9824414849281311\n",
      "Epoch 7747: train loss 0.9824414849281311 val loss 1.017180323600769\n",
      "Epoch 7748: train loss 1.017180323600769 val loss 0.9750263094902039\n",
      "Epoch 7749: train loss 0.9750263094902039 val loss 0.9855446219444275\n",
      "Epoch 7750: train loss 0.9855446219444275 val loss 1.0111732482910156\n",
      "Epoch 7751: train loss 1.0111732482910156 val loss 0.9748859405517578\n",
      "Epoch 7752: train loss 0.9748859405517578 val loss 0.981086254119873\n",
      "Epoch 7753: train loss 0.981086254119873 val loss 0.9898707866668701\n",
      "Epoch 7754: train loss 0.9898707866668701 val loss 0.9994717836380005\n",
      "Epoch 7755: train loss 0.9994717836380005 val loss 0.9743417501449585\n",
      "Epoch 7756: train loss 0.9743417501449585 val loss 0.9682724475860596\n",
      "Epoch 7757: train loss 0.9682724475860596 val loss 0.9731830358505249\n",
      "Epoch 7758: train loss 0.9731830358505249 val loss 0.9679577350616455\n",
      "Epoch 7759: train loss 0.9679577350616455 val loss 0.9664149284362793\n",
      "Epoch 7760: train loss 0.9664149284362793 val loss 0.9714918732643127\n",
      "Epoch 7761: train loss 0.9714918732643127 val loss 0.9694582223892212\n",
      "Epoch 7762: train loss 0.9694582223892212 val loss 0.9769642353057861\n",
      "Epoch 7763: train loss 0.9769642353057861 val loss 0.9923802018165588\n",
      "Epoch 7764: train loss 0.9923802018165588 val loss 0.9664180278778076\n",
      "Epoch 7765: train loss 0.9664180278778076 val loss 1.0253509283065796\n",
      "Epoch 7766: train loss 1.0253509283065796 val loss 0.9944381713867188\n",
      "Epoch 7767: train loss 0.9944381713867188 val loss 0.9706283807754517\n",
      "Epoch 7768: train loss 0.9706283807754517 val loss 0.9939039945602417\n",
      "Epoch 7769: train loss 0.9939039945602417 val loss 1.0132488012313843\n",
      "Epoch 7770: train loss 1.0132488012313843 val loss 0.9705945253372192\n",
      "Epoch 7771: train loss 0.9705945253372192 val loss 1.0160804986953735\n",
      "Epoch 7772: train loss 1.0160804986953735 val loss 0.9997456073760986\n",
      "Epoch 7773: train loss 0.9997456073760986 val loss 0.970181941986084\n",
      "Epoch 7774: train loss 0.970181941986084 val loss 1.0032343864440918\n",
      "Epoch 7775: train loss 1.0032343864440918 val loss 1.0165820121765137\n",
      "Epoch 7776: train loss 1.0165820121765137 val loss 0.9700118899345398\n",
      "Epoch 7777: train loss 0.9700118899345398 val loss 1.022260069847107\n",
      "Epoch 7778: train loss 1.022260069847107 val loss 0.9816504716873169\n",
      "Epoch 7779: train loss 0.9816504716873169 val loss 0.979701042175293\n",
      "Epoch 7780: train loss 0.979701042175293 val loss 0.9833693504333496\n",
      "Epoch 7781: train loss 0.9833693504333496 val loss 0.9876384139060974\n",
      "Epoch 7782: train loss 0.9876384139060974 val loss 0.9679135680198669\n",
      "Epoch 7783: train loss 0.9679135680198669 val loss 1.0103487968444824\n",
      "Epoch 7784: train loss 1.0103487968444824 val loss 1.0053651332855225\n",
      "Epoch 7785: train loss 1.0053651332855225 val loss 0.9678953886032104\n",
      "Epoch 7786: train loss 0.9678953886032104 val loss 1.0118770599365234\n",
      "Epoch 7787: train loss 1.0118770599365234 val loss 1.0086123943328857\n",
      "Epoch 7788: train loss 1.0086123943328857 val loss 0.9675970077514648\n",
      "Epoch 7789: train loss 0.9675970077514648 val loss 1.029158115386963\n",
      "Epoch 7790: train loss 1.029158115386963 val loss 0.9781438708305359\n",
      "Epoch 7791: train loss 0.9781438708305359 val loss 1.0029070377349854\n",
      "Epoch 7792: train loss 1.0029070377349854 val loss 1.018126368522644\n",
      "Epoch 7793: train loss 1.018126368522644 val loss 0.9672695994377136\n",
      "Epoch 7794: train loss 0.9672695994377136 val loss 0.9985899925231934\n",
      "Epoch 7795: train loss 0.9985899925231934 val loss 0.9838118553161621\n",
      "Epoch 7796: train loss 0.9838118553161621 val loss 0.9850170612335205\n",
      "Epoch 7797: train loss 0.9850170612335205 val loss 0.9675206542015076\n",
      "Epoch 7798: train loss 0.9675206542015076 val loss 0.9820775985717773\n",
      "Epoch 7799: train loss 0.9820775985717773 val loss 1.050297737121582\n",
      "Epoch 7800: train loss 1.050297737121582 val loss 0.9812145233154297\n",
      "Epoch 7801: train loss 0.9812145233154297 val loss 1.1229753494262695\n",
      "Epoch 7802: train loss 1.1229753494262695 val loss 0.9698895215988159\n",
      "Epoch 7803: train loss 0.9698895215988159 val loss 1.108710527420044\n",
      "Epoch 7804: train loss 1.108710527420044 val loss 0.9834084510803223\n",
      "Epoch 7805: train loss 0.9834084510803223 val loss 1.161378264427185\n",
      "Epoch 7806: train loss 1.161378264427185 val loss 0.9819712042808533\n",
      "Epoch 7807: train loss 0.9819712042808533 val loss 1.2555029392242432\n",
      "Epoch 7808: train loss 1.2555029392242432 val loss 1.174799919128418\n",
      "Epoch 7809: train loss 1.174799919128418 val loss 1.0853123664855957\n",
      "Epoch 7810: train loss 1.0853123664855957 val loss 1.096552848815918\n",
      "Epoch 7811: train loss 1.096552848815918 val loss 1.0873098373413086\n",
      "Epoch 7812: train loss 1.0873098373413086 val loss 1.0610880851745605\n",
      "Epoch 7813: train loss 1.0610880851745605 val loss 1.0461814403533936\n",
      "Epoch 7814: train loss 1.0461814403533936 val loss 0.9800370931625366\n",
      "Epoch 7815: train loss 0.9800370931625366 val loss 1.0766522884368896\n",
      "Epoch 7816: train loss 1.0766522884368896 val loss 0.9784846305847168\n",
      "Epoch 7817: train loss 0.9784846305847168 val loss 1.0921108722686768\n",
      "Epoch 7818: train loss 1.0921108722686768 val loss 0.9706568121910095\n",
      "Epoch 7819: train loss 0.9706568121910095 val loss 1.0096186399459839\n",
      "Epoch 7820: train loss 1.0096186399459839 val loss 0.9727380275726318\n",
      "Epoch 7821: train loss 0.9727380275726318 val loss 0.9667824506759644\n",
      "Epoch 7822: train loss 0.9667824506759644 val loss 0.9658536911010742\n",
      "Epoch 7823: train loss 0.9658536911010742 val loss 0.9778599739074707\n",
      "Epoch 7824: train loss 0.9778599739074707 val loss 1.0173239707946777\n",
      "Epoch 7825: train loss 1.0173239707946777 val loss 0.9654195308685303\n",
      "Epoch 7826: train loss 0.9654195308685303 val loss 1.0595827102661133\n",
      "Epoch 7827: train loss 1.0595827102661133 val loss 0.9953560829162598\n",
      "Epoch 7828: train loss 0.9953560829162598 val loss 0.9838482737541199\n",
      "Epoch 7829: train loss 0.9838482737541199 val loss 1.040982723236084\n",
      "Epoch 7830: train loss 1.040982723236084 val loss 0.9682084321975708\n",
      "Epoch 7831: train loss 0.9682084321975708 val loss 1.00975501537323\n",
      "Epoch 7832: train loss 1.00975501537323 val loss 0.996247410774231\n",
      "Epoch 7833: train loss 0.996247410774231 val loss 0.9706054329872131\n",
      "Epoch 7834: train loss 0.9706054329872131 val loss 0.973612904548645\n",
      "Epoch 7835: train loss 0.973612904548645 val loss 0.9691455960273743\n",
      "Epoch 7836: train loss 0.9691455960273743 val loss 0.964783787727356\n",
      "Epoch 7837: train loss 0.964783787727356 val loss 0.9663999080657959\n",
      "Epoch 7838: train loss 0.9663999080657959 val loss 0.965758204460144\n",
      "Epoch 7839: train loss 0.965758204460144 val loss 0.9658505916595459\n",
      "Epoch 7840: train loss 0.9658505916595459 val loss 0.9708174467086792\n",
      "Epoch 7841: train loss 0.9708174467086792 val loss 0.9650191068649292\n",
      "Epoch 7842: train loss 0.9650191068649292 val loss 0.96441650390625\n",
      "Epoch 7843: train loss 0.96441650390625 val loss 0.9661582708358765\n",
      "Epoch 7844: train loss 0.9661582708358765 val loss 0.9685605764389038\n",
      "Epoch 7845: train loss 0.9685605764389038 val loss 0.9635002017021179\n",
      "Epoch 7846: train loss 0.9635002017021179 val loss 0.9671544432640076\n",
      "Epoch 7847: train loss 0.9671544432640076 val loss 0.9689146280288696\n",
      "Epoch 7848: train loss 0.9689146280288696 val loss 0.9636291861534119\n",
      "Epoch 7849: train loss 0.9636291861534119 val loss 0.9642821550369263\n",
      "Epoch 7850: train loss 0.9642821550369263 val loss 0.9633282423019409\n",
      "Epoch 7851: train loss 0.9633282423019409 val loss 0.9701719284057617\n",
      "Epoch 7852: train loss 0.9701719284057617 val loss 0.9930789470672607\n",
      "Epoch 7853: train loss 0.9930789470672607 val loss 0.9641064405441284\n",
      "Epoch 7854: train loss 0.9641064405441284 val loss 1.0276570320129395\n",
      "Epoch 7855: train loss 1.0276570320129395 val loss 0.9805382490158081\n",
      "Epoch 7856: train loss 0.9805382490158081 val loss 0.9722753763198853\n",
      "Epoch 7857: train loss 0.9722753763198853 val loss 1.0018969774246216\n",
      "Epoch 7858: train loss 1.0018969774246216 val loss 0.9853025674819946\n",
      "Epoch 7859: train loss 0.9853025674819946 val loss 0.9702796936035156\n",
      "Epoch 7860: train loss 0.9702796936035156 val loss 1.0054656267166138\n",
      "Epoch 7861: train loss 1.0054656267166138 val loss 0.9852378368377686\n",
      "Epoch 7862: train loss 0.9852378368377686 val loss 0.9705338478088379\n",
      "Epoch 7863: train loss 0.9705338478088379 val loss 1.0072728395462036\n",
      "Epoch 7864: train loss 1.0072728395462036 val loss 0.9806385040283203\n",
      "Epoch 7865: train loss 0.9806385040283203 val loss 0.9660381078720093\n",
      "Epoch 7866: train loss 0.9660381078720093 val loss 0.9797356128692627\n",
      "Epoch 7867: train loss 0.9797356128692627 val loss 1.0335698127746582\n",
      "Epoch 7868: train loss 1.0335698127746582 val loss 0.9687309861183167\n",
      "Epoch 7869: train loss 0.9687309861183167 val loss 1.0681772232055664\n",
      "Epoch 7870: train loss 1.0681772232055664 val loss 0.9730596542358398\n",
      "Epoch 7871: train loss 0.9730596542358398 val loss 1.024172306060791\n",
      "Epoch 7872: train loss 1.024172306060791 val loss 1.0028722286224365\n",
      "Epoch 7873: train loss 1.0028722286224365 val loss 0.9633105993270874\n",
      "Epoch 7874: train loss 0.9633105993270874 val loss 0.972358763217926\n",
      "Epoch 7875: train loss 0.972358763217926 val loss 0.9628552198410034\n",
      "Epoch 7876: train loss 0.9628552198410034 val loss 0.972234845161438\n",
      "Epoch 7877: train loss 0.972234845161438 val loss 1.0065395832061768\n",
      "Epoch 7878: train loss 1.0065395832061768 val loss 0.9668917655944824\n",
      "Epoch 7879: train loss 0.9668917655944824 val loss 0.9687505960464478\n",
      "Epoch 7880: train loss 0.9687505960464478 val loss 1.0001517534255981\n",
      "Epoch 7881: train loss 1.0001517534255981 val loss 0.9725388288497925\n",
      "Epoch 7882: train loss 0.9725388288497925 val loss 0.9739397764205933\n",
      "Epoch 7883: train loss 0.9739397764205933 val loss 0.9729345440864563\n",
      "Epoch 7884: train loss 0.9729345440864563 val loss 0.9716900587081909\n",
      "Epoch 7885: train loss 0.9716900587081909 val loss 0.9776018857955933\n",
      "Epoch 7886: train loss 0.9776018857955933 val loss 0.9723192453384399\n",
      "Epoch 7887: train loss 0.9723192453384399 val loss 0.9960180521011353\n",
      "Epoch 7888: train loss 0.9960180521011353 val loss 0.9861629009246826\n",
      "Epoch 7889: train loss 0.9861629009246826 val loss 0.9677274823188782\n",
      "Epoch 7890: train loss 0.9677274823188782 val loss 1.015026330947876\n",
      "Epoch 7891: train loss 1.015026330947876 val loss 0.9778866767883301\n",
      "Epoch 7892: train loss 0.9778866767883301 val loss 0.9730437994003296\n",
      "Epoch 7893: train loss 0.9730437994003296 val loss 0.9828130602836609\n",
      "Epoch 7894: train loss 0.9828130602836609 val loss 1.0035747289657593\n",
      "Epoch 7895: train loss 1.0035747289657593 val loss 0.9647390246391296\n",
      "Epoch 7896: train loss 0.9647390246391296 val loss 0.9947214126586914\n",
      "Epoch 7897: train loss 0.9947214126586914 val loss 1.0406168699264526\n",
      "Epoch 7898: train loss 1.0406168699264526 val loss 0.9821393489837646\n",
      "Epoch 7899: train loss 0.9821393489837646 val loss 1.080750584602356\n",
      "Epoch 7900: train loss 1.080750584602356 val loss 0.9662803411483765\n",
      "Epoch 7901: train loss 0.9662803411483765 val loss 1.0219488143920898\n",
      "Epoch 7902: train loss 1.0219488143920898 val loss 0.9663227796554565\n",
      "Epoch 7903: train loss 0.9663227796554565 val loss 1.0185986757278442\n",
      "Epoch 7904: train loss 1.0185986757278442 val loss 1.0077235698699951\n",
      "Epoch 7905: train loss 1.0077235698699951 val loss 0.9670473337173462\n",
      "Epoch 7906: train loss 0.9670473337173462 val loss 1.0401569604873657\n",
      "Epoch 7907: train loss 1.0401569604873657 val loss 0.9717223048210144\n",
      "Epoch 7908: train loss 0.9717223048210144 val loss 0.9970201253890991\n",
      "Epoch 7909: train loss 0.9970201253890991 val loss 1.0085171461105347\n",
      "Epoch 7910: train loss 1.0085171461105347 val loss 0.9637801647186279\n",
      "Epoch 7911: train loss 0.9637801647186279 val loss 0.9751029014587402\n",
      "Epoch 7912: train loss 0.9751029014587402 val loss 0.9634162187576294\n",
      "Epoch 7913: train loss 0.9634162187576294 val loss 1.0016365051269531\n",
      "Epoch 7914: train loss 1.0016365051269531 val loss 1.0152701139450073\n",
      "Epoch 7915: train loss 1.0152701139450073 val loss 0.9674360752105713\n",
      "Epoch 7916: train loss 0.9674360752105713 val loss 1.0468305349349976\n",
      "Epoch 7917: train loss 1.0468305349349976 val loss 0.9683315753936768\n",
      "Epoch 7918: train loss 0.9683315753936768 val loss 0.9974719285964966\n",
      "Epoch 7919: train loss 0.9974719285964966 val loss 1.003479242324829\n",
      "Epoch 7920: train loss 1.003479242324829 val loss 0.9631030559539795\n",
      "Epoch 7921: train loss 0.9631030559539795 val loss 0.9737614393234253\n",
      "Epoch 7922: train loss 0.9737614393234253 val loss 0.962114155292511\n",
      "Epoch 7923: train loss 0.962114155292511 val loss 0.9628735780715942\n",
      "Epoch 7924: train loss 0.9628735780715942 val loss 0.9674654603004456\n",
      "Epoch 7925: train loss 0.9674654603004456 val loss 0.961478054523468\n",
      "Epoch 7926: train loss 0.961478054523468 val loss 0.9617701172828674\n",
      "Epoch 7927: train loss 0.9617701172828674 val loss 0.9615284204483032\n",
      "Epoch 7928: train loss 0.9615284204483032 val loss 0.9624274969100952\n",
      "Epoch 7929: train loss 0.9624274969100952 val loss 0.9640398025512695\n",
      "Epoch 7930: train loss 0.9640398025512695 val loss 0.9623960256576538\n",
      "Epoch 7931: train loss 0.9623960256576538 val loss 0.9702171087265015\n",
      "Epoch 7932: train loss 0.9702171087265015 val loss 0.9724109768867493\n",
      "Epoch 7933: train loss 0.9724109768867493 val loss 0.9664859771728516\n",
      "Epoch 7934: train loss 0.9664859771728516 val loss 0.993574857711792\n",
      "Epoch 7935: train loss 0.993574857711792 val loss 0.9955880641937256\n",
      "Epoch 7936: train loss 0.9955880641937256 val loss 0.9627139568328857\n",
      "Epoch 7937: train loss 0.9627139568328857 val loss 1.018929123878479\n",
      "Epoch 7938: train loss 1.018929123878479 val loss 1.0258821249008179\n",
      "Epoch 7939: train loss 1.0258821249008179 val loss 0.9768790006637573\n",
      "Epoch 7940: train loss 0.9768790006637573 val loss 1.0660957098007202\n",
      "Epoch 7941: train loss 1.0660957098007202 val loss 0.9637464284896851\n",
      "Epoch 7942: train loss 0.9637464284896851 val loss 0.9979256391525269\n",
      "Epoch 7943: train loss 0.9979256391525269 val loss 1.0158755779266357\n",
      "Epoch 7944: train loss 1.0158755779266357 val loss 0.960418701171875\n",
      "Epoch 7945: train loss 0.960418701171875 val loss 0.9848195314407349\n",
      "Epoch 7946: train loss 0.9848195314407349 val loss 0.9649851322174072\n",
      "Epoch 7947: train loss 0.9649851322174072 val loss 0.9823979139328003\n",
      "Epoch 7948: train loss 0.9823979139328003 val loss 1.0695515871047974\n",
      "Epoch 7949: train loss 1.0695515871047974 val loss 0.9664961099624634\n",
      "Epoch 7950: train loss 0.9664961099624634 val loss 1.100299596786499\n",
      "Epoch 7951: train loss 1.100299596786499 val loss 0.9617782831192017\n",
      "Epoch 7952: train loss 0.9617782831192017 val loss 1.0563007593154907\n",
      "Epoch 7953: train loss 1.0563007593154907 val loss 0.9602030515670776\n",
      "Epoch 7954: train loss 0.9602030515670776 val loss 1.082681655883789\n",
      "Epoch 7955: train loss 1.082681655883789 val loss 0.9767057299613953\n",
      "Epoch 7956: train loss 0.9767057299613953 val loss 1.007301688194275\n",
      "Epoch 7957: train loss 1.007301688194275 val loss 0.9989826679229736\n",
      "Epoch 7958: train loss 0.9989826679229736 val loss 0.9620203375816345\n",
      "Epoch 7959: train loss 0.9620203375816345 val loss 0.9698878526687622\n",
      "Epoch 7960: train loss 0.9698878526687622 val loss 0.9641239643096924\n",
      "Epoch 7961: train loss 0.9641239643096924 val loss 0.9644051790237427\n",
      "Epoch 7962: train loss 0.9644051790237427 val loss 0.9795492887496948\n",
      "Epoch 7963: train loss 0.9795492887496948 val loss 0.961335301399231\n",
      "Epoch 7964: train loss 0.961335301399231 val loss 0.9666156768798828\n",
      "Epoch 7965: train loss 0.9666156768798828 val loss 0.9738267660140991\n",
      "Epoch 7966: train loss 0.9738267660140991 val loss 0.9606833457946777\n",
      "Epoch 7967: train loss 0.9606833457946777 val loss 0.9845902919769287\n",
      "Epoch 7968: train loss 0.9845902919769287 val loss 1.0361862182617188\n",
      "Epoch 7969: train loss 1.0361862182617188 val loss 0.9713551998138428\n",
      "Epoch 7970: train loss 0.9713551998138428 val loss 1.0383845567703247\n",
      "Epoch 7971: train loss 1.0383845567703247 val loss 0.9657078981399536\n",
      "Epoch 7972: train loss 0.9657078981399536 val loss 0.9870328903198242\n",
      "Epoch 7973: train loss 0.9870328903198242 val loss 1.0174751281738281\n",
      "Epoch 7974: train loss 1.0174751281738281 val loss 0.959862232208252\n",
      "Epoch 7975: train loss 0.959862232208252 val loss 0.9719312787055969\n",
      "Epoch 7976: train loss 0.9719312787055969 val loss 0.9596280455589294\n",
      "Epoch 7977: train loss 0.9596280455589294 val loss 0.9731178283691406\n",
      "Epoch 7978: train loss 0.9731178283691406 val loss 1.0276508331298828\n",
      "Epoch 7979: train loss 1.0276508331298828 val loss 0.9617770910263062\n",
      "Epoch 7980: train loss 0.9617770910263062 val loss 1.0419468879699707\n",
      "Epoch 7981: train loss 1.0419468879699707 val loss 0.9721364974975586\n",
      "Epoch 7982: train loss 0.9721364974975586 val loss 1.0084779262542725\n",
      "Epoch 7983: train loss 1.0084779262542725 val loss 1.0166161060333252\n",
      "Epoch 7984: train loss 1.0166161060333252 val loss 0.961550235748291\n",
      "Epoch 7985: train loss 0.961550235748291 val loss 0.9781332015991211\n",
      "Epoch 7986: train loss 0.9781332015991211 val loss 0.9630604982376099\n",
      "Epoch 7987: train loss 0.9630604982376099 val loss 0.9613581895828247\n",
      "Epoch 7988: train loss 0.9613581895828247 val loss 0.9601204991340637\n",
      "Epoch 7989: train loss 0.9601204991340637 val loss 0.9604099988937378\n",
      "Epoch 7990: train loss 0.9604099988937378 val loss 0.9596606492996216\n",
      "Epoch 7991: train loss 0.9596606492996216 val loss 0.9599536657333374\n",
      "Epoch 7992: train loss 0.9599536657333374 val loss 0.9636488556861877\n",
      "Epoch 7993: train loss 0.9636488556861877 val loss 0.959247350692749\n",
      "Epoch 7994: train loss 0.959247350692749 val loss 0.9697617888450623\n",
      "Epoch 7995: train loss 0.9697617888450623 val loss 1.0186984539031982\n",
      "Epoch 7996: train loss 1.0186984539031982 val loss 0.9617326259613037\n",
      "Epoch 7997: train loss 0.9617326259613037 val loss 1.0280373096466064\n",
      "Epoch 7998: train loss 1.0280373096466064 val loss 0.9849027395248413\n",
      "Epoch 7999: train loss 0.9849027395248413 val loss 0.9833183288574219\n",
      "Epoch 8000: train loss 0.9833183288574219 val loss 1.0112051963806152\n",
      "Epoch 8001: train loss 1.0112051963806152 val loss 0.973279595375061\n",
      "Epoch 8002: train loss 0.973279595375061 val loss 0.9901669025421143\n",
      "Epoch 8003: train loss 0.9901669025421143 val loss 1.0174155235290527\n",
      "Epoch 8004: train loss 1.0174155235290527 val loss 0.9653285145759583\n",
      "Epoch 8005: train loss 0.9653285145759583 val loss 0.9805634021759033\n",
      "Epoch 8006: train loss 0.9805634021759033 val loss 0.9894291162490845\n",
      "Epoch 8007: train loss 0.9894291162490845 val loss 0.9737135171890259\n",
      "Epoch 8008: train loss 0.9737135171890259 val loss 0.9617874622344971\n",
      "Epoch 8009: train loss 0.9617874622344971 val loss 1.0127379894256592\n",
      "Epoch 8010: train loss 1.0127379894256592 val loss 0.9722893834114075\n",
      "Epoch 8011: train loss 0.9722893834114075 val loss 0.9800643920898438\n",
      "Epoch 8012: train loss 0.9800643920898438 val loss 1.0127108097076416\n",
      "Epoch 8013: train loss 1.0127108097076416 val loss 0.96211838722229\n",
      "Epoch 8014: train loss 0.96211838722229 val loss 0.9702788591384888\n",
      "Epoch 8015: train loss 0.9702788591384888 val loss 0.9603049755096436\n",
      "Epoch 8016: train loss 0.9603049755096436 val loss 0.961588978767395\n",
      "Epoch 8017: train loss 0.961588978767395 val loss 0.9617883563041687\n",
      "Epoch 8018: train loss 0.9617883563041687 val loss 0.9609943628311157\n",
      "Epoch 8019: train loss 0.9609943628311157 val loss 0.9606393575668335\n",
      "Epoch 8020: train loss 0.9606393575668335 val loss 0.9623981714248657\n",
      "Epoch 8021: train loss 0.9623981714248657 val loss 0.9604317545890808\n",
      "Epoch 8022: train loss 0.9604317545890808 val loss 0.9587103128433228\n",
      "Epoch 8023: train loss 0.9587103128433228 val loss 0.9602864980697632\n",
      "Epoch 8024: train loss 0.9602864980697632 val loss 0.9575254321098328\n",
      "Epoch 8025: train loss 0.9575254321098328 val loss 0.957838773727417\n",
      "Epoch 8026: train loss 0.957838773727417 val loss 0.9580532312393188\n",
      "Epoch 8027: train loss 0.9580532312393188 val loss 0.95839524269104\n",
      "Epoch 8028: train loss 0.95839524269104 val loss 0.9632818698883057\n",
      "Epoch 8029: train loss 0.9632818698883057 val loss 0.9661654233932495\n",
      "Epoch 8030: train loss 0.9661654233932495 val loss 0.9843233823776245\n",
      "Epoch 8031: train loss 0.9843233823776245 val loss 0.9703255295753479\n",
      "Epoch 8032: train loss 0.9703255295753479 val loss 0.9702285528182983\n",
      "Epoch 8033: train loss 0.9702285528182983 val loss 0.9622772932052612\n",
      "Epoch 8034: train loss 0.9622772932052612 val loss 1.0034139156341553\n",
      "Epoch 8035: train loss 1.0034139156341553 val loss 0.9778449535369873\n",
      "Epoch 8036: train loss 0.9778449535369873 val loss 0.9650729894638062\n",
      "Epoch 8037: train loss 0.9650729894638062 val loss 1.000867247581482\n",
      "Epoch 8038: train loss 1.000867247581482 val loss 0.981163501739502\n",
      "Epoch 8039: train loss 0.981163501739502 val loss 0.9674954414367676\n",
      "Epoch 8040: train loss 0.9674954414367676 val loss 0.9903552532196045\n",
      "Epoch 8041: train loss 0.9903552532196045 val loss 0.9861342906951904\n",
      "Epoch 8042: train loss 0.9861342906951904 val loss 0.9583339691162109\n",
      "Epoch 8043: train loss 0.9583339691162109 val loss 0.9842538237571716\n",
      "Epoch 8044: train loss 0.9842538237571716 val loss 1.0287282466888428\n",
      "Epoch 8045: train loss 1.0287282466888428 val loss 0.9638243913650513\n",
      "Epoch 8046: train loss 0.9638243913650513 val loss 1.0654062032699585\n",
      "Epoch 8047: train loss 1.0654062032699585 val loss 0.96552574634552\n",
      "Epoch 8048: train loss 0.96552574634552 val loss 1.0138933658599854\n",
      "Epoch 8049: train loss 1.0138933658599854 val loss 1.0012779235839844\n",
      "Epoch 8050: train loss 1.0012779235839844 val loss 0.9594485759735107\n",
      "Epoch 8051: train loss 0.9594485759735107 val loss 0.9912403225898743\n",
      "Epoch 8052: train loss 0.9912403225898743 val loss 0.9776198863983154\n",
      "Epoch 8053: train loss 0.9776198863983154 val loss 0.9775248765945435\n",
      "Epoch 8054: train loss 0.9775248765945435 val loss 0.9606989026069641\n",
      "Epoch 8055: train loss 0.9606989026069641 val loss 0.973158597946167\n",
      "Epoch 8056: train loss 0.973158597946167 val loss 1.019885540008545\n",
      "Epoch 8057: train loss 1.019885540008545 val loss 0.9640729427337646\n",
      "Epoch 8058: train loss 0.9640729427337646 val loss 1.06023371219635\n",
      "Epoch 8059: train loss 1.06023371219635 val loss 0.9737406969070435\n",
      "Epoch 8060: train loss 0.9737406969070435 val loss 1.0136812925338745\n",
      "Epoch 8061: train loss 1.0136812925338745 val loss 1.0122426748275757\n",
      "Epoch 8062: train loss 1.0122426748275757 val loss 0.9607998132705688\n",
      "Epoch 8063: train loss 0.9607998132705688 val loss 1.0005348920822144\n",
      "Epoch 8064: train loss 1.0005348920822144 val loss 0.956807553768158\n",
      "Epoch 8065: train loss 0.956807553768158 val loss 1.0201274156570435\n",
      "Epoch 8066: train loss 1.0201274156570435 val loss 0.9781467914581299\n",
      "Epoch 8067: train loss 0.9781467914581299 val loss 0.9698206186294556\n",
      "Epoch 8068: train loss 0.9698206186294556 val loss 0.9914924502372742\n",
      "Epoch 8069: train loss 0.9914924502372742 val loss 0.975868821144104\n",
      "Epoch 8070: train loss 0.975868821144104 val loss 0.9688413143157959\n",
      "Epoch 8071: train loss 0.9688413143157959 val loss 0.9776533842086792\n",
      "Epoch 8072: train loss 0.9776533842086792 val loss 0.979270339012146\n",
      "Epoch 8073: train loss 0.979270339012146 val loss 0.9601894617080688\n",
      "Epoch 8074: train loss 0.9601894617080688 val loss 1.011201024055481\n",
      "Epoch 8075: train loss 1.011201024055481 val loss 0.9765162467956543\n",
      "Epoch 8076: train loss 0.9765162467956543 val loss 0.9649426937103271\n",
      "Epoch 8077: train loss 0.9649426937103271 val loss 0.9985644817352295\n",
      "Epoch 8078: train loss 0.9985644817352295 val loss 0.9742124080657959\n",
      "Epoch 8079: train loss 0.9742124080657959 val loss 0.9577172994613647\n",
      "Epoch 8080: train loss 0.9577172994613647 val loss 0.9828608632087708\n",
      "Epoch 8081: train loss 0.9828608632087708 val loss 1.0165581703186035\n",
      "Epoch 8082: train loss 1.0165581703186035 val loss 0.9581760168075562\n",
      "Epoch 8083: train loss 0.9581760168075562 val loss 1.0201663970947266\n",
      "Epoch 8084: train loss 1.0201663970947266 val loss 0.9970805644989014\n",
      "Epoch 8085: train loss 0.9970805644989014 val loss 0.9593527317047119\n",
      "Epoch 8086: train loss 0.9593527317047119 val loss 1.0412319898605347\n",
      "Epoch 8087: train loss 1.0412319898605347 val loss 0.9678137898445129\n",
      "Epoch 8088: train loss 0.9678137898445129 val loss 1.002357840538025\n",
      "Epoch 8089: train loss 1.002357840538025 val loss 1.023362159729004\n",
      "Epoch 8090: train loss 1.023362159729004 val loss 0.9624559879302979\n",
      "Epoch 8091: train loss 0.9624559879302979 val loss 1.0693674087524414\n",
      "Epoch 8092: train loss 1.0693674087524414 val loss 0.9597011804580688\n",
      "Epoch 8093: train loss 0.9597011804580688 val loss 1.0675196647644043\n",
      "Epoch 8094: train loss 1.0675196647644043 val loss 0.9693504571914673\n",
      "Epoch 8095: train loss 0.9693504571914673 val loss 1.0304090976715088\n",
      "Epoch 8096: train loss 1.0304090976715088 val loss 0.974808931350708\n",
      "Epoch 8097: train loss 0.974808931350708 val loss 0.9629138708114624\n",
      "Epoch 8098: train loss 0.9629138708114624 val loss 0.960463285446167\n",
      "Epoch 8099: train loss 0.960463285446167 val loss 0.9783428907394409\n",
      "Epoch 8100: train loss 0.9783428907394409 val loss 1.0090906620025635\n",
      "Epoch 8101: train loss 1.0090906620025635 val loss 0.9564233422279358\n",
      "Epoch 8102: train loss 0.9564233422279358 val loss 1.023921251296997\n",
      "Epoch 8103: train loss 1.023921251296997 val loss 0.9835848808288574\n",
      "Epoch 8104: train loss 0.9835848808288574 val loss 0.9707510471343994\n",
      "Epoch 8105: train loss 0.9707510471343994 val loss 1.0234061479568481\n",
      "Epoch 8106: train loss 1.0234061479568481 val loss 0.9649838209152222\n",
      "Epoch 8107: train loss 0.9649838209152222 val loss 0.9822087287902832\n",
      "Epoch 8108: train loss 0.9822087287902832 val loss 1.00627601146698\n",
      "Epoch 8109: train loss 1.00627601146698 val loss 0.9575520753860474\n",
      "Epoch 8110: train loss 0.9575520753860474 val loss 0.985590934753418\n",
      "Epoch 8111: train loss 0.985590934753418 val loss 0.9774909615516663\n",
      "Epoch 8112: train loss 0.9774909615516663 val loss 0.9734792709350586\n",
      "Epoch 8113: train loss 0.9734792709350586 val loss 0.9657365083694458\n",
      "Epoch 8114: train loss 0.9657365083694458 val loss 0.9791483879089355\n",
      "Epoch 8115: train loss 0.9791483879089355 val loss 0.9983268976211548\n",
      "Epoch 8116: train loss 0.9983268976211548 val loss 0.9576225280761719\n",
      "Epoch 8117: train loss 0.9576225280761719 val loss 1.011989951133728\n",
      "Epoch 8118: train loss 1.011989951133728 val loss 1.0138483047485352\n",
      "Epoch 8119: train loss 1.0138483047485352 val loss 0.9647060036659241\n",
      "Epoch 8120: train loss 0.9647060036659241 val loss 1.0359106063842773\n",
      "Epoch 8121: train loss 1.0359106063842773 val loss 0.9632259607315063\n",
      "Epoch 8122: train loss 0.9632259607315063 val loss 0.989793598651886\n",
      "Epoch 8123: train loss 0.989793598651886 val loss 1.003525733947754\n",
      "Epoch 8124: train loss 1.003525733947754 val loss 0.9554251432418823\n",
      "Epoch 8125: train loss 0.9554251432418823 val loss 0.983195960521698\n",
      "Epoch 8126: train loss 0.983195960521698 val loss 0.9882674217224121\n",
      "Epoch 8127: train loss 0.9882674217224121 val loss 0.9653574228286743\n",
      "Epoch 8128: train loss 0.9653574228286743 val loss 0.9684640765190125\n",
      "Epoch 8129: train loss 0.9684640765190125 val loss 0.9607436656951904\n",
      "Epoch 8130: train loss 0.9607436656951904 val loss 0.9599601030349731\n",
      "Epoch 8131: train loss 0.9599601030349731 val loss 0.9728692770004272\n",
      "Epoch 8132: train loss 0.9728692770004272 val loss 0.9590089321136475\n",
      "Epoch 8133: train loss 0.9590089321136475 val loss 1.0106558799743652\n",
      "Epoch 8134: train loss 1.0106558799743652 val loss 0.9726394414901733\n",
      "Epoch 8135: train loss 0.9726394414901733 val loss 0.970327615737915\n",
      "Epoch 8136: train loss 0.970327615737915 val loss 1.0066100358963013\n",
      "Epoch 8137: train loss 1.0066100358963013 val loss 0.9581233263015747\n",
      "Epoch 8138: train loss 0.9581233263015747 val loss 0.9607670307159424\n",
      "Epoch 8139: train loss 0.9607670307159424 val loss 0.9721535444259644\n",
      "Epoch 8140: train loss 0.9721535444259644 val loss 1.0153987407684326\n",
      "Epoch 8141: train loss 1.0153987407684326 val loss 0.9566671252250671\n",
      "Epoch 8142: train loss 0.9566671252250671 val loss 1.0288236141204834\n",
      "Epoch 8143: train loss 1.0288236141204834 val loss 0.9890096187591553\n",
      "Epoch 8144: train loss 0.9890096187591553 val loss 0.9600480794906616\n",
      "Epoch 8145: train loss 0.9600480794906616 val loss 1.0334277153015137\n",
      "Epoch 8146: train loss 1.0334277153015137 val loss 0.9674047231674194\n",
      "Epoch 8147: train loss 0.9674047231674194 val loss 0.9953259229660034\n",
      "Epoch 8148: train loss 0.9953259229660034 val loss 1.036864995956421\n",
      "Epoch 8149: train loss 1.036864995956421 val loss 0.9562293291091919\n",
      "Epoch 8150: train loss 0.9562293291091919 val loss 1.04087233543396\n",
      "Epoch 8151: train loss 1.04087233543396 val loss 0.9559968709945679\n",
      "Epoch 8152: train loss 0.9559968709945679 val loss 1.0193978548049927\n",
      "Epoch 8153: train loss 1.0193978548049927 val loss 1.0019508600234985\n",
      "Epoch 8154: train loss 1.0019508600234985 val loss 0.9623898267745972\n",
      "Epoch 8155: train loss 0.9623898267745972 val loss 1.0420506000518799\n",
      "Epoch 8156: train loss 1.0420506000518799 val loss 0.9588090181350708\n",
      "Epoch 8157: train loss 0.9588090181350708 val loss 0.9770400524139404\n",
      "Epoch 8158: train loss 0.9770400524139404 val loss 0.9644309282302856\n",
      "Epoch 8159: train loss 0.9644309282302856 val loss 0.9733839631080627\n",
      "Epoch 8160: train loss 0.9733839631080627 val loss 0.9572004079818726\n",
      "Epoch 8161: train loss 0.9572004079818726 val loss 1.0005261898040771\n",
      "Epoch 8162: train loss 1.0005261898040771 val loss 0.9781434535980225\n",
      "Epoch 8163: train loss 0.9781434535980225 val loss 0.955310583114624\n",
      "Epoch 8164: train loss 0.955310583114624 val loss 0.9840040802955627\n",
      "Epoch 8165: train loss 0.9840040802955627 val loss 1.0094293355941772\n",
      "Epoch 8166: train loss 1.0094293355941772 val loss 0.9545794725418091\n",
      "Epoch 8167: train loss 0.9545794725418091 val loss 1.0064011812210083\n",
      "Epoch 8168: train loss 1.0064011812210083 val loss 0.9796739816665649\n",
      "Epoch 8169: train loss 0.9796739816665649 val loss 0.9559152126312256\n",
      "Epoch 8170: train loss 0.9559152126312256 val loss 0.9815562963485718\n",
      "Epoch 8171: train loss 0.9815562963485718 val loss 1.006628155708313\n",
      "Epoch 8172: train loss 1.006628155708313 val loss 0.9554522037506104\n",
      "Epoch 8173: train loss 0.9554522037506104 val loss 1.0139005184173584\n",
      "Epoch 8174: train loss 1.0139005184173584 val loss 0.9947189092636108\n",
      "Epoch 8175: train loss 0.9947189092636108 val loss 0.9569669961929321\n",
      "Epoch 8176: train loss 0.9569669961929321 val loss 1.036013126373291\n",
      "Epoch 8177: train loss 1.036013126373291 val loss 0.9666177034378052\n",
      "Epoch 8178: train loss 0.9666177034378052 val loss 1.0017558336257935\n",
      "Epoch 8179: train loss 1.0017558336257935 val loss 1.0171055793762207\n",
      "Epoch 8180: train loss 1.0171055793762207 val loss 0.9624499082565308\n",
      "Epoch 8181: train loss 0.9624499082565308 val loss 1.0877790451049805\n",
      "Epoch 8182: train loss 1.0877790451049805 val loss 0.9656463861465454\n",
      "Epoch 8183: train loss 0.9656463861465454 val loss 1.1054480075836182\n",
      "Epoch 8184: train loss 1.1054480075836182 val loss 0.9605140686035156\n",
      "Epoch 8185: train loss 0.9605140686035156 val loss 1.1296253204345703\n",
      "Epoch 8186: train loss 1.1296253204345703 val loss 0.9844375848770142\n",
      "Epoch 8187: train loss 0.9844375848770142 val loss 1.178450584411621\n",
      "Epoch 8188: train loss 1.178450584411621 val loss 1.0255815982818604\n",
      "Epoch 8189: train loss 1.0255815982818604 val loss 1.20840322971344\n",
      "Epoch 8190: train loss 1.20840322971344 val loss 1.1713690757751465\n",
      "Epoch 8191: train loss 1.1713690757751465 val loss 1.0560188293457031\n",
      "Epoch 8192: train loss 1.0560188293457031 val loss 1.0822875499725342\n",
      "Epoch 8193: train loss 1.0822875499725342 val loss 1.0546032190322876\n",
      "Epoch 8194: train loss 1.0546032190322876 val loss 1.0491282939910889\n",
      "Epoch 8195: train loss 1.0491282939910889 val loss 1.060797929763794\n",
      "Epoch 8196: train loss 1.060797929763794 val loss 0.986501932144165\n",
      "Epoch 8197: train loss 0.986501932144165 val loss 1.1068918704986572\n",
      "Epoch 8198: train loss 1.1068918704986572 val loss 1.0051664113998413\n",
      "Epoch 8199: train loss 1.0051664113998413 val loss 1.162552833557129\n",
      "Epoch 8200: train loss 1.162552833557129 val loss 1.041487216949463\n",
      "Epoch 8201: train loss 1.041487216949463 val loss 1.2096271514892578\n",
      "Epoch 8202: train loss 1.2096271514892578 val loss 1.2234481573104858\n",
      "Epoch 8203: train loss 1.2234481573104858 val loss 0.9727117419242859\n",
      "Epoch 8204: train loss 0.9727117419242859 val loss 1.1379358768463135\n",
      "Epoch 8205: train loss 1.1379358768463135 val loss 0.9685361385345459\n",
      "Epoch 8206: train loss 0.9685361385345459 val loss 1.0866014957427979\n",
      "Epoch 8207: train loss 1.0866014957427979 val loss 0.9568314552307129\n",
      "Epoch 8208: train loss 0.9568314552307129 val loss 1.0938609838485718\n",
      "Epoch 8209: train loss 1.0938609838485718 val loss 0.9567539691925049\n",
      "Epoch 8210: train loss 0.9567539691925049 val loss 1.0025274753570557\n",
      "Epoch 8211: train loss 1.0025274753570557 val loss 0.9647797346115112\n",
      "Epoch 8212: train loss 0.9647797346115112 val loss 0.9556883573532104\n",
      "Epoch 8213: train loss 0.9556883573532104 val loss 0.9594491720199585\n",
      "Epoch 8214: train loss 0.9594491720199585 val loss 0.9580085277557373\n",
      "Epoch 8215: train loss 0.9580085277557373 val loss 0.954849123954773\n",
      "Epoch 8216: train loss 0.954849123954773 val loss 0.9655371904373169\n",
      "Epoch 8217: train loss 0.9655371904373169 val loss 0.9569994211196899\n",
      "Epoch 8218: train loss 0.9569994211196899 val loss 0.9851598739624023\n",
      "Epoch 8219: train loss 0.9851598739624023 val loss 0.9912041425704956\n",
      "Epoch 8220: train loss 0.9912041425704956 val loss 0.9594248533248901\n",
      "Epoch 8221: train loss 0.9594248533248901 val loss 1.0167741775512695\n",
      "Epoch 8222: train loss 1.0167741775512695 val loss 0.9654899835586548\n",
      "Epoch 8223: train loss 0.9654899835586548 val loss 0.991801917552948\n",
      "Epoch 8224: train loss 0.991801917552948 val loss 1.0304901599884033\n",
      "Epoch 8225: train loss 1.0304901599884033 val loss 0.9560767412185669\n",
      "Epoch 8226: train loss 0.9560767412185669 val loss 1.0031489133834839\n",
      "Epoch 8227: train loss 1.0031489133834839 val loss 0.9604441523551941\n",
      "Epoch 8228: train loss 0.9604441523551941 val loss 0.9580132961273193\n",
      "Epoch 8229: train loss 0.9580132961273193 val loss 0.9633843898773193\n",
      "Epoch 8230: train loss 0.9633843898773193 val loss 0.956084132194519\n",
      "Epoch 8231: train loss 0.956084132194519 val loss 0.9949368238449097\n",
      "Epoch 8232: train loss 0.9949368238449097 val loss 0.9807595610618591\n",
      "Epoch 8233: train loss 0.9807595610618591 val loss 0.9572277069091797\n",
      "Epoch 8234: train loss 0.9572277069091797 val loss 1.0193047523498535\n",
      "Epoch 8235: train loss 1.0193047523498535 val loss 0.9697669744491577\n",
      "Epoch 8236: train loss 0.9697669744491577 val loss 0.9764362573623657\n",
      "Epoch 8237: train loss 0.9764362573623657 val loss 0.9974104166030884\n",
      "Epoch 8238: train loss 0.9974104166030884 val loss 0.955025851726532\n",
      "Epoch 8239: train loss 0.955025851726532 val loss 0.9634673595428467\n",
      "Epoch 8240: train loss 0.9634673595428467 val loss 0.9580670595169067\n",
      "Epoch 8241: train loss 0.9580670595169067 val loss 0.957201361656189\n",
      "Epoch 8242: train loss 0.957201361656189 val loss 0.9636273384094238\n",
      "Epoch 8243: train loss 0.9636273384094238 val loss 0.9537810683250427\n",
      "Epoch 8244: train loss 0.9537810683250427 val loss 0.9756057262420654\n",
      "Epoch 8245: train loss 0.9756057262420654 val loss 0.993785560131073\n",
      "Epoch 8246: train loss 0.993785560131073 val loss 0.9545894861221313\n",
      "Epoch 8247: train loss 0.9545894861221313 val loss 0.9776265025138855\n",
      "Epoch 8248: train loss 0.9776265025138855 val loss 1.0361117124557495\n",
      "Epoch 8249: train loss 1.0361117124557495 val loss 0.9753888845443726\n",
      "Epoch 8250: train loss 0.9753888845443726 val loss 1.1112477779388428\n",
      "Epoch 8251: train loss 1.1112477779388428 val loss 0.9876506924629211\n",
      "Epoch 8252: train loss 0.9876506924629211 val loss 1.1830573081970215\n",
      "Epoch 8253: train loss 1.1830573081970215 val loss 1.0888265371322632\n",
      "Epoch 8254: train loss 1.0888265371322632 val loss 1.140545129776001\n",
      "Epoch 8255: train loss 1.140545129776001 val loss 1.1386339664459229\n",
      "Epoch 8256: train loss 1.1386339664459229 val loss 1.0472044944763184\n",
      "Epoch 8257: train loss 1.0472044944763184 val loss 1.1390116214752197\n",
      "Epoch 8258: train loss 1.1390116214752197 val loss 0.973016083240509\n",
      "Epoch 8259: train loss 0.973016083240509 val loss 1.0815473794937134\n",
      "Epoch 8260: train loss 1.0815473794937134 val loss 0.981648862361908\n",
      "Epoch 8261: train loss 0.981648862361908 val loss 1.0055445432662964\n",
      "Epoch 8262: train loss 1.0055445432662964 val loss 1.0088577270507812\n",
      "Epoch 8263: train loss 1.0088577270507812 val loss 0.9550550580024719\n",
      "Epoch 8264: train loss 0.9550550580024719 val loss 0.9784130454063416\n",
      "Epoch 8265: train loss 0.9784130454063416 val loss 0.9596245288848877\n",
      "Epoch 8266: train loss 0.9596245288848877 val loss 0.9620726108551025\n",
      "Epoch 8267: train loss 0.9620726108551025 val loss 1.0083833932876587\n",
      "Epoch 8268: train loss 1.0083833932876587 val loss 0.9531813859939575\n",
      "Epoch 8269: train loss 0.9531813859939575 val loss 1.015302300453186\n",
      "Epoch 8270: train loss 1.015302300453186 val loss 0.9747892618179321\n",
      "Epoch 8271: train loss 0.9747892618179321 val loss 0.96735018491745\n",
      "Epoch 8272: train loss 0.96735018491745 val loss 1.0060184001922607\n",
      "Epoch 8273: train loss 1.0060184001922607 val loss 0.9569619297981262\n",
      "Epoch 8274: train loss 0.9569619297981262 val loss 0.9733860492706299\n",
      "Epoch 8275: train loss 0.9733860492706299 val loss 0.9674344062805176\n",
      "Epoch 8276: train loss 0.9674344062805176 val loss 0.9797989726066589\n",
      "Epoch 8277: train loss 0.9797989726066589 val loss 0.9535990953445435\n",
      "Epoch 8278: train loss 0.9535990953445435 val loss 0.9712526798248291\n",
      "Epoch 8279: train loss 0.9712526798248291 val loss 1.022019624710083\n",
      "Epoch 8280: train loss 1.022019624710083 val loss 0.9614065885543823\n",
      "Epoch 8281: train loss 0.9614065885543823 val loss 1.0381519794464111\n",
      "Epoch 8282: train loss 1.0381519794464111 val loss 0.9647945761680603\n",
      "Epoch 8283: train loss 0.9647945761680603 val loss 1.0146911144256592\n",
      "Epoch 8284: train loss 1.0146911144256592 val loss 0.9844697713851929\n",
      "Epoch 8285: train loss 0.9844697713851929 val loss 0.9521515965461731\n",
      "Epoch 8286: train loss 0.9521515965461731 val loss 0.974854588508606\n",
      "Epoch 8287: train loss 0.974854588508606 val loss 0.9692143797874451\n",
      "Epoch 8288: train loss 0.9692143797874451 val loss 0.976127564907074\n",
      "Epoch 8289: train loss 0.976127564907074 val loss 0.9522958993911743\n",
      "Epoch 8290: train loss 0.9522958993911743 val loss 0.9797203540802002\n",
      "Epoch 8291: train loss 0.9797203540802002 val loss 1.0127829313278198\n",
      "Epoch 8292: train loss 1.0127829313278198 val loss 0.9616585969924927\n",
      "Epoch 8293: train loss 0.9616585969924927 val loss 1.0243223905563354\n",
      "Epoch 8294: train loss 1.0243223905563354 val loss 0.9567813873291016\n",
      "Epoch 8295: train loss 0.9567813873291016 val loss 0.9758783578872681\n",
      "Epoch 8296: train loss 0.9758783578872681 val loss 0.9789607524871826\n",
      "Epoch 8297: train loss 0.9789607524871826 val loss 0.9636725187301636\n",
      "Epoch 8298: train loss 0.9636725187301636 val loss 0.9638104438781738\n",
      "Epoch 8299: train loss 0.9638104438781738 val loss 0.966978907585144\n",
      "Epoch 8300: train loss 0.966978907585144 val loss 0.9935631155967712\n",
      "Epoch 8301: train loss 0.9935631155967712 val loss 0.9525079727172852\n",
      "Epoch 8302: train loss 0.9525079727172852 val loss 0.9997764825820923\n",
      "Epoch 8303: train loss 0.9997764825820923 val loss 0.9762245416641235\n",
      "Epoch 8304: train loss 0.9762245416641235 val loss 0.9586736559867859\n",
      "Epoch 8305: train loss 0.9586736559867859 val loss 0.9976885914802551\n",
      "Epoch 8306: train loss 0.9976885914802551 val loss 0.9633215665817261\n",
      "Epoch 8307: train loss 0.9633215665817261 val loss 0.9825363159179688\n",
      "Epoch 8308: train loss 0.9825363159179688 val loss 0.992224931716919\n",
      "Epoch 8309: train loss 0.992224931716919 val loss 0.9521878957748413\n",
      "Epoch 8310: train loss 0.9521878957748413 val loss 0.9838367700576782\n",
      "Epoch 8311: train loss 0.9838367700576782 val loss 0.9584521055221558\n",
      "Epoch 8312: train loss 0.9584521055221558 val loss 0.9564535021781921\n",
      "Epoch 8313: train loss 0.9564535021781921 val loss 0.9598486423492432\n",
      "Epoch 8314: train loss 0.9598486423492432 val loss 0.9525301456451416\n",
      "Epoch 8315: train loss 0.9525301456451416 val loss 0.9797108173370361\n",
      "Epoch 8316: train loss 0.9797108173370361 val loss 0.9963265657424927\n",
      "Epoch 8317: train loss 0.9963265657424927 val loss 0.9514700174331665\n",
      "Epoch 8318: train loss 0.9514700174331665 val loss 1.0069013833999634\n",
      "Epoch 8319: train loss 1.0069013833999634 val loss 0.9632560014724731\n",
      "Epoch 8320: train loss 0.9632560014724731 val loss 0.9773012399673462\n",
      "Epoch 8321: train loss 0.9773012399673462 val loss 1.0020595788955688\n",
      "Epoch 8322: train loss 1.0020595788955688 val loss 0.9578473567962646\n",
      "Epoch 8323: train loss 0.9578473567962646 val loss 0.9682230949401855\n",
      "Epoch 8324: train loss 0.9682230949401855 val loss 0.9649128317832947\n",
      "Epoch 8325: train loss 0.9649128317832947 val loss 0.968005895614624\n",
      "Epoch 8326: train loss 0.968005895614624 val loss 0.9563090801239014\n",
      "Epoch 8327: train loss 0.9563090801239014 val loss 0.9888875484466553\n",
      "Epoch 8328: train loss 0.9888875484466553 val loss 0.9715421199798584\n",
      "Epoch 8329: train loss 0.9715421199798584 val loss 0.9588107466697693\n",
      "Epoch 8330: train loss 0.9588107466697693 val loss 0.9858108162879944\n",
      "Epoch 8331: train loss 0.9858108162879944 val loss 0.9710432887077332\n",
      "Epoch 8332: train loss 0.9710432887077332 val loss 0.9512940645217896\n",
      "Epoch 8333: train loss 0.9512940645217896 val loss 0.9644873142242432\n",
      "Epoch 8334: train loss 0.9644873142242432 val loss 1.0172195434570312\n",
      "Epoch 8335: train loss 1.0172195434570312 val loss 0.953713059425354\n",
      "Epoch 8336: train loss 0.953713059425354 val loss 1.0699368715286255\n",
      "Epoch 8337: train loss 1.0699368715286255 val loss 0.9512708187103271\n",
      "Epoch 8338: train loss 0.9512708187103271 val loss 1.040040373802185\n",
      "Epoch 8339: train loss 1.040040373802185 val loss 0.9529162645339966\n",
      "Epoch 8340: train loss 0.9529162645339966 val loss 1.0125008821487427\n",
      "Epoch 8341: train loss 1.0125008821487427 val loss 0.9926446080207825\n",
      "Epoch 8342: train loss 0.9926446080207825 val loss 0.958325982093811\n",
      "Epoch 8343: train loss 0.958325982093811 val loss 1.017277717590332\n",
      "Epoch 8344: train loss 1.017277717590332 val loss 0.9610092639923096\n",
      "Epoch 8345: train loss 0.9610092639923096 val loss 0.9871786832809448\n",
      "Epoch 8346: train loss 0.9871786832809448 val loss 0.9843459129333496\n",
      "Epoch 8347: train loss 0.9843459129333496 val loss 0.9532859921455383\n",
      "Epoch 8348: train loss 0.9532859921455383 val loss 0.9554613828659058\n",
      "Epoch 8349: train loss 0.9554613828659058 val loss 0.9677156209945679\n",
      "Epoch 8350: train loss 0.9677156209945679 val loss 1.0049641132354736\n",
      "Epoch 8351: train loss 1.0049641132354736 val loss 0.9521595239639282\n",
      "Epoch 8352: train loss 0.9521595239639282 val loss 1.0642743110656738\n",
      "Epoch 8353: train loss 1.0642743110656738 val loss 0.9544191360473633\n",
      "Epoch 8354: train loss 0.9544191360473633 val loss 0.9929313659667969\n",
      "Epoch 8355: train loss 0.9929313659667969 val loss 0.9912586212158203\n",
      "Epoch 8356: train loss 0.9912586212158203 val loss 0.9499503374099731\n",
      "Epoch 8357: train loss 0.9499503374099731 val loss 0.9625972509384155\n",
      "Epoch 8358: train loss 0.9625972509384155 val loss 0.9499099850654602\n",
      "Epoch 8359: train loss 0.9499099850654602 val loss 0.9767026901245117\n",
      "Epoch 8360: train loss 0.9767026901245117 val loss 1.0096631050109863\n",
      "Epoch 8361: train loss 1.0096631050109863 val loss 0.9588618278503418\n",
      "Epoch 8362: train loss 0.9588618278503418 val loss 1.022628664970398\n",
      "Epoch 8363: train loss 1.022628664970398 val loss 0.9592984914779663\n",
      "Epoch 8364: train loss 0.9592984914779663 val loss 0.9854307174682617\n",
      "Epoch 8365: train loss 0.9854307174682617 val loss 0.982304573059082\n",
      "Epoch 8366: train loss 0.982304573059082 val loss 0.9541203379631042\n",
      "Epoch 8367: train loss 0.9541203379631042 val loss 0.9544774293899536\n",
      "Epoch 8368: train loss 0.9544774293899536 val loss 0.967150092124939\n",
      "Epoch 8369: train loss 0.967150092124939 val loss 1.008424997329712\n",
      "Epoch 8370: train loss 1.008424997329712 val loss 0.9536271095275879\n",
      "Epoch 8371: train loss 0.9536271095275879 val loss 1.0519065856933594\n",
      "Epoch 8372: train loss 1.0519065856933594 val loss 0.956550121307373\n",
      "Epoch 8373: train loss 0.956550121307373 val loss 0.997891902923584\n",
      "Epoch 8374: train loss 0.997891902923584 val loss 0.992193877696991\n",
      "Epoch 8375: train loss 0.992193877696991 val loss 0.9516963958740234\n",
      "Epoch 8376: train loss 0.9516963958740234 val loss 0.9847039580345154\n",
      "Epoch 8377: train loss 0.9847039580345154 val loss 0.9563484191894531\n",
      "Epoch 8378: train loss 0.9563484191894531 val loss 0.9684854745864868\n",
      "Epoch 8379: train loss 0.9684854745864868 val loss 1.050823450088501\n",
      "Epoch 8380: train loss 1.050823450088501 val loss 0.962940514087677\n",
      "Epoch 8381: train loss 0.962940514087677 val loss 1.0529844760894775\n",
      "Epoch 8382: train loss 1.0529844760894775 val loss 0.9630491137504578\n",
      "Epoch 8383: train loss 0.9630491137504578 val loss 1.0114703178405762\n",
      "Epoch 8384: train loss 1.0114703178405762 val loss 0.9592798948287964\n",
      "Epoch 8385: train loss 0.9592798948287964 val loss 0.9825665950775146\n",
      "Epoch 8386: train loss 0.9825665950775146 val loss 1.0255954265594482\n",
      "Epoch 8387: train loss 1.0255954265594482 val loss 0.9726468324661255\n",
      "Epoch 8388: train loss 0.9726468324661255 val loss 1.0844331979751587\n",
      "Epoch 8389: train loss 1.0844331979751587 val loss 0.9679422378540039\n",
      "Epoch 8390: train loss 0.9679422378540039 val loss 1.1971766948699951\n",
      "Epoch 8391: train loss 1.1971766948699951 val loss 1.1155411005020142\n",
      "Epoch 8392: train loss 1.1155411005020142 val loss 1.082977533340454\n",
      "Epoch 8393: train loss 1.082977533340454 val loss 1.093650221824646\n",
      "Epoch 8394: train loss 1.093650221824646 val loss 1.0361818075180054\n",
      "Epoch 8395: train loss 1.0361818075180054 val loss 1.0839078426361084\n",
      "Epoch 8396: train loss 1.0839078426361084 val loss 1.003715991973877\n",
      "Epoch 8397: train loss 1.003715991973877 val loss 0.992666482925415\n",
      "Epoch 8398: train loss 0.992666482925415 val loss 1.0841467380523682\n",
      "Epoch 8399: train loss 1.0841467380523682 val loss 1.0120587348937988\n",
      "Epoch 8400: train loss 1.0120587348937988 val loss 1.1014525890350342\n",
      "Epoch 8401: train loss 1.1014525890350342 val loss 1.0283482074737549\n",
      "Epoch 8402: train loss 1.0283482074737549 val loss 1.1625940799713135\n",
      "Epoch 8403: train loss 1.1625940799713135 val loss 1.1406159400939941\n",
      "Epoch 8404: train loss 1.1406159400939941 val loss 1.0397133827209473\n",
      "Epoch 8405: train loss 1.0397133827209473 val loss 1.0553232431411743\n",
      "Epoch 8406: train loss 1.0553232431411743 val loss 1.0582119226455688\n",
      "Epoch 8407: train loss 1.0582119226455688 val loss 1.024327039718628\n",
      "Epoch 8408: train loss 1.024327039718628 val loss 1.0398272275924683\n",
      "Epoch 8409: train loss 1.0398272275924683 val loss 0.9692877531051636\n",
      "Epoch 8410: train loss 0.9692877531051636 val loss 1.0706911087036133\n",
      "Epoch 8411: train loss 1.0706911087036133 val loss 0.976325273513794\n",
      "Epoch 8412: train loss 0.976325273513794 val loss 1.111443042755127\n",
      "Epoch 8413: train loss 1.111443042755127 val loss 1.010624647140503\n",
      "Epoch 8414: train loss 1.010624647140503 val loss 1.1445374488830566\n",
      "Epoch 8415: train loss 1.1445374488830566 val loss 1.0851860046386719\n",
      "Epoch 8416: train loss 1.0851860046386719 val loss 1.0833845138549805\n",
      "Epoch 8417: train loss 1.0833845138549805 val loss 1.0866000652313232\n",
      "Epoch 8418: train loss 1.0866000652313232 val loss 1.0397114753723145\n",
      "Epoch 8419: train loss 1.0397114753723145 val loss 1.0508930683135986\n",
      "Epoch 8420: train loss 1.0508930683135986 val loss 1.007346510887146\n",
      "Epoch 8421: train loss 1.007346510887146 val loss 0.9838703870773315\n",
      "Epoch 8422: train loss 0.9838703870773315 val loss 1.048352599143982\n",
      "Epoch 8423: train loss 1.048352599143982 val loss 0.9943504333496094\n",
      "Epoch 8424: train loss 0.9943504333496094 val loss 1.0479352474212646\n",
      "Epoch 8425: train loss 1.0479352474212646 val loss 0.9568446278572083\n",
      "Epoch 8426: train loss 0.9568446278572083 val loss 0.9817588329315186\n",
      "Epoch 8427: train loss 0.9817588329315186 val loss 0.959429144859314\n",
      "Epoch 8428: train loss 0.959429144859314 val loss 1.0068340301513672\n",
      "Epoch 8429: train loss 1.0068340301513672 val loss 0.9627474546432495\n",
      "Epoch 8430: train loss 0.9627474546432495 val loss 0.9838021993637085\n",
      "Epoch 8431: train loss 0.9838021993637085 val loss 0.9803858995437622\n",
      "Epoch 8432: train loss 0.9803858995437622 val loss 0.9525651335716248\n",
      "Epoch 8433: train loss 0.9525651335716248 val loss 0.9668766260147095\n",
      "Epoch 8434: train loss 0.9668766260147095 val loss 0.9693022966384888\n",
      "Epoch 8435: train loss 0.9693022966384888 val loss 0.9728261232376099\n",
      "Epoch 8436: train loss 0.9728261232376099 val loss 0.9569947719573975\n",
      "Epoch 8437: train loss 0.9569947719573975 val loss 0.9856946468353271\n",
      "Epoch 8438: train loss 0.9856946468353271 val loss 0.9658198356628418\n",
      "Epoch 8439: train loss 0.9658198356628418 val loss 0.9669021368026733\n",
      "Epoch 8440: train loss 0.9669021368026733 val loss 0.974309504032135\n",
      "Epoch 8441: train loss 0.974309504032135 val loss 0.9649652242660522\n",
      "Epoch 8442: train loss 0.9649652242660522 val loss 0.9682382345199585\n",
      "Epoch 8443: train loss 0.9682382345199585 val loss 0.9799017906188965\n",
      "Epoch 8444: train loss 0.9799017906188965 val loss 0.9591129422187805\n",
      "Epoch 8445: train loss 0.9591129422187805 val loss 0.9675619602203369\n",
      "Epoch 8446: train loss 0.9675619602203369 val loss 0.9809491634368896\n",
      "Epoch 8447: train loss 0.9809491634368896 val loss 0.9561595916748047\n",
      "Epoch 8448: train loss 0.9561595916748047 val loss 0.9607839584350586\n",
      "Epoch 8449: train loss 0.9607839584350586 val loss 0.9724394083023071\n",
      "Epoch 8450: train loss 0.9724394083023071 val loss 0.9580198526382446\n",
      "Epoch 8451: train loss 0.9580198526382446 val loss 0.9603485465049744\n",
      "Epoch 8452: train loss 0.9603485465049744 val loss 0.9877955913543701\n",
      "Epoch 8453: train loss 0.9877955913543701 val loss 0.9550230503082275\n",
      "Epoch 8454: train loss 0.9550230503082275 val loss 0.9613140821456909\n",
      "Epoch 8455: train loss 0.9613140821456909 val loss 0.9853401184082031\n",
      "Epoch 8456: train loss 0.9853401184082031 val loss 0.9552688598632812\n",
      "Epoch 8457: train loss 0.9552688598632812 val loss 0.9627424478530884\n",
      "Epoch 8458: train loss 0.9627424478530884 val loss 0.9822620749473572\n",
      "Epoch 8459: train loss 0.9822620749473572 val loss 0.951129674911499\n",
      "Epoch 8460: train loss 0.951129674911499 val loss 0.9592881202697754\n",
      "Epoch 8461: train loss 0.9592881202697754 val loss 0.9510915279388428\n",
      "Epoch 8462: train loss 0.9510915279388428 val loss 0.9522837400436401\n",
      "Epoch 8463: train loss 0.9522837400436401 val loss 0.9637680053710938\n",
      "Epoch 8464: train loss 0.9637680053710938 val loss 0.9516533017158508\n",
      "Epoch 8465: train loss 0.9516533017158508 val loss 0.9571114778518677\n",
      "Epoch 8466: train loss 0.9571114778518677 val loss 0.9934836626052856\n",
      "Epoch 8467: train loss 0.9934836626052856 val loss 0.9492599964141846\n",
      "Epoch 8468: train loss 0.9492599964141846 val loss 0.9702945947647095\n",
      "Epoch 8469: train loss 0.9702945947647095 val loss 0.9978101253509521\n",
      "Epoch 8470: train loss 0.9978101253509521 val loss 0.953920841217041\n",
      "Epoch 8471: train loss 0.953920841217041 val loss 1.004770278930664\n",
      "Epoch 8472: train loss 1.004770278930664 val loss 0.9654641151428223\n",
      "Epoch 8473: train loss 0.9654641151428223 val loss 1.0011086463928223\n",
      "Epoch 8474: train loss 1.0011086463928223 val loss 0.9842879772186279\n",
      "Epoch 8475: train loss 0.9842879772186279 val loss 0.9505332708358765\n",
      "Epoch 8476: train loss 0.9505332708358765 val loss 0.9739122986793518\n",
      "Epoch 8477: train loss 0.9739122986793518 val loss 0.9512472748756409\n",
      "Epoch 8478: train loss 0.9512472748756409 val loss 0.9783488512039185\n",
      "Epoch 8479: train loss 0.9783488512039185 val loss 1.0027192831039429\n",
      "Epoch 8480: train loss 1.0027192831039429 val loss 0.9596232175827026\n",
      "Epoch 8481: train loss 0.9596232175827026 val loss 1.0028672218322754\n",
      "Epoch 8482: train loss 1.0028672218322754 val loss 0.9612419605255127\n",
      "Epoch 8483: train loss 0.9612419605255127 val loss 0.9866994619369507\n",
      "Epoch 8484: train loss 0.9866994619369507 val loss 1.0054357051849365\n",
      "Epoch 8485: train loss 1.0054357051849365 val loss 0.9499666690826416\n",
      "Epoch 8486: train loss 0.9499666690826416 val loss 0.9825925827026367\n",
      "Epoch 8487: train loss 0.9825925827026367 val loss 0.9640631079673767\n",
      "Epoch 8488: train loss 0.9640631079673767 val loss 0.9598550200462341\n",
      "Epoch 8489: train loss 0.9598550200462341 val loss 0.9607920050621033\n",
      "Epoch 8490: train loss 0.9607920050621033 val loss 0.9539234638214111\n",
      "Epoch 8491: train loss 0.9539234638214111 val loss 0.9555902481079102\n",
      "Epoch 8492: train loss 0.9555902481079102 val loss 0.9654004573822021\n",
      "Epoch 8493: train loss 0.9654004573822021 val loss 0.9506441950798035\n",
      "Epoch 8494: train loss 0.9506441950798035 val loss 1.000167727470398\n",
      "Epoch 8495: train loss 1.000167727470398 val loss 0.9536253213882446\n",
      "Epoch 8496: train loss 0.9536253213882446 val loss 0.9667177200317383\n",
      "Epoch 8497: train loss 0.9667177200317383 val loss 0.9859325885772705\n",
      "Epoch 8498: train loss 0.9859325885772705 val loss 0.9524757862091064\n",
      "Epoch 8499: train loss 0.9524757862091064 val loss 0.9607301950454712\n",
      "Epoch 8500: train loss 0.9607301950454712 val loss 0.950698971748352\n",
      "Epoch 8501: train loss 0.950698971748352 val loss 0.9512403011322021\n",
      "Epoch 8502: train loss 0.9512403011322021 val loss 0.954839289188385\n",
      "Epoch 8503: train loss 0.954839289188385 val loss 0.9489051103591919\n",
      "Epoch 8504: train loss 0.9489051103591919 val loss 0.9564936757087708\n",
      "Epoch 8505: train loss 0.9564936757087708 val loss 0.9645663499832153\n",
      "Epoch 8506: train loss 0.9645663499832153 val loss 0.9483671188354492\n",
      "Epoch 8507: train loss 0.9483671188354492 val loss 0.9700846672058105\n",
      "Epoch 8508: train loss 0.9700846672058105 val loss 0.9891330003738403\n",
      "Epoch 8509: train loss 0.9891330003738403 val loss 0.9476950168609619\n",
      "Epoch 8510: train loss 0.9476950168609619 val loss 0.9734286069869995\n",
      "Epoch 8511: train loss 0.9734286069869995 val loss 1.0009735822677612\n",
      "Epoch 8512: train loss 1.0009735822677612 val loss 0.9537345170974731\n",
      "Epoch 8513: train loss 0.9537345170974731 val loss 1.0295618772506714\n",
      "Epoch 8514: train loss 1.0295618772506714 val loss 0.9584659337997437\n",
      "Epoch 8515: train loss 0.9584659337997437 val loss 0.995589017868042\n",
      "Epoch 8516: train loss 0.995589017868042 val loss 0.9859921932220459\n",
      "Epoch 8517: train loss 0.9859921932220459 val loss 0.9480048418045044\n",
      "Epoch 8518: train loss 0.9480048418045044 val loss 1.0014437437057495\n",
      "Epoch 8519: train loss 1.0014437437057495 val loss 0.9484233856201172\n",
      "Epoch 8520: train loss 0.9484233856201172 val loss 1.0003714561462402\n",
      "Epoch 8521: train loss 1.0003714561462402 val loss 0.9829031229019165\n",
      "Epoch 8522: train loss 0.9829031229019165 val loss 0.962249755859375\n",
      "Epoch 8523: train loss 0.962249755859375 val loss 0.9800238609313965\n",
      "Epoch 8524: train loss 0.9800238609313965 val loss 0.9573385119438171\n",
      "Epoch 8525: train loss 0.9573385119438171 val loss 0.9676697254180908\n",
      "Epoch 8526: train loss 0.9676697254180908 val loss 0.9806734323501587\n",
      "Epoch 8527: train loss 0.9806734323501587 val loss 0.9543964862823486\n",
      "Epoch 8528: train loss 0.9543964862823486 val loss 0.9586164951324463\n",
      "Epoch 8529: train loss 0.9586164951324463 val loss 0.9491606950759888\n",
      "Epoch 8530: train loss 0.9491606950759888 val loss 0.9479355812072754\n",
      "Epoch 8531: train loss 0.9479355812072754 val loss 0.9573754072189331\n",
      "Epoch 8532: train loss 0.9573754072189331 val loss 0.9474600553512573\n",
      "Epoch 8533: train loss 0.9474600553512573 val loss 0.9623388051986694\n",
      "Epoch 8534: train loss 0.9623388051986694 val loss 0.9775763154029846\n",
      "Epoch 8535: train loss 0.9775763154029846 val loss 0.9471136331558228\n",
      "Epoch 8536: train loss 0.9471136331558228 val loss 0.9882962703704834\n",
      "Epoch 8537: train loss 0.9882962703704834 val loss 0.9677258729934692\n",
      "Epoch 8538: train loss 0.9677258729934692 val loss 0.9526078104972839\n",
      "Epoch 8539: train loss 0.9526078104972839 val loss 0.9802062511444092\n",
      "Epoch 8540: train loss 0.9802062511444092 val loss 0.9669042825698853\n",
      "Epoch 8541: train loss 0.9669042825698853 val loss 0.9531307220458984\n",
      "Epoch 8542: train loss 0.9531307220458984 val loss 0.9749093055725098\n",
      "Epoch 8543: train loss 0.9749093055725098 val loss 0.9773272275924683\n",
      "Epoch 8544: train loss 0.9773272275924683 val loss 0.9480355978012085\n",
      "Epoch 8545: train loss 0.9480355978012085 val loss 0.9721113443374634\n",
      "Epoch 8546: train loss 0.9721113443374634 val loss 1.0028295516967773\n",
      "Epoch 8547: train loss 1.0028295516967773 val loss 0.9554784297943115\n",
      "Epoch 8548: train loss 0.9554784297943115 val loss 1.016410231590271\n",
      "Epoch 8549: train loss 1.016410231590271 val loss 0.9514467716217041\n",
      "Epoch 8550: train loss 0.9514467716217041 val loss 0.9711676836013794\n",
      "Epoch 8551: train loss 0.9711676836013794 val loss 0.9695676565170288\n",
      "Epoch 8552: train loss 0.9695676565170288 val loss 0.9583369493484497\n",
      "Epoch 8553: train loss 0.9583369493484497 val loss 0.9574511647224426\n",
      "Epoch 8554: train loss 0.9574511647224426 val loss 0.9560596942901611\n",
      "Epoch 8555: train loss 0.9560596942901611 val loss 0.9495139718055725\n",
      "Epoch 8556: train loss 0.9495139718055725 val loss 0.946629524230957\n",
      "Epoch 8557: train loss 0.946629524230957 val loss 0.946890115737915\n",
      "Epoch 8558: train loss 0.946890115737915 val loss 0.9531017541885376\n",
      "Epoch 8559: train loss 0.9531017541885376 val loss 0.9464548826217651\n",
      "Epoch 8560: train loss 0.9464548826217651 val loss 0.9456208944320679\n",
      "Epoch 8561: train loss 0.9456208944320679 val loss 0.9459631443023682\n",
      "Epoch 8562: train loss 0.9459631443023682 val loss 0.9502735137939453\n",
      "Epoch 8563: train loss 0.9502735137939453 val loss 0.9469457864761353\n",
      "Epoch 8564: train loss 0.9469457864761353 val loss 0.9454805254936218\n",
      "Epoch 8565: train loss 0.9454805254936218 val loss 0.9448981285095215\n",
      "Epoch 8566: train loss 0.9448981285095215 val loss 0.9467229843139648\n",
      "Epoch 8567: train loss 0.9467229843139648 val loss 0.9518073797225952\n",
      "Epoch 8568: train loss 0.9518073797225952 val loss 0.9460080862045288\n",
      "Epoch 8569: train loss 0.9460080862045288 val loss 0.9498526453971863\n",
      "Epoch 8570: train loss 0.9498526453971863 val loss 0.9526399374008179\n",
      "Epoch 8571: train loss 0.9526399374008179 val loss 0.946634829044342\n",
      "Epoch 8572: train loss 0.946634829044342 val loss 0.946106493473053\n",
      "Epoch 8573: train loss 0.946106493473053 val loss 0.9469783306121826\n",
      "Epoch 8574: train loss 0.9469783306121826 val loss 0.9520273804664612\n",
      "Epoch 8575: train loss 0.9520273804664612 val loss 0.9519056677818298\n",
      "Epoch 8576: train loss 0.9519056677818298 val loss 0.9473152160644531\n",
      "Epoch 8577: train loss 0.9473152160644531 val loss 0.9479756355285645\n",
      "Epoch 8578: train loss 0.9479756355285645 val loss 0.9492080211639404\n",
      "Epoch 8579: train loss 0.9492080211639404 val loss 0.9486932754516602\n",
      "Epoch 8580: train loss 0.9486932754516602 val loss 0.945881724357605\n",
      "Epoch 8581: train loss 0.945881724357605 val loss 0.9502153396606445\n",
      "Epoch 8582: train loss 0.9502153396606445 val loss 0.951727569103241\n",
      "Epoch 8583: train loss 0.951727569103241 val loss 0.9467741250991821\n",
      "Epoch 8584: train loss 0.9467741250991821 val loss 0.9460060596466064\n",
      "Epoch 8585: train loss 0.9460060596466064 val loss 0.9457079172134399\n",
      "Epoch 8586: train loss 0.9457079172134399 val loss 0.9449505805969238\n",
      "Epoch 8587: train loss 0.9449505805969238 val loss 0.9472231864929199\n",
      "Epoch 8588: train loss 0.9472231864929199 val loss 0.9486663341522217\n",
      "Epoch 8589: train loss 0.9486663341522217 val loss 0.9485900402069092\n",
      "Epoch 8590: train loss 0.9485900402069092 val loss 0.944827675819397\n",
      "Epoch 8591: train loss 0.944827675819397 val loss 0.9515529870986938\n",
      "Epoch 8592: train loss 0.9515529870986938 val loss 0.9505021572113037\n",
      "Epoch 8593: train loss 0.9505021572113037 val loss 0.9481643438339233\n",
      "Epoch 8594: train loss 0.9481643438339233 val loss 0.9452060461044312\n",
      "Epoch 8595: train loss 0.9452060461044312 val loss 0.9490352869033813\n",
      "Epoch 8596: train loss 0.9490352869033813 val loss 0.9532809257507324\n",
      "Epoch 8597: train loss 0.9532809257507324 val loss 0.9594162702560425\n",
      "Epoch 8598: train loss 0.9594162702560425 val loss 0.9484293460845947\n",
      "Epoch 8599: train loss 0.9484293460845947 val loss 0.9981812238693237\n",
      "Epoch 8600: train loss 0.9981812238693237 val loss 0.9755994081497192\n",
      "Epoch 8601: train loss 0.9755994081497192 val loss 0.9462862014770508\n",
      "Epoch 8602: train loss 0.9462862014770508 val loss 0.985012948513031\n",
      "Epoch 8603: train loss 0.985012948513031 val loss 0.9571244120597839\n",
      "Epoch 8604: train loss 0.9571244120597839 val loss 0.9666815996170044\n",
      "Epoch 8605: train loss 0.9666815996170044 val loss 1.0162765979766846\n",
      "Epoch 8606: train loss 1.0162765979766846 val loss 0.9444546699523926\n",
      "Epoch 8607: train loss 0.9444546699523926 val loss 1.0048584938049316\n",
      "Epoch 8608: train loss 1.0048584938049316 val loss 0.9528424739837646\n",
      "Epoch 8609: train loss 0.9528424739837646 val loss 0.9484361410140991\n",
      "Epoch 8610: train loss 0.9484361410140991 val loss 0.9821392893791199\n",
      "Epoch 8611: train loss 0.9821392893791199 val loss 0.9623744487762451\n",
      "Epoch 8612: train loss 0.9623744487762451 val loss 0.9510254859924316\n",
      "Epoch 8613: train loss 0.9510254859924316 val loss 0.9497841596603394\n",
      "Epoch 8614: train loss 0.9497841596603394 val loss 0.9676347970962524\n",
      "Epoch 8615: train loss 0.9676347970962524 val loss 0.9919023513793945\n",
      "Epoch 8616: train loss 0.9919023513793945 val loss 0.9459296464920044\n",
      "Epoch 8617: train loss 0.9459296464920044 val loss 0.9987941980361938\n",
      "Epoch 8618: train loss 0.9987941980361938 val loss 0.9732666015625\n",
      "Epoch 8619: train loss 0.9732666015625 val loss 0.9585156440734863\n",
      "Epoch 8620: train loss 0.9585156440734863 val loss 0.9774123430252075\n",
      "Epoch 8621: train loss 0.9774123430252075 val loss 0.9583549499511719\n",
      "Epoch 8622: train loss 0.9583549499511719 val loss 0.9646248817443848\n",
      "Epoch 8623: train loss 0.9646248817443848 val loss 0.9957489967346191\n",
      "Epoch 8624: train loss 0.9957489967346191 val loss 0.9511774778366089\n",
      "Epoch 8625: train loss 0.9511774778366089 val loss 0.9592523574829102\n",
      "Epoch 8626: train loss 0.9592523574829102 val loss 0.9457769393920898\n",
      "Epoch 8627: train loss 0.9457769393920898 val loss 0.9474524855613708\n",
      "Epoch 8628: train loss 0.9474524855613708 val loss 0.9491393566131592\n",
      "Epoch 8629: train loss 0.9491393566131592 val loss 0.9452627897262573\n",
      "Epoch 8630: train loss 0.9452627897262573 val loss 0.9449687004089355\n",
      "Epoch 8631: train loss 0.9449687004089355 val loss 0.9444707632064819\n",
      "Epoch 8632: train loss 0.9444707632064819 val loss 0.9447256326675415\n",
      "Epoch 8633: train loss 0.9447256326675415 val loss 0.9500754475593567\n",
      "Epoch 8634: train loss 0.9500754475593567 val loss 0.9472407102584839\n",
      "Epoch 8635: train loss 0.9472407102584839 val loss 0.9438352584838867\n",
      "Epoch 8636: train loss 0.9438352584838867 val loss 0.9458557963371277\n",
      "Epoch 8637: train loss 0.9458557963371277 val loss 0.953964352607727\n",
      "Epoch 8638: train loss 0.953964352607727 val loss 0.9549736380577087\n",
      "Epoch 8639: train loss 0.9549736380577087 val loss 0.9465000629425049\n",
      "Epoch 8640: train loss 0.9465000629425049 val loss 0.9788187742233276\n",
      "Epoch 8641: train loss 0.9788187742233276 val loss 0.9748626947402954\n",
      "Epoch 8642: train loss 0.9748626947402954 val loss 0.9455251693725586\n",
      "Epoch 8643: train loss 0.9455251693725586 val loss 0.9751396179199219\n",
      "Epoch 8644: train loss 0.9751396179199219 val loss 1.0006366968154907\n",
      "Epoch 8645: train loss 1.0006366968154907 val loss 0.9513987898826599\n",
      "Epoch 8646: train loss 0.9513987898826599 val loss 1.014075756072998\n",
      "Epoch 8647: train loss 1.014075756072998 val loss 0.9577723145484924\n",
      "Epoch 8648: train loss 0.9577723145484924 val loss 0.9953141212463379\n",
      "Epoch 8649: train loss 0.9953141212463379 val loss 0.9866085052490234\n",
      "Epoch 8650: train loss 0.9866085052490234 val loss 0.9455575942993164\n",
      "Epoch 8651: train loss 0.9455575942993164 val loss 0.9931007623672485\n",
      "Epoch 8652: train loss 0.9931007623672485 val loss 0.9519835114479065\n",
      "Epoch 8653: train loss 0.9519835114479065 val loss 0.9592479467391968\n",
      "Epoch 8654: train loss 0.9592479467391968 val loss 1.0105499029159546\n",
      "Epoch 8655: train loss 1.0105499029159546 val loss 0.9502226710319519\n",
      "Epoch 8656: train loss 0.9502226710319519 val loss 1.0588507652282715\n",
      "Epoch 8657: train loss 1.0588507652282715 val loss 0.9469671249389648\n",
      "Epoch 8658: train loss 0.9469671249389648 val loss 0.995278000831604\n",
      "Epoch 8659: train loss 0.995278000831604 val loss 0.9673753976821899\n",
      "Epoch 8660: train loss 0.9673753976821899 val loss 0.9489848613739014\n",
      "Epoch 8661: train loss 0.9489848613739014 val loss 0.944921612739563\n",
      "Epoch 8662: train loss 0.944921612739563 val loss 0.974968433380127\n",
      "Epoch 8663: train loss 0.974968433380127 val loss 0.9854706525802612\n",
      "Epoch 8664: train loss 0.9854706525802612 val loss 0.9436730146408081\n",
      "Epoch 8665: train loss 0.9436730146408081 val loss 0.9715626835823059\n",
      "Epoch 8666: train loss 0.9715626835823059 val loss 1.0035970211029053\n",
      "Epoch 8667: train loss 1.0035970211029053 val loss 0.9522072076797485\n",
      "Epoch 8668: train loss 0.9522072076797485 val loss 1.027018666267395\n",
      "Epoch 8669: train loss 1.027018666267395 val loss 0.9590716361999512\n",
      "Epoch 8670: train loss 0.9590716361999512 val loss 1.0052666664123535\n",
      "Epoch 8671: train loss 1.0052666664123535 val loss 0.9745359420776367\n",
      "Epoch 8672: train loss 0.9745359420776367 val loss 0.9441050291061401\n",
      "Epoch 8673: train loss 0.9441050291061401 val loss 0.9701990485191345\n",
      "Epoch 8674: train loss 0.9701990485191345 val loss 0.9510339498519897\n",
      "Epoch 8675: train loss 0.9510339498519897 val loss 0.9577449560165405\n",
      "Epoch 8676: train loss 0.9577449560165405 val loss 1.0311567783355713\n",
      "Epoch 8677: train loss 1.0311567783355713 val loss 0.9524260759353638\n",
      "Epoch 8678: train loss 0.9524260759353638 val loss 1.0589183568954468\n",
      "Epoch 8679: train loss 1.0589183568954468 val loss 0.9471155405044556\n",
      "Epoch 8680: train loss 0.9471155405044556 val loss 0.9998430013656616\n",
      "Epoch 8681: train loss 0.9998430013656616 val loss 0.9516293406486511\n",
      "Epoch 8682: train loss 0.9516293406486511 val loss 0.9875681400299072\n",
      "Epoch 8683: train loss 0.9875681400299072 val loss 0.9986385703086853\n",
      "Epoch 8684: train loss 0.9986385703086853 val loss 0.954188346862793\n",
      "Epoch 8685: train loss 0.954188346862793 val loss 1.019856572151184\n",
      "Epoch 8686: train loss 1.019856572151184 val loss 0.9444073438644409\n",
      "Epoch 8687: train loss 0.9444073438644409 val loss 0.9696571826934814\n",
      "Epoch 8688: train loss 0.9696571826934814 val loss 0.9508964419364929\n",
      "Epoch 8689: train loss 0.9508964419364929 val loss 0.9450459480285645\n",
      "Epoch 8690: train loss 0.9450459480285645 val loss 0.9530717134475708\n",
      "Epoch 8691: train loss 0.9530717134475708 val loss 0.9440072774887085\n",
      "Epoch 8692: train loss 0.9440072774887085 val loss 0.9480790495872498\n",
      "Epoch 8693: train loss 0.9480790495872498 val loss 0.9528318643569946\n",
      "Epoch 8694: train loss 0.9528318643569946 val loss 0.9455718994140625\n",
      "Epoch 8695: train loss 0.9455718994140625 val loss 0.9818604588508606\n",
      "Epoch 8696: train loss 0.9818604588508606 val loss 0.9698377847671509\n",
      "Epoch 8697: train loss 0.9698377847671509 val loss 0.9453572034835815\n",
      "Epoch 8698: train loss 0.9453572034835815 val loss 1.007335901260376\n",
      "Epoch 8699: train loss 1.007335901260376 val loss 0.9564904570579529\n",
      "Epoch 8700: train loss 0.9564904570579529 val loss 0.989205002784729\n",
      "Epoch 8701: train loss 0.989205002784729 val loss 1.004376769065857\n",
      "Epoch 8702: train loss 1.004376769065857 val loss 0.944170355796814\n",
      "Epoch 8703: train loss 0.944170355796814 val loss 0.9825912714004517\n",
      "Epoch 8704: train loss 0.9825912714004517 val loss 0.9583280086517334\n",
      "Epoch 8705: train loss 0.9583280086517334 val loss 0.9444730281829834\n",
      "Epoch 8706: train loss 0.9444730281829834 val loss 0.9488420486450195\n",
      "Epoch 8707: train loss 0.9488420486450195 val loss 0.9507104158401489\n",
      "Epoch 8708: train loss 0.9507104158401489 val loss 0.9423894882202148\n",
      "Epoch 8709: train loss 0.9423894882202148 val loss 0.9658957719802856\n",
      "Epoch 8710: train loss 0.9658957719802856 val loss 0.9583434462547302\n",
      "Epoch 8711: train loss 0.9583434462547302 val loss 0.9456804990768433\n",
      "Epoch 8712: train loss 0.9456804990768433 val loss 0.9424149990081787\n",
      "Epoch 8713: train loss 0.9424149990081787 val loss 0.9501670598983765\n",
      "Epoch 8714: train loss 0.9501670598983765 val loss 0.9483472108840942\n",
      "Epoch 8715: train loss 0.9483472108840942 val loss 0.9427298307418823\n",
      "Epoch 8716: train loss 0.9427298307418823 val loss 0.9860005974769592\n",
      "Epoch 8717: train loss 0.9860005974769592 val loss 0.9684784412384033\n",
      "Epoch 8718: train loss 0.9684784412384033 val loss 0.9489301443099976\n",
      "Epoch 8719: train loss 0.9489301443099976 val loss 0.9947763085365295\n",
      "Epoch 8720: train loss 0.9947763085365295 val loss 0.964933454990387\n",
      "Epoch 8721: train loss 0.964933454990387 val loss 0.9496487379074097\n",
      "Epoch 8722: train loss 0.9496487379074097 val loss 0.9914200305938721\n",
      "Epoch 8723: train loss 0.9914200305938721 val loss 0.9663413763046265\n",
      "Epoch 8724: train loss 0.9663413763046265 val loss 0.9467047452926636\n",
      "Epoch 8725: train loss 0.9467047452926636 val loss 1.0009373426437378\n",
      "Epoch 8726: train loss 1.0009373426437378 val loss 0.9582957029342651\n",
      "Epoch 8727: train loss 0.9582957029342651 val loss 0.9434709548950195\n",
      "Epoch 8728: train loss 0.9434709548950195 val loss 0.9928947687149048\n",
      "Epoch 8729: train loss 0.9928947687149048 val loss 0.9568270444869995\n",
      "Epoch 8730: train loss 0.9568270444869995 val loss 0.9715229272842407\n",
      "Epoch 8731: train loss 0.9715229272842407 val loss 1.0413540601730347\n",
      "Epoch 8732: train loss 1.0413540601730347 val loss 0.9428156614303589\n",
      "Epoch 8733: train loss 0.9428156614303589 val loss 1.053025245666504\n",
      "Epoch 8734: train loss 1.053025245666504 val loss 0.9432512521743774\n",
      "Epoch 8735: train loss 0.9432512521743774 val loss 1.016050100326538\n",
      "Epoch 8736: train loss 1.016050100326538 val loss 0.9624011516571045\n",
      "Epoch 8737: train loss 0.9624011516571045 val loss 0.9491927623748779\n",
      "Epoch 8738: train loss 0.9491927623748779 val loss 0.9833565354347229\n",
      "Epoch 8739: train loss 0.9833565354347229 val loss 0.9550734758377075\n",
      "Epoch 8740: train loss 0.9550734758377075 val loss 0.9674054384231567\n",
      "Epoch 8741: train loss 0.9674054384231567 val loss 1.0431017875671387\n",
      "Epoch 8742: train loss 1.0431017875671387 val loss 0.9429054260253906\n",
      "Epoch 8743: train loss 0.9429054260253906 val loss 0.9875664710998535\n",
      "Epoch 8744: train loss 0.9875664710998535 val loss 0.962427020072937\n",
      "Epoch 8745: train loss 0.962427020072937 val loss 0.94430011510849\n",
      "Epoch 8746: train loss 0.94430011510849 val loss 0.9418387413024902\n",
      "Epoch 8747: train loss 0.9418387413024902 val loss 0.9515556693077087\n",
      "Epoch 8748: train loss 0.9515556693077087 val loss 0.9869996309280396\n",
      "Epoch 8749: train loss 0.9869996309280396 val loss 0.945685863494873\n",
      "Epoch 8750: train loss 0.945685863494873 val loss 0.9706671237945557\n",
      "Epoch 8751: train loss 0.9706671237945557 val loss 1.0192971229553223\n",
      "Epoch 8752: train loss 1.0192971229553223 val loss 0.9621028900146484\n",
      "Epoch 8753: train loss 0.9621028900146484 val loss 1.0770467519760132\n",
      "Epoch 8754: train loss 1.0770467519760132 val loss 0.9523458480834961\n",
      "Epoch 8755: train loss 0.9523458480834961 val loss 1.0718324184417725\n",
      "Epoch 8756: train loss 1.0718324184417725 val loss 0.9600062966346741\n",
      "Epoch 8757: train loss 0.9600062966346741 val loss 1.0567275285720825\n",
      "Epoch 8758: train loss 1.0567275285720825 val loss 0.9447963237762451\n",
      "Epoch 8759: train loss 0.9447963237762451 val loss 1.02692711353302\n",
      "Epoch 8760: train loss 1.02692711353302 val loss 0.9490097761154175\n",
      "Epoch 8761: train loss 0.9490097761154175 val loss 1.0692284107208252\n",
      "Epoch 8762: train loss 1.0692284107208252 val loss 0.9419214725494385\n",
      "Epoch 8763: train loss 0.9419214725494385 val loss 1.0407886505126953\n",
      "Epoch 8764: train loss 1.0407886505126953 val loss 0.9524083733558655\n",
      "Epoch 8765: train loss 0.9524083733558655 val loss 0.9783898591995239\n",
      "Epoch 8766: train loss 0.9783898591995239 val loss 1.0149047374725342\n",
      "Epoch 8767: train loss 1.0149047374725342 val loss 0.9588775038719177\n",
      "Epoch 8768: train loss 0.9588775038719177 val loss 1.0853768587112427\n",
      "Epoch 8769: train loss 1.0853768587112427 val loss 0.9572491645812988\n",
      "Epoch 8770: train loss 0.9572491645812988 val loss 1.1797305345535278\n",
      "Epoch 8771: train loss 1.1797305345535278 val loss 1.0853559970855713\n",
      "Epoch 8772: train loss 1.0853559970855713 val loss 1.1426520347595215\n",
      "Epoch 8773: train loss 1.1426520347595215 val loss 1.173296332359314\n",
      "Epoch 8774: train loss 1.173296332359314 val loss 1.018751859664917\n",
      "Epoch 8775: train loss 1.018751859664917 val loss 1.1262362003326416\n",
      "Epoch 8776: train loss 1.1262362003326416 val loss 0.955458402633667\n",
      "Epoch 8777: train loss 0.955458402633667 val loss 1.1547574996948242\n",
      "Epoch 8778: train loss 1.1547574996948242 val loss 0.948138952255249\n",
      "Epoch 8779: train loss 0.948138952255249 val loss 1.0479471683502197\n",
      "Epoch 8780: train loss 1.0479471683502197 val loss 0.9434441924095154\n",
      "Epoch 8781: train loss 0.9434441924095154 val loss 1.0387771129608154\n",
      "Epoch 8782: train loss 1.0387771129608154 val loss 0.9569458961486816\n",
      "Epoch 8783: train loss 0.9569458961486816 val loss 0.9572124481201172\n",
      "Epoch 8784: train loss 0.9572124481201172 val loss 1.002598762512207\n",
      "Epoch 8785: train loss 1.002598762512207 val loss 0.9428536295890808\n",
      "Epoch 8786: train loss 0.9428536295890808 val loss 0.9947406649589539\n",
      "Epoch 8787: train loss 0.9947406649589539 val loss 0.948060154914856\n",
      "Epoch 8788: train loss 0.948060154914856 val loss 0.9742729067802429\n",
      "Epoch 8789: train loss 0.9742729067802429 val loss 1.008402705192566\n",
      "Epoch 8790: train loss 1.008402705192566 val loss 0.9614104628562927\n",
      "Epoch 8791: train loss 0.9614104628562927 val loss 0.9700988531112671\n",
      "Epoch 8792: train loss 0.9700988531112671 val loss 0.9774652719497681\n",
      "Epoch 8793: train loss 0.9774652719497681 val loss 0.9608951807022095\n",
      "Epoch 8794: train loss 0.9608951807022095 val loss 0.9577243328094482\n",
      "Epoch 8795: train loss 0.9577243328094482 val loss 0.9480760097503662\n",
      "Epoch 8796: train loss 0.9480760097503662 val loss 0.9817132949829102\n",
      "Epoch 8797: train loss 0.9817132949829102 val loss 0.9487645626068115\n",
      "Epoch 8798: train loss 0.9487645626068115 val loss 0.9539069533348083\n",
      "Epoch 8799: train loss 0.9539069533348083 val loss 1.0039716958999634\n",
      "Epoch 8800: train loss 1.0039716958999634 val loss 0.9479758739471436\n",
      "Epoch 8801: train loss 0.9479758739471436 val loss 1.0326452255249023\n",
      "Epoch 8802: train loss 1.0326452255249023 val loss 0.9445086717605591\n",
      "Epoch 8803: train loss 0.9445086717605591 val loss 1.0387213230133057\n",
      "Epoch 8804: train loss 1.0387213230133057 val loss 0.9499034881591797\n",
      "Epoch 8805: train loss 0.9499034881591797 val loss 0.9812170267105103\n",
      "Epoch 8806: train loss 0.9812170267105103 val loss 1.0265295505523682\n",
      "Epoch 8807: train loss 1.0265295505523682 val loss 0.9811965823173523\n",
      "Epoch 8808: train loss 0.9811965823173523 val loss 1.0523455142974854\n",
      "Epoch 8809: train loss 1.0523455142974854 val loss 0.9527149200439453\n",
      "Epoch 8810: train loss 0.9527149200439453 val loss 1.0160069465637207\n",
      "Epoch 8811: train loss 1.0160069465637207 val loss 0.9601315259933472\n",
      "Epoch 8812: train loss 0.9601315259933472 val loss 0.9862790107727051\n",
      "Epoch 8813: train loss 0.9862790107727051 val loss 0.9580074548721313\n",
      "Epoch 8814: train loss 0.9580074548721313 val loss 0.9833219647407532\n",
      "Epoch 8815: train loss 0.9833219647407532 val loss 0.9619647264480591\n",
      "Epoch 8816: train loss 0.9619647264480591 val loss 0.9426221251487732\n",
      "Epoch 8817: train loss 0.9426221251487732 val loss 0.9520100355148315\n",
      "Epoch 8818: train loss 0.9520100355148315 val loss 0.9478346109390259\n",
      "Epoch 8819: train loss 0.9478346109390259 val loss 0.9427193403244019\n",
      "Epoch 8820: train loss 0.9427193403244019 val loss 0.9522297382354736\n",
      "Epoch 8821: train loss 0.9522297382354736 val loss 0.9408774375915527\n",
      "Epoch 8822: train loss 0.9408774375915527 val loss 0.9485628604888916\n",
      "Epoch 8823: train loss 0.9485628604888916 val loss 0.9446653127670288\n",
      "Epoch 8824: train loss 0.9446653127670288 val loss 0.9422996044158936\n",
      "Epoch 8825: train loss 0.9422996044158936 val loss 0.9502996206283569\n",
      "Epoch 8826: train loss 0.9502996206283569 val loss 0.9559546709060669\n",
      "Epoch 8827: train loss 0.9559546709060669 val loss 0.9466894865036011\n",
      "Epoch 8828: train loss 0.9466894865036011 val loss 0.9845446348190308\n",
      "Epoch 8829: train loss 0.9845446348190308 val loss 0.9533268213272095\n",
      "Epoch 8830: train loss 0.9533268213272095 val loss 0.9727286100387573\n",
      "Epoch 8831: train loss 0.9727286100387573 val loss 0.9796408414840698\n",
      "Epoch 8832: train loss 0.9796408414840698 val loss 0.9421426057815552\n",
      "Epoch 8833: train loss 0.9421426057815552 val loss 0.95447838306427\n",
      "Epoch 8834: train loss 0.95447838306427 val loss 0.9405204057693481\n",
      "Epoch 8835: train loss 0.9405204057693481 val loss 0.9580852389335632\n",
      "Epoch 8836: train loss 0.9580852389335632 val loss 0.9787420034408569\n",
      "Epoch 8837: train loss 0.9787420034408569 val loss 0.9410583972930908\n",
      "Epoch 8838: train loss 0.9410583972930908 val loss 0.9557727575302124\n",
      "Epoch 8839: train loss 0.9557727575302124 val loss 0.9881203174591064\n",
      "Epoch 8840: train loss 0.9881203174591064 val loss 0.9401817321777344\n",
      "Epoch 8841: train loss 0.9401817321777344 val loss 0.9813900589942932\n",
      "Epoch 8842: train loss 0.9813900589942932 val loss 0.9754079580307007\n",
      "Epoch 8843: train loss 0.9754079580307007 val loss 0.9409526586532593\n",
      "Epoch 8844: train loss 0.9409526586532593 val loss 1.0020101070404053\n",
      "Epoch 8845: train loss 1.0020101070404053 val loss 0.9660297632217407\n",
      "Epoch 8846: train loss 0.9660297632217407 val loss 0.9604929685592651\n",
      "Epoch 8847: train loss 0.9604929685592651 val loss 1.0104612112045288\n",
      "Epoch 8848: train loss 1.0104612112045288 val loss 0.9402956962585449\n",
      "Epoch 8849: train loss 0.9402956962585449 val loss 0.9664130210876465\n",
      "Epoch 8850: train loss 0.9664130210876465 val loss 0.948635458946228\n",
      "Epoch 8851: train loss 0.948635458946228 val loss 0.9492311477661133\n",
      "Epoch 8852: train loss 0.9492311477661133 val loss 0.9816616177558899\n",
      "Epoch 8853: train loss 0.9816616177558899 val loss 0.944118082523346\n",
      "Epoch 8854: train loss 0.944118082523346 val loss 0.9699835777282715\n",
      "Epoch 8855: train loss 0.9699835777282715 val loss 1.0120303630828857\n",
      "Epoch 8856: train loss 1.0120303630828857 val loss 0.9553444385528564\n",
      "Epoch 8857: train loss 0.9553444385528564 val loss 0.9946675896644592\n",
      "Epoch 8858: train loss 0.9946675896644592 val loss 0.9493341445922852\n",
      "Epoch 8859: train loss 0.9493341445922852 val loss 0.9731037616729736\n",
      "Epoch 8860: train loss 0.9731037616729736 val loss 0.9962379932403564\n",
      "Epoch 8861: train loss 0.9962379932403564 val loss 0.9429522752761841\n",
      "Epoch 8862: train loss 0.9429522752761841 val loss 0.9623988270759583\n",
      "Epoch 8863: train loss 0.9623988270759583 val loss 0.9584885239601135\n",
      "Epoch 8864: train loss 0.9584885239601135 val loss 0.9427222609519958\n",
      "Epoch 8865: train loss 0.9427222609519958 val loss 0.9436928033828735\n",
      "Epoch 8866: train loss 0.9436928033828735 val loss 0.947109043598175\n",
      "Epoch 8867: train loss 0.947109043598175 val loss 0.9416923522949219\n",
      "Epoch 8868: train loss 0.9416923522949219 val loss 0.9449378252029419\n",
      "Epoch 8869: train loss 0.9449378252029419 val loss 0.9482769966125488\n",
      "Epoch 8870: train loss 0.9482769966125488 val loss 0.94031822681427\n",
      "Epoch 8871: train loss 0.94031822681427 val loss 0.940710186958313\n",
      "Epoch 8872: train loss 0.940710186958313 val loss 0.9417372941970825\n",
      "Epoch 8873: train loss 0.9417372941970825 val loss 0.9445892572402954\n",
      "Epoch 8874: train loss 0.9445892572402954 val loss 0.9408522844314575\n",
      "Epoch 8875: train loss 0.9408522844314575 val loss 0.9403372406959534\n",
      "Epoch 8876: train loss 0.9403372406959534 val loss 0.9548605680465698\n",
      "Epoch 8877: train loss 0.9548605680465698 val loss 0.9687399864196777\n",
      "Epoch 8878: train loss 0.9687399864196777 val loss 0.9404448866844177\n",
      "Epoch 8879: train loss 0.9404448866844177 val loss 1.0196619033813477\n",
      "Epoch 8880: train loss 1.0196619033813477 val loss 0.9601031541824341\n",
      "Epoch 8881: train loss 0.9601031541824341 val loss 0.9603320360183716\n",
      "Epoch 8882: train loss 0.9603320360183716 val loss 1.0101189613342285\n",
      "Epoch 8883: train loss 1.0101189613342285 val loss 0.9421952962875366\n",
      "Epoch 8884: train loss 0.9421952962875366 val loss 0.9590461254119873\n",
      "Epoch 8885: train loss 0.9590461254119873 val loss 0.9625535607337952\n",
      "Epoch 8886: train loss 0.9625535607337952 val loss 0.947252631187439\n",
      "Epoch 8887: train loss 0.947252631187439 val loss 0.9467195868492126\n",
      "Epoch 8888: train loss 0.9467195868492126 val loss 0.9577041864395142\n",
      "Epoch 8889: train loss 0.9577041864395142 val loss 0.9536391496658325\n",
      "Epoch 8890: train loss 0.9536391496658325 val loss 0.9512985348701477\n",
      "Epoch 8891: train loss 0.9512985348701477 val loss 0.9625434875488281\n",
      "Epoch 8892: train loss 0.9625434875488281 val loss 0.9519810080528259\n",
      "Epoch 8893: train loss 0.9519810080528259 val loss 0.9509885311126709\n",
      "Epoch 8894: train loss 0.9509885311126709 val loss 0.9511018991470337\n",
      "Epoch 8895: train loss 0.9511018991470337 val loss 0.9431357979774475\n",
      "Epoch 8896: train loss 0.9431357979774475 val loss 0.9427124261856079\n",
      "Epoch 8897: train loss 0.9427124261856079 val loss 0.951606273651123\n",
      "Epoch 8898: train loss 0.951606273651123 val loss 0.9815270900726318\n",
      "Epoch 8899: train loss 0.9815270900726318 val loss 0.9440284967422485\n",
      "Epoch 8900: train loss 0.9440284967422485 val loss 0.9490686655044556\n",
      "Epoch 8901: train loss 0.9490686655044556 val loss 0.9863697290420532\n",
      "Epoch 8902: train loss 0.9863697290420532 val loss 0.9423860311508179\n",
      "Epoch 8903: train loss 0.9423860311508179 val loss 1.0174351930618286\n",
      "Epoch 8904: train loss 1.0174351930618286 val loss 0.9761142730712891\n",
      "Epoch 8905: train loss 0.9761142730712891 val loss 0.9502902030944824\n",
      "Epoch 8906: train loss 0.9502902030944824 val loss 0.9939181804656982\n",
      "Epoch 8907: train loss 0.9939181804656982 val loss 0.951977014541626\n",
      "Epoch 8908: train loss 0.951977014541626 val loss 0.9674890041351318\n",
      "Epoch 8909: train loss 0.9674890041351318 val loss 0.9793859720230103\n",
      "Epoch 8910: train loss 0.9793859720230103 val loss 0.941499650478363\n",
      "Epoch 8911: train loss 0.941499650478363 val loss 0.9551862478256226\n",
      "Epoch 8912: train loss 0.9551862478256226 val loss 0.9729526042938232\n",
      "Epoch 8913: train loss 0.9729526042938232 val loss 0.9497799873352051\n",
      "Epoch 8914: train loss 0.9497799873352051 val loss 0.9569485783576965\n",
      "Epoch 8915: train loss 0.9569485783576965 val loss 0.9876855611801147\n",
      "Epoch 8916: train loss 0.9876855611801147 val loss 0.9492588043212891\n",
      "Epoch 8917: train loss 0.9492588043212891 val loss 0.9736915826797485\n",
      "Epoch 8918: train loss 0.9736915826797485 val loss 1.0046924352645874\n",
      "Epoch 8919: train loss 1.0046924352645874 val loss 0.9408023357391357\n",
      "Epoch 8920: train loss 0.9408023357391357 val loss 0.9703583717346191\n",
      "Epoch 8921: train loss 0.9703583717346191 val loss 0.9795796871185303\n",
      "Epoch 8922: train loss 0.9795796871185303 val loss 0.9404261112213135\n",
      "Epoch 8923: train loss 0.9404261112213135 val loss 0.9465732574462891\n",
      "Epoch 8924: train loss 0.9465732574462891 val loss 0.9401746988296509\n",
      "Epoch 8925: train loss 0.9401746988296509 val loss 0.9408861994743347\n",
      "Epoch 8926: train loss 0.9408861994743347 val loss 0.9538464546203613\n",
      "Epoch 8927: train loss 0.9538464546203613 val loss 0.9610544443130493\n",
      "Epoch 8928: train loss 0.9610544443130493 val loss 0.9435145854949951\n",
      "Epoch 8929: train loss 0.9435145854949951 val loss 0.9481717348098755\n",
      "Epoch 8930: train loss 0.9481717348098755 val loss 0.9434523582458496\n",
      "Epoch 8931: train loss 0.9434523582458496 val loss 0.9398207664489746\n",
      "Epoch 8932: train loss 0.9398207664489746 val loss 0.939092755317688\n",
      "Epoch 8933: train loss 0.939092755317688 val loss 0.9426613450050354\n",
      "Epoch 8934: train loss 0.9426613450050354 val loss 0.9506978988647461\n",
      "Epoch 8935: train loss 0.9506978988647461 val loss 0.9405792951583862\n",
      "Epoch 8936: train loss 0.9405792951583862 val loss 0.9892655611038208\n",
      "Epoch 8937: train loss 0.9892655611038208 val loss 0.9523866176605225\n",
      "Epoch 8938: train loss 0.9523866176605225 val loss 0.9446191191673279\n",
      "Epoch 8939: train loss 0.9446191191673279 val loss 0.9706003665924072\n",
      "Epoch 8940: train loss 0.9706003665924072 val loss 0.949664831161499\n",
      "Epoch 8941: train loss 0.949664831161499 val loss 0.9526895880699158\n",
      "Epoch 8942: train loss 0.9526895880699158 val loss 0.9546514749526978\n",
      "Epoch 8943: train loss 0.9546514749526978 val loss 0.9495482444763184\n",
      "Epoch 8944: train loss 0.9495482444763184 val loss 0.9481766223907471\n",
      "Epoch 8945: train loss 0.9481766223907471 val loss 0.9553950428962708\n",
      "Epoch 8946: train loss 0.9553950428962708 val loss 0.953885555267334\n",
      "Epoch 8947: train loss 0.953885555267334 val loss 0.9424697160720825\n",
      "Epoch 8948: train loss 0.9424697160720825 val loss 0.9419960975646973\n",
      "Epoch 8949: train loss 0.9419960975646973 val loss 0.9460563659667969\n",
      "Epoch 8950: train loss 0.9460563659667969 val loss 0.9378712177276611\n",
      "Epoch 8951: train loss 0.9378712177276611 val loss 0.9433945417404175\n",
      "Epoch 8952: train loss 0.9433945417404175 val loss 0.9491442441940308\n",
      "Epoch 8953: train loss 0.9491442441940308 val loss 0.9443888664245605\n",
      "Epoch 8954: train loss 0.9443888664245605 val loss 0.9597004055976868\n",
      "Epoch 8955: train loss 0.9597004055976868 val loss 0.9521427750587463\n",
      "Epoch 8956: train loss 0.9521427750587463 val loss 0.9466078281402588\n",
      "Epoch 8957: train loss 0.9466078281402588 val loss 0.9647717475891113\n",
      "Epoch 8958: train loss 0.9647717475891113 val loss 0.9514967203140259\n",
      "Epoch 8959: train loss 0.9514967203140259 val loss 0.943112850189209\n",
      "Epoch 8960: train loss 0.943112850189209 val loss 0.9782555103302002\n",
      "Epoch 8961: train loss 0.9782555103302002 val loss 0.9539811611175537\n",
      "Epoch 8962: train loss 0.9539811611175537 val loss 0.9386348128318787\n",
      "Epoch 8963: train loss 0.9386348128318787 val loss 0.9574401378631592\n",
      "Epoch 8964: train loss 0.9574401378631592 val loss 0.9828188419342041\n",
      "Epoch 8965: train loss 0.9828188419342041 val loss 0.9394608736038208\n",
      "Epoch 8966: train loss 0.9394608736038208 val loss 0.9631154537200928\n",
      "Epoch 8967: train loss 0.9631154537200928 val loss 0.97023606300354\n",
      "Epoch 8968: train loss 0.97023606300354 val loss 0.9414113759994507\n",
      "Epoch 8969: train loss 0.9414113759994507 val loss 0.9407285451889038\n",
      "Epoch 8970: train loss 0.9407285451889038 val loss 0.9509338140487671\n",
      "Epoch 8971: train loss 0.9509338140487671 val loss 0.9403115510940552\n",
      "Epoch 8972: train loss 0.9403115510940552 val loss 0.9588733911514282\n",
      "Epoch 8973: train loss 0.9588733911514282 val loss 0.9869394898414612\n",
      "Epoch 8974: train loss 0.9869394898414612 val loss 0.9394494295120239\n",
      "Epoch 8975: train loss 0.9394494295120239 val loss 1.006232500076294\n",
      "Epoch 8976: train loss 1.006232500076294 val loss 0.9563707709312439\n",
      "Epoch 8977: train loss 0.9563707709312439 val loss 0.9485412836074829\n",
      "Epoch 8978: train loss 0.9485412836074829 val loss 0.9772233963012695\n",
      "Epoch 8979: train loss 0.9772233963012695 val loss 0.9510104656219482\n",
      "Epoch 8980: train loss 0.9510104656219482 val loss 0.9454056024551392\n",
      "Epoch 8981: train loss 0.9454056024551392 val loss 0.9678269624710083\n",
      "Epoch 8982: train loss 0.9678269624710083 val loss 0.9520587921142578\n",
      "Epoch 8983: train loss 0.9520587921142578 val loss 0.9485986828804016\n",
      "Epoch 8984: train loss 0.9485986828804016 val loss 0.9564574956893921\n",
      "Epoch 8985: train loss 0.9564574956893921 val loss 0.9479635953903198\n",
      "Epoch 8986: train loss 0.9479635953903198 val loss 0.9473826289176941\n",
      "Epoch 8987: train loss 0.9473826289176941 val loss 0.9514939785003662\n",
      "Epoch 8988: train loss 0.9514939785003662 val loss 0.9548594355583191\n",
      "Epoch 8989: train loss 0.9548594355583191 val loss 0.9434451460838318\n",
      "Epoch 8990: train loss 0.9434451460838318 val loss 0.9425132274627686\n",
      "Epoch 8991: train loss 0.9425132274627686 val loss 0.9458078145980835\n",
      "Epoch 8992: train loss 0.9458078145980835 val loss 0.9425657391548157\n",
      "Epoch 8993: train loss 0.9425657391548157 val loss 0.9492272138595581\n",
      "Epoch 8994: train loss 0.9492272138595581 val loss 0.9478045701980591\n",
      "Epoch 8995: train loss 0.9478045701980591 val loss 0.9410655498504639\n",
      "Epoch 8996: train loss 0.9410655498504639 val loss 0.9728164672851562\n",
      "Epoch 8997: train loss 0.9728164672851562 val loss 0.9501229524612427\n",
      "Epoch 8998: train loss 0.9501229524612427 val loss 0.9481521844863892\n",
      "Epoch 8999: train loss 0.9481521844863892 val loss 0.9615991115570068\n",
      "Epoch 9000: train loss 0.9615991115570068 val loss 0.9440019130706787\n",
      "Epoch 9001: train loss 0.9440019130706787 val loss 0.9478588104248047\n",
      "Epoch 9002: train loss 0.9478588104248047 val loss 0.9488437175750732\n",
      "Epoch 9003: train loss 0.9488437175750732 val loss 0.9564193487167358\n",
      "Epoch 9004: train loss 0.9564193487167358 val loss 0.9726377725601196\n",
      "Epoch 9005: train loss 0.9726377725601196 val loss 0.9409782886505127\n",
      "Epoch 9006: train loss 0.9409782886505127 val loss 0.9577882289886475\n",
      "Epoch 9007: train loss 0.9577882289886475 val loss 1.017275333404541\n",
      "Epoch 9008: train loss 1.017275333404541 val loss 0.9412364363670349\n",
      "Epoch 9009: train loss 0.9412364363670349 val loss 0.9816610813140869\n",
      "Epoch 9010: train loss 0.9816610813140869 val loss 0.9699345827102661\n",
      "Epoch 9011: train loss 0.9699345827102661 val loss 0.9387552738189697\n",
      "Epoch 9012: train loss 0.9387552738189697 val loss 0.9766898155212402\n",
      "Epoch 9013: train loss 0.9766898155212402 val loss 0.9631969928741455\n",
      "Epoch 9014: train loss 0.9631969928741455 val loss 0.9387260675430298\n",
      "Epoch 9015: train loss 0.9387260675430298 val loss 0.9456372261047363\n",
      "Epoch 9016: train loss 0.9456372261047363 val loss 0.9424653053283691\n",
      "Epoch 9017: train loss 0.9424653053283691 val loss 0.9392725825309753\n",
      "Epoch 9018: train loss 0.9392725825309753 val loss 0.9375675916671753\n",
      "Epoch 9019: train loss 0.9375675916671753 val loss 0.9469795823097229\n",
      "Epoch 9020: train loss 0.9469795823097229 val loss 0.957138180732727\n",
      "Epoch 9021: train loss 0.957138180732727 val loss 0.9421497583389282\n",
      "Epoch 9022: train loss 0.9421497583389282 val loss 0.9375932216644287\n",
      "Epoch 9023: train loss 0.9375932216644287 val loss 0.9616285562515259\n",
      "Epoch 9024: train loss 0.9616285562515259 val loss 0.9441429376602173\n",
      "Epoch 9025: train loss 0.9441429376602173 val loss 0.9433612823486328\n",
      "Epoch 9026: train loss 0.9433612823486328 val loss 0.9553130269050598\n",
      "Epoch 9027: train loss 0.9553130269050598 val loss 0.9448902606964111\n",
      "Epoch 9028: train loss 0.9448902606964111 val loss 0.9657331705093384\n",
      "Epoch 9029: train loss 0.9657331705093384 val loss 0.9777977466583252\n",
      "Epoch 9030: train loss 0.9777977466583252 val loss 0.9403972625732422\n",
      "Epoch 9031: train loss 0.9403972625732422 val loss 0.9986268281936646\n",
      "Epoch 9032: train loss 0.9986268281936646 val loss 0.9643509387969971\n",
      "Epoch 9033: train loss 0.9643509387969971 val loss 0.944941520690918\n",
      "Epoch 9034: train loss 0.944941520690918 val loss 0.9804311990737915\n",
      "Epoch 9035: train loss 0.9804311990737915 val loss 0.9469566345214844\n",
      "Epoch 9036: train loss 0.9469566345214844 val loss 0.9456008672714233\n",
      "Epoch 9037: train loss 0.9456008672714233 val loss 0.9457602500915527\n",
      "Epoch 9038: train loss 0.9457602500915527 val loss 0.9447462558746338\n",
      "Epoch 9039: train loss 0.9447462558746338 val loss 0.9397072792053223\n",
      "Epoch 9040: train loss 0.9397072792053223 val loss 0.9460511207580566\n",
      "Epoch 9041: train loss 0.9460511207580566 val loss 0.950452446937561\n",
      "Epoch 9042: train loss 0.950452446937561 val loss 0.9441515207290649\n",
      "Epoch 9043: train loss 0.9441515207290649 val loss 0.9388976097106934\n",
      "Epoch 9044: train loss 0.9388976097106934 val loss 0.9490169286727905\n",
      "Epoch 9045: train loss 0.9490169286727905 val loss 0.9380902051925659\n",
      "Epoch 9046: train loss 0.9380902051925659 val loss 0.9563000202178955\n",
      "Epoch 9047: train loss 0.9563000202178955 val loss 0.9734324216842651\n",
      "Epoch 9048: train loss 0.9734324216842651 val loss 0.9418727159500122\n",
      "Epoch 9049: train loss 0.9418727159500122 val loss 0.941820502281189\n",
      "Epoch 9050: train loss 0.941820502281189 val loss 0.9612723588943481\n",
      "Epoch 9051: train loss 0.9612723588943481 val loss 0.9736679792404175\n",
      "Epoch 9052: train loss 0.9736679792404175 val loss 0.9372166395187378\n",
      "Epoch 9053: train loss 0.9372166395187378 val loss 0.9464882612228394\n",
      "Epoch 9054: train loss 0.9464882612228394 val loss 0.940784215927124\n",
      "Epoch 9055: train loss 0.940784215927124 val loss 0.9397020936012268\n",
      "Epoch 9056: train loss 0.9397020936012268 val loss 0.9385992288589478\n",
      "Epoch 9057: train loss 0.9385992288589478 val loss 0.9479478597640991\n",
      "Epoch 9058: train loss 0.9479478597640991 val loss 0.9780409336090088\n",
      "Epoch 9059: train loss 0.9780409336090088 val loss 0.944656491279602\n",
      "Epoch 9060: train loss 0.944656491279602 val loss 0.9508039951324463\n",
      "Epoch 9061: train loss 0.9508039951324463 val loss 1.0085134506225586\n",
      "Epoch 9062: train loss 1.0085134506225586 val loss 0.9402350187301636\n",
      "Epoch 9063: train loss 0.9402350187301636 val loss 1.009781002998352\n",
      "Epoch 9064: train loss 1.009781002998352 val loss 0.9649142026901245\n",
      "Epoch 9065: train loss 0.9649142026901245 val loss 0.941477358341217\n",
      "Epoch 9066: train loss 0.941477358341217 val loss 0.9645838737487793\n",
      "Epoch 9067: train loss 0.9645838737487793 val loss 0.9770469665527344\n",
      "Epoch 9068: train loss 0.9770469665527344 val loss 0.9364879131317139\n",
      "Epoch 9069: train loss 0.9364879131317139 val loss 1.0094047784805298\n",
      "Epoch 9070: train loss 1.0094047784805298 val loss 0.9557933807373047\n",
      "Epoch 9071: train loss 0.9557933807373047 val loss 0.95718914270401\n",
      "Epoch 9072: train loss 0.95718914270401 val loss 0.9866865873336792\n",
      "Epoch 9073: train loss 0.9866865873336792 val loss 0.9384481310844421\n",
      "Epoch 9074: train loss 0.9384481310844421 val loss 0.947142481803894\n",
      "Epoch 9075: train loss 0.947142481803894 val loss 0.9394183158874512\n",
      "Epoch 9076: train loss 0.9394183158874512 val loss 0.9430185556411743\n",
      "Epoch 9077: train loss 0.9430185556411743 val loss 0.9728976488113403\n",
      "Epoch 9078: train loss 0.9728976488113403 val loss 0.9465559720993042\n",
      "Epoch 9079: train loss 0.9465559720993042 val loss 0.9391007423400879\n",
      "Epoch 9080: train loss 0.9391007423400879 val loss 0.9393190145492554\n",
      "Epoch 9081: train loss 0.9393190145492554 val loss 0.9468739032745361\n",
      "Epoch 9082: train loss 0.9468739032745361 val loss 0.9400551319122314\n",
      "Epoch 9083: train loss 0.9400551319122314 val loss 0.942925214767456\n",
      "Epoch 9084: train loss 0.942925214767456 val loss 0.9389433860778809\n",
      "Epoch 9085: train loss 0.9389433860778809 val loss 0.9382181167602539\n",
      "Epoch 9086: train loss 0.9382181167602539 val loss 0.9442442655563354\n",
      "Epoch 9087: train loss 0.9442442655563354 val loss 0.9377124905586243\n",
      "Epoch 9088: train loss 0.9377124905586243 val loss 0.9390584230422974\n",
      "Epoch 9089: train loss 0.9390584230422974 val loss 0.9418659210205078\n",
      "Epoch 9090: train loss 0.9418659210205078 val loss 0.940359354019165\n",
      "Epoch 9091: train loss 0.940359354019165 val loss 0.9376480579376221\n",
      "Epoch 9092: train loss 0.9376480579376221 val loss 0.9548738598823547\n",
      "Epoch 9093: train loss 0.9548738598823547 val loss 0.9727047085762024\n",
      "Epoch 9094: train loss 0.9727047085762024 val loss 0.9384351968765259\n",
      "Epoch 9095: train loss 0.9384351968765259 val loss 0.9507008194923401\n",
      "Epoch 9096: train loss 0.9507008194923401 val loss 0.9601467251777649\n",
      "Epoch 9097: train loss 0.9601467251777649 val loss 0.9393322467803955\n",
      "Epoch 9098: train loss 0.9393322467803955 val loss 0.9985964298248291\n",
      "Epoch 9099: train loss 0.9985964298248291 val loss 0.9702335596084595\n",
      "Epoch 9100: train loss 0.9702335596084595 val loss 0.939983606338501\n",
      "Epoch 9101: train loss 0.939983606338501 val loss 0.9714885950088501\n",
      "Epoch 9102: train loss 0.9714885950088501 val loss 0.9715651273727417\n",
      "Epoch 9103: train loss 0.9715651273727417 val loss 0.9362128973007202\n",
      "Epoch 9104: train loss 0.9362128973007202 val loss 1.014682412147522\n",
      "Epoch 9105: train loss 1.014682412147522 val loss 0.9775213003158569\n",
      "Epoch 9106: train loss 0.9775213003158569 val loss 0.9443421363830566\n",
      "Epoch 9107: train loss 0.9443421363830566 val loss 0.9916092157363892\n",
      "Epoch 9108: train loss 0.9916092157363892 val loss 0.9445708990097046\n",
      "Epoch 9109: train loss 0.9445708990097046 val loss 0.9484196901321411\n",
      "Epoch 9110: train loss 0.9484196901321411 val loss 0.9583507776260376\n",
      "Epoch 9111: train loss 0.9583507776260376 val loss 0.9536052942276001\n",
      "Epoch 9112: train loss 0.9536052942276001 val loss 0.9439993500709534\n",
      "Epoch 9113: train loss 0.9439993500709534 val loss 0.9591166973114014\n",
      "Epoch 9114: train loss 0.9591166973114014 val loss 0.9473922848701477\n",
      "Epoch 9115: train loss 0.9473922848701477 val loss 0.9543240070343018\n",
      "Epoch 9116: train loss 0.9543240070343018 val loss 0.994958758354187\n",
      "Epoch 9117: train loss 0.994958758354187 val loss 0.9445929527282715\n",
      "Epoch 9118: train loss 0.9445929527282715 val loss 0.9858530759811401\n",
      "Epoch 9119: train loss 0.9858530759811401 val loss 0.9886549711227417\n",
      "Epoch 9120: train loss 0.9886549711227417 val loss 0.9363603591918945\n",
      "Epoch 9121: train loss 0.9363603591918945 val loss 0.9532178640365601\n",
      "Epoch 9122: train loss 0.9532178640365601 val loss 0.9748470187187195\n",
      "Epoch 9123: train loss 0.9748470187187195 val loss 0.9446673393249512\n",
      "Epoch 9124: train loss 0.9446673393249512 val loss 0.9455521106719971\n",
      "Epoch 9125: train loss 0.9455521106719971 val loss 0.9652082920074463\n",
      "Epoch 9126: train loss 0.9652082920074463 val loss 0.9441503286361694\n",
      "Epoch 9127: train loss 0.9441503286361694 val loss 0.9424198269844055\n",
      "Epoch 9128: train loss 0.9424198269844055 val loss 0.9556608200073242\n",
      "Epoch 9129: train loss 0.9556608200073242 val loss 0.9506057500839233\n",
      "Epoch 9130: train loss 0.9506057500839233 val loss 0.9381853342056274\n",
      "Epoch 9131: train loss 0.9381853342056274 val loss 0.9417135715484619\n",
      "Epoch 9132: train loss 0.9417135715484619 val loss 0.9757281541824341\n",
      "Epoch 9133: train loss 0.9757281541824341 val loss 0.9513641595840454\n",
      "Epoch 9134: train loss 0.9513641595840454 val loss 0.9415696859359741\n",
      "Epoch 9135: train loss 0.9415696859359741 val loss 0.9420833587646484\n",
      "Epoch 9136: train loss 0.9420833587646484 val loss 0.9365959167480469\n",
      "Epoch 9137: train loss 0.9365959167480469 val loss 0.9365052580833435\n",
      "Epoch 9138: train loss 0.9365052580833435 val loss 0.9551345109939575\n",
      "Epoch 9139: train loss 0.9551345109939575 val loss 0.9526472091674805\n",
      "Epoch 9140: train loss 0.9526472091674805 val loss 0.9439371824264526\n",
      "Epoch 9141: train loss 0.9439371824264526 val loss 0.9399217963218689\n",
      "Epoch 9142: train loss 0.9399217963218689 val loss 0.9705692529678345\n",
      "Epoch 9143: train loss 0.9705692529678345 val loss 0.9681534171104431\n",
      "Epoch 9144: train loss 0.9681534171104431 val loss 0.9365668296813965\n",
      "Epoch 9145: train loss 0.9365668296813965 val loss 0.952986478805542\n",
      "Epoch 9146: train loss 0.952986478805542 val loss 0.9911412000656128\n",
      "Epoch 9147: train loss 0.9911412000656128 val loss 0.9407261610031128\n",
      "Epoch 9148: train loss 0.9407261610031128 val loss 1.047271490097046\n",
      "Epoch 9149: train loss 1.047271490097046 val loss 0.9523379802703857\n",
      "Epoch 9150: train loss 0.9523379802703857 val loss 0.9875713586807251\n",
      "Epoch 9151: train loss 0.9875713586807251 val loss 1.0054130554199219\n",
      "Epoch 9152: train loss 1.0054130554199219 val loss 0.9369738101959229\n",
      "Epoch 9153: train loss 0.9369738101959229 val loss 0.971636176109314\n",
      "Epoch 9154: train loss 0.971636176109314 val loss 0.9665627479553223\n",
      "Epoch 9155: train loss 0.9665627479553223 val loss 0.937369704246521\n",
      "Epoch 9156: train loss 0.937369704246521 val loss 0.9367464780807495\n",
      "Epoch 9157: train loss 0.9367464780807495 val loss 0.9694614410400391\n",
      "Epoch 9158: train loss 0.9694614410400391 val loss 0.9596935510635376\n",
      "Epoch 9159: train loss 0.9596935510635376 val loss 0.9350623488426208\n",
      "Epoch 9160: train loss 0.9350623488426208 val loss 0.9893721342086792\n",
      "Epoch 9161: train loss 0.9893721342086792 val loss 0.9761807918548584\n",
      "Epoch 9162: train loss 0.9761807918548584 val loss 0.939329981803894\n",
      "Epoch 9163: train loss 0.939329981803894 val loss 0.9984813928604126\n",
      "Epoch 9164: train loss 0.9984813928604126 val loss 0.9517420530319214\n",
      "Epoch 9165: train loss 0.9517420530319214 val loss 0.9424498677253723\n",
      "Epoch 9166: train loss 0.9424498677253723 val loss 0.9622064828872681\n",
      "Epoch 9167: train loss 0.9622064828872681 val loss 0.9479331970214844\n",
      "Epoch 9168: train loss 0.9479331970214844 val loss 0.9397381544113159\n",
      "Epoch 9169: train loss 0.9397381544113159 val loss 0.9400905966758728\n",
      "Epoch 9170: train loss 0.9400905966758728 val loss 0.9388948082923889\n",
      "Epoch 9171: train loss 0.9388948082923889 val loss 0.9353819489479065\n",
      "Epoch 9172: train loss 0.9353819489479065 val loss 0.9502019882202148\n",
      "Epoch 9173: train loss 0.9502019882202148 val loss 0.9830222725868225\n",
      "Epoch 9174: train loss 0.9830222725868225 val loss 0.9415578842163086\n",
      "Epoch 9175: train loss 0.9415578842163086 val loss 0.9520158171653748\n",
      "Epoch 9176: train loss 0.9520158171653748 val loss 0.9970399141311646\n",
      "Epoch 9177: train loss 0.9970399141311646 val loss 0.9361712336540222\n",
      "Epoch 9178: train loss 0.9361712336540222 val loss 1.0136494636535645\n",
      "Epoch 9179: train loss 1.0136494636535645 val loss 0.9684760570526123\n",
      "Epoch 9180: train loss 0.9684760570526123 val loss 0.94407057762146\n",
      "Epoch 9181: train loss 0.94407057762146 val loss 1.012271523475647\n",
      "Epoch 9182: train loss 1.012271523475647 val loss 0.9488885402679443\n",
      "Epoch 9183: train loss 0.9488885402679443 val loss 0.9606956243515015\n",
      "Epoch 9184: train loss 0.9606956243515015 val loss 0.9926629066467285\n",
      "Epoch 9185: train loss 0.9926629066467285 val loss 0.9385563135147095\n",
      "Epoch 9186: train loss 0.9385563135147095 val loss 0.9513297080993652\n",
      "Epoch 9187: train loss 0.9513297080993652 val loss 0.9795960187911987\n",
      "Epoch 9188: train loss 0.9795960187911987 val loss 0.9540579319000244\n",
      "Epoch 9189: train loss 0.9540579319000244 val loss 0.9380447864532471\n",
      "Epoch 9190: train loss 0.9380447864532471 val loss 0.9441853761672974\n",
      "Epoch 9191: train loss 0.9441853761672974 val loss 0.9438859224319458\n",
      "Epoch 9192: train loss 0.9438859224319458 val loss 0.9353175163269043\n",
      "Epoch 9193: train loss 0.9353175163269043 val loss 0.962496280670166\n",
      "Epoch 9194: train loss 0.962496280670166 val loss 0.9547373056411743\n",
      "Epoch 9195: train loss 0.9547373056411743 val loss 0.9345436096191406\n",
      "Epoch 9196: train loss 0.9345436096191406 val loss 0.9405763149261475\n",
      "Epoch 9197: train loss 0.9405763149261475 val loss 0.9417651891708374\n",
      "Epoch 9198: train loss 0.9417651891708374 val loss 0.939223051071167\n",
      "Epoch 9199: train loss 0.939223051071167 val loss 0.944979190826416\n",
      "Epoch 9200: train loss 0.944979190826416 val loss 0.941582441329956\n",
      "Epoch 9201: train loss 0.941582441329956 val loss 0.9380484223365784\n",
      "Epoch 9202: train loss 0.9380484223365784 val loss 0.9363285303115845\n",
      "Epoch 9203: train loss 0.9363285303115845 val loss 0.9336719512939453\n",
      "Epoch 9204: train loss 0.9336719512939453 val loss 0.9349137544631958\n",
      "Epoch 9205: train loss 0.9349137544631958 val loss 0.944152295589447\n",
      "Epoch 9206: train loss 0.944152295589447 val loss 0.9520283937454224\n",
      "Epoch 9207: train loss 0.9520283937454224 val loss 0.97513747215271\n",
      "Epoch 9208: train loss 0.97513747215271 val loss 0.9336420297622681\n",
      "Epoch 9209: train loss 0.9336420297622681 val loss 0.9392445087432861\n",
      "Epoch 9210: train loss 0.9392445087432861 val loss 0.937926173210144\n",
      "Epoch 9211: train loss 0.937926173210144 val loss 0.9364238381385803\n",
      "Epoch 9212: train loss 0.9364238381385803 val loss 0.9352219104766846\n",
      "Epoch 9213: train loss 0.9352219104766846 val loss 0.9411097764968872\n",
      "Epoch 9214: train loss 0.9411097764968872 val loss 0.9587229490280151\n",
      "Epoch 9215: train loss 0.9587229490280151 val loss 0.9690927267074585\n",
      "Epoch 9216: train loss 0.9690927267074585 val loss 0.9346554279327393\n",
      "Epoch 9217: train loss 0.9346554279327393 val loss 0.9439992308616638\n",
      "Epoch 9218: train loss 0.9439992308616638 val loss 0.9544563889503479\n",
      "Epoch 9219: train loss 0.9544563889503479 val loss 0.9415273666381836\n",
      "Epoch 9220: train loss 0.9415273666381836 val loss 0.9409501552581787\n",
      "Epoch 9221: train loss 0.9409501552581787 val loss 0.9494134187698364\n",
      "Epoch 9222: train loss 0.9494134187698364 val loss 0.9518073797225952\n",
      "Epoch 9223: train loss 0.9518073797225952 val loss 0.960145890712738\n",
      "Epoch 9224: train loss 0.960145890712738 val loss 0.9383955597877502\n",
      "Epoch 9225: train loss 0.9383955597877502 val loss 0.9499984979629517\n",
      "Epoch 9226: train loss 0.9499984979629517 val loss 0.9745990037918091\n",
      "Epoch 9227: train loss 0.9745990037918091 val loss 0.9409545660018921\n",
      "Epoch 9228: train loss 0.9409545660018921 val loss 0.9468591213226318\n",
      "Epoch 9229: train loss 0.9468591213226318 val loss 0.9633220434188843\n",
      "Epoch 9230: train loss 0.9633220434188843 val loss 0.9357283115386963\n",
      "Epoch 9231: train loss 0.9357283115386963 val loss 0.9497380256652832\n",
      "Epoch 9232: train loss 0.9497380256652832 val loss 0.9843913316726685\n",
      "Epoch 9233: train loss 0.9843913316726685 val loss 0.9487614631652832\n",
      "Epoch 9234: train loss 0.9487614631652832 val loss 0.9386736154556274\n",
      "Epoch 9235: train loss 0.9386736154556274 val loss 0.9652860164642334\n",
      "Epoch 9236: train loss 0.9652860164642334 val loss 0.953822135925293\n",
      "Epoch 9237: train loss 0.953822135925293 val loss 0.9356440305709839\n",
      "Epoch 9238: train loss 0.9356440305709839 val loss 0.9803943634033203\n",
      "Epoch 9239: train loss 0.9803943634033203 val loss 0.9945340156555176\n",
      "Epoch 9240: train loss 0.9945340156555176 val loss 0.9402810335159302\n",
      "Epoch 9241: train loss 0.9402810335159302 val loss 0.9781302213668823\n",
      "Epoch 9242: train loss 0.9781302213668823 val loss 0.9742059707641602\n",
      "Epoch 9243: train loss 0.9742059707641602 val loss 0.9349470138549805\n",
      "Epoch 9244: train loss 0.9349470138549805 val loss 0.97586989402771\n",
      "Epoch 9245: train loss 0.97586989402771 val loss 0.9645425081253052\n",
      "Epoch 9246: train loss 0.9645425081253052 val loss 0.9367155432701111\n",
      "Epoch 9247: train loss 0.9367155432701111 val loss 0.941606879234314\n",
      "Epoch 9248: train loss 0.941606879234314 val loss 0.9709098935127258\n",
      "Epoch 9249: train loss 0.9709098935127258 val loss 0.9557703733444214\n",
      "Epoch 9250: train loss 0.9557703733444214 val loss 0.9342743158340454\n",
      "Epoch 9251: train loss 0.9342743158340454 val loss 0.9390084147453308\n",
      "Epoch 9252: train loss 0.9390084147453308 val loss 0.9352400302886963\n",
      "Epoch 9253: train loss 0.9352400302886963 val loss 0.9359961748123169\n",
      "Epoch 9254: train loss 0.9359961748123169 val loss 0.9346256256103516\n",
      "Epoch 9255: train loss 0.9346256256103516 val loss 0.9430947303771973\n",
      "Epoch 9256: train loss 0.9430947303771973 val loss 0.9589956402778625\n",
      "Epoch 9257: train loss 0.9589956402778625 val loss 0.9339965581893921\n",
      "Epoch 9258: train loss 0.9339965581893921 val loss 0.9334704279899597\n",
      "Epoch 9259: train loss 0.9334704279899597 val loss 0.942397952079773\n",
      "Epoch 9260: train loss 0.942397952079773 val loss 0.939145565032959\n",
      "Epoch 9261: train loss 0.939145565032959 val loss 0.9330205917358398\n",
      "Epoch 9262: train loss 0.9330205917358398 val loss 0.9583711624145508\n",
      "Epoch 9263: train loss 0.9583711624145508 val loss 0.9621530771255493\n",
      "Epoch 9264: train loss 0.9621530771255493 val loss 0.9371166229248047\n",
      "Epoch 9265: train loss 0.9371166229248047 val loss 0.9349352121353149\n",
      "Epoch 9266: train loss 0.9349352121353149 val loss 0.9460870027542114\n",
      "Epoch 9267: train loss 0.9460870027542114 val loss 0.9372680187225342\n",
      "Epoch 9268: train loss 0.9372680187225342 val loss 0.9474442601203918\n",
      "Epoch 9269: train loss 0.9474442601203918 val loss 0.9759095907211304\n",
      "Epoch 9270: train loss 0.9759095907211304 val loss 0.9358978271484375\n",
      "Epoch 9271: train loss 0.9358978271484375 val loss 0.9373247623443604\n",
      "Epoch 9272: train loss 0.9373247623443604 val loss 0.9494428634643555\n",
      "Epoch 9273: train loss 0.9494428634643555 val loss 0.9347980618476868\n",
      "Epoch 9274: train loss 0.9347980618476868 val loss 0.9391689300537109\n",
      "Epoch 9275: train loss 0.9391689300537109 val loss 0.9731659889221191\n",
      "Epoch 9276: train loss 0.9731659889221191 val loss 0.9438092708587646\n",
      "Epoch 9277: train loss 0.9438092708587646 val loss 0.9358450174331665\n",
      "Epoch 9278: train loss 0.9358450174331665 val loss 0.9353063106536865\n",
      "Epoch 9279: train loss 0.9353063106536865 val loss 0.9385631084442139\n",
      "Epoch 9280: train loss 0.9385631084442139 val loss 0.941927433013916\n",
      "Epoch 9281: train loss 0.941927433013916 val loss 0.9337202310562134\n",
      "Epoch 9282: train loss 0.9337202310562134 val loss 0.9639315009117126\n",
      "Epoch 9283: train loss 0.9639315009117126 val loss 0.977852463722229\n",
      "Epoch 9284: train loss 0.977852463722229 val loss 0.9383320808410645\n",
      "Epoch 9285: train loss 0.9383320808410645 val loss 0.9433928728103638\n",
      "Epoch 9286: train loss 0.9433928728103638 val loss 0.9672456979751587\n",
      "Epoch 9287: train loss 0.9672456979751587 val loss 0.9486733675003052\n",
      "Epoch 9288: train loss 0.9486733675003052 val loss 0.942835807800293\n",
      "Epoch 9289: train loss 0.942835807800293 val loss 0.9465352296829224\n",
      "Epoch 9290: train loss 0.9465352296829224 val loss 0.9377871751785278\n",
      "Epoch 9291: train loss 0.9377871751785278 val loss 0.9668383598327637\n",
      "Epoch 9292: train loss 0.9668383598327637 val loss 0.9752075672149658\n",
      "Epoch 9293: train loss 0.9752075672149658 val loss 0.9356933832168579\n",
      "Epoch 9294: train loss 0.9356933832168579 val loss 1.0029374361038208\n",
      "Epoch 9295: train loss 1.0029374361038208 val loss 0.9578743577003479\n",
      "Epoch 9296: train loss 0.9578743577003479 val loss 0.9434723854064941\n",
      "Epoch 9297: train loss 0.9434723854064941 val loss 0.9641677141189575\n",
      "Epoch 9298: train loss 0.9641677141189575 val loss 0.957555890083313\n",
      "Epoch 9299: train loss 0.957555890083313 val loss 0.9321010112762451\n",
      "Epoch 9300: train loss 0.9321010112762451 val loss 0.9930223226547241\n",
      "Epoch 9301: train loss 0.9930223226547241 val loss 0.9726224541664124\n",
      "Epoch 9302: train loss 0.9726224541664124 val loss 0.936964750289917\n",
      "Epoch 9303: train loss 0.936964750289917 val loss 0.9952213764190674\n",
      "Epoch 9304: train loss 0.9952213764190674 val loss 0.9552204608917236\n",
      "Epoch 9305: train loss 0.9552204608917236 val loss 0.9328403472900391\n",
      "Epoch 9306: train loss 0.9328403472900391 val loss 0.9582428932189941\n",
      "Epoch 9307: train loss 0.9582428932189941 val loss 0.966012716293335\n",
      "Epoch 9308: train loss 0.966012716293335 val loss 0.9387553930282593\n",
      "Epoch 9309: train loss 0.9387553930282593 val loss 0.9431254863739014\n",
      "Epoch 9310: train loss 0.9431254863739014 val loss 0.9844597578048706\n",
      "Epoch 9311: train loss 0.9844597578048706 val loss 0.9343537092208862\n",
      "Epoch 9312: train loss 0.9343537092208862 val loss 1.006729245185852\n",
      "Epoch 9313: train loss 1.006729245185852 val loss 1.003751277923584\n",
      "Epoch 9314: train loss 1.003751277923584 val loss 0.9621204137802124\n",
      "Epoch 9315: train loss 0.9621204137802124 val loss 1.0498141050338745\n",
      "Epoch 9316: train loss 1.0498141050338745 val loss 0.9352816343307495\n",
      "Epoch 9317: train loss 0.9352816343307495 val loss 1.052037239074707\n",
      "Epoch 9318: train loss 1.052037239074707 val loss 0.9382359981536865\n",
      "Epoch 9319: train loss 0.9382359981536865 val loss 0.9644070863723755\n",
      "Epoch 9320: train loss 0.9644070863723755 val loss 0.9929443597793579\n",
      "Epoch 9321: train loss 0.9929443597793579 val loss 0.9424368739128113\n",
      "Epoch 9322: train loss 0.9424368739128113 val loss 0.9953502416610718\n",
      "Epoch 9323: train loss 0.9953502416610718 val loss 0.9466798305511475\n",
      "Epoch 9324: train loss 0.9466798305511475 val loss 0.9582178592681885\n",
      "Epoch 9325: train loss 0.9582178592681885 val loss 0.9773620367050171\n",
      "Epoch 9326: train loss 0.9773620367050171 val loss 0.9461784362792969\n",
      "Epoch 9327: train loss 0.9461784362792969 val loss 0.9394782781600952\n",
      "Epoch 9328: train loss 0.9394782781600952 val loss 0.9605835676193237\n",
      "Epoch 9329: train loss 0.9605835676193237 val loss 0.953899621963501\n",
      "Epoch 9330: train loss 0.953899621963501 val loss 0.9354254007339478\n",
      "Epoch 9331: train loss 0.9354254007339478 val loss 0.959243893623352\n",
      "Epoch 9332: train loss 0.959243893623352 val loss 0.9651786684989929\n",
      "Epoch 9333: train loss 0.9651786684989929 val loss 0.9370625019073486\n",
      "Epoch 9334: train loss 0.9370625019073486 val loss 0.9406978487968445\n",
      "Epoch 9335: train loss 0.9406978487968445 val loss 0.9745659828186035\n",
      "Epoch 9336: train loss 0.9745659828186035 val loss 0.9442020058631897\n",
      "Epoch 9337: train loss 0.9442020058631897 val loss 0.9356367588043213\n",
      "Epoch 9338: train loss 0.9356367588043213 val loss 0.9390175342559814\n",
      "Epoch 9339: train loss 0.9390175342559814 val loss 0.9359233379364014\n",
      "Epoch 9340: train loss 0.9359233379364014 val loss 0.94051194190979\n",
      "Epoch 9341: train loss 0.94051194190979 val loss 0.9584701061248779\n",
      "Epoch 9342: train loss 0.9584701061248779 val loss 0.9351018667221069\n",
      "Epoch 9343: train loss 0.9351018667221069 val loss 0.933081865310669\n",
      "Epoch 9344: train loss 0.933081865310669 val loss 0.9606732130050659\n",
      "Epoch 9345: train loss 0.9606732130050659 val loss 0.9456908702850342\n",
      "Epoch 9346: train loss 0.9456908702850342 val loss 0.939351499080658\n",
      "Epoch 9347: train loss 0.939351499080658 val loss 0.9343982338905334\n",
      "Epoch 9348: train loss 0.9343982338905334 val loss 0.946319580078125\n",
      "Epoch 9349: train loss 0.946319580078125 val loss 0.9752416610717773\n",
      "Epoch 9350: train loss 0.9752416610717773 val loss 0.9348865747451782\n",
      "Epoch 9351: train loss 0.9348865747451782 val loss 0.9474427700042725\n",
      "Epoch 9352: train loss 0.9474427700042725 val loss 0.9596964120864868\n",
      "Epoch 9353: train loss 0.9596964120864868 val loss 0.9454977512359619\n",
      "Epoch 9354: train loss 0.9454977512359619 val loss 0.9360841512680054\n",
      "Epoch 9355: train loss 0.9360841512680054 val loss 0.9343229532241821\n",
      "Epoch 9356: train loss 0.9343229532241821 val loss 0.9342367649078369\n",
      "Epoch 9357: train loss 0.9342367649078369 val loss 0.9527366161346436\n",
      "Epoch 9358: train loss 0.9527366161346436 val loss 0.9412646293640137\n",
      "Epoch 9359: train loss 0.9412646293640137 val loss 0.9374278783798218\n",
      "Epoch 9360: train loss 0.9374278783798218 val loss 0.9630047082901001\n",
      "Epoch 9361: train loss 0.9630047082901001 val loss 0.9663680791854858\n",
      "Epoch 9362: train loss 0.9663680791854858 val loss 0.9389076232910156\n",
      "Epoch 9363: train loss 0.9389076232910156 val loss 0.9468734264373779\n",
      "Epoch 9364: train loss 0.9468734264373779 val loss 0.9472414255142212\n",
      "Epoch 9365: train loss 0.9472414255142212 val loss 0.9427281618118286\n",
      "Epoch 9366: train loss 0.9427281618118286 val loss 0.9431804418563843\n",
      "Epoch 9367: train loss 0.9431804418563843 val loss 0.9347455501556396\n",
      "Epoch 9368: train loss 0.9347455501556396 val loss 0.9338045120239258\n",
      "Epoch 9369: train loss 0.9338045120239258 val loss 0.9337952136993408\n",
      "Epoch 9370: train loss 0.9337952136993408 val loss 0.9343095421791077\n",
      "Epoch 9371: train loss 0.9343095421791077 val loss 0.9406847953796387\n",
      "Epoch 9372: train loss 0.9406847953796387 val loss 0.9312701225280762\n",
      "Epoch 9373: train loss 0.9312701225280762 val loss 0.9335188269615173\n",
      "Epoch 9374: train loss 0.9335188269615173 val loss 0.9364582300186157\n",
      "Epoch 9375: train loss 0.9364582300186157 val loss 0.9371827840805054\n",
      "Epoch 9376: train loss 0.9371827840805054 val loss 0.9331457614898682\n",
      "Epoch 9377: train loss 0.9331457614898682 val loss 0.9425020217895508\n",
      "Epoch 9378: train loss 0.9425020217895508 val loss 0.9541147947311401\n",
      "Epoch 9379: train loss 0.9541147947311401 val loss 0.957472562789917\n",
      "Epoch 9380: train loss 0.957472562789917 val loss 0.9333323240280151\n",
      "Epoch 9381: train loss 0.9333323240280151 val loss 0.9454907774925232\n",
      "Epoch 9382: train loss 0.9454907774925232 val loss 0.9557884931564331\n",
      "Epoch 9383: train loss 0.9557884931564331 val loss 0.9357740879058838\n",
      "Epoch 9384: train loss 0.9357740879058838 val loss 0.9367001056671143\n",
      "Epoch 9385: train loss 0.9367001056671143 val loss 0.9424641132354736\n",
      "Epoch 9386: train loss 0.9424641132354736 val loss 0.9650783538818359\n",
      "Epoch 9387: train loss 0.9650783538818359 val loss 0.9456706047058105\n",
      "Epoch 9388: train loss 0.9456706047058105 val loss 0.9359121918678284\n",
      "Epoch 9389: train loss 0.9359121918678284 val loss 0.9414623975753784\n",
      "Epoch 9390: train loss 0.9414623975753784 val loss 0.9347879886627197\n",
      "Epoch 9391: train loss 0.9347879886627197 val loss 0.9337860941886902\n",
      "Epoch 9392: train loss 0.9337860941886902 val loss 0.9331759214401245\n",
      "Epoch 9393: train loss 0.9331759214401245 val loss 0.9332535862922668\n",
      "Epoch 9394: train loss 0.9332535862922668 val loss 0.9444528818130493\n",
      "Epoch 9395: train loss 0.9444528818130493 val loss 0.9487153887748718\n",
      "Epoch 9396: train loss 0.9487153887748718 val loss 0.9397597312927246\n",
      "Epoch 9397: train loss 0.9397597312927246 val loss 0.9380112886428833\n",
      "Epoch 9398: train loss 0.9380112886428833 val loss 0.9427518844604492\n",
      "Epoch 9399: train loss 0.9427518844604492 val loss 0.936126708984375\n",
      "Epoch 9400: train loss 0.936126708984375 val loss 0.9510897994041443\n",
      "Epoch 9401: train loss 0.9510897994041443 val loss 1.0288374423980713\n",
      "Epoch 9402: train loss 1.0288374423980713 val loss 0.9347275495529175\n",
      "Epoch 9403: train loss 0.9347275495529175 val loss 1.0101494789123535\n",
      "Epoch 9404: train loss 1.0101494789123535 val loss 0.9382574558258057\n",
      "Epoch 9405: train loss 0.9382574558258057 val loss 0.9704403877258301\n",
      "Epoch 9406: train loss 0.9704403877258301 val loss 1.0464861392974854\n",
      "Epoch 9407: train loss 1.0464861392974854 val loss 0.9378917217254639\n",
      "Epoch 9408: train loss 0.9378917217254639 val loss 1.0431151390075684\n",
      "Epoch 9409: train loss 1.0431151390075684 val loss 0.9342391490936279\n",
      "Epoch 9410: train loss 0.9342391490936279 val loss 1.0142834186553955\n",
      "Epoch 9411: train loss 1.0142834186553955 val loss 0.9340905547142029\n",
      "Epoch 9412: train loss 0.9340905547142029 val loss 0.9620600938796997\n",
      "Epoch 9413: train loss 0.9620600938796997 val loss 0.9658221006393433\n",
      "Epoch 9414: train loss 0.9658221006393433 val loss 0.9325286746025085\n",
      "Epoch 9415: train loss 0.9325286746025085 val loss 0.9315251111984253\n",
      "Epoch 9416: train loss 0.9315251111984253 val loss 0.9337472915649414\n",
      "Epoch 9417: train loss 0.9337472915649414 val loss 0.931552529335022\n",
      "Epoch 9418: train loss 0.931552529335022 val loss 0.9429195523262024\n",
      "Epoch 9419: train loss 0.9429195523262024 val loss 0.9804121255874634\n",
      "Epoch 9420: train loss 0.9804121255874634 val loss 0.9374691247940063\n",
      "Epoch 9421: train loss 0.9374691247940063 val loss 0.9430124759674072\n",
      "Epoch 9422: train loss 0.9430124759674072 val loss 1.006590485572815\n",
      "Epoch 9423: train loss 1.006590485572815 val loss 0.9339842200279236\n",
      "Epoch 9424: train loss 0.9339842200279236 val loss 1.0110644102096558\n",
      "Epoch 9425: train loss 1.0110644102096558 val loss 0.9613081812858582\n",
      "Epoch 9426: train loss 0.9613081812858582 val loss 0.9613245725631714\n",
      "Epoch 9427: train loss 0.9613245725631714 val loss 0.988936185836792\n",
      "Epoch 9428: train loss 0.988936185836792 val loss 0.9357849359512329\n",
      "Epoch 9429: train loss 0.9357849359512329 val loss 0.9789323806762695\n",
      "Epoch 9430: train loss 0.9789323806762695 val loss 0.9756855964660645\n",
      "Epoch 9431: train loss 0.9756855964660645 val loss 0.9357663989067078\n",
      "Epoch 9432: train loss 0.9357663989067078 val loss 0.9522374868392944\n",
      "Epoch 9433: train loss 0.9522374868392944 val loss 0.9677771329879761\n",
      "Epoch 9434: train loss 0.9677771329879761 val loss 0.9434280395507812\n",
      "Epoch 9435: train loss 0.9434280395507812 val loss 0.940104603767395\n",
      "Epoch 9436: train loss 0.940104603767395 val loss 0.9762992858886719\n",
      "Epoch 9437: train loss 0.9762992858886719 val loss 0.9598053693771362\n",
      "Epoch 9438: train loss 0.9598053693771362 val loss 0.9330775737762451\n",
      "Epoch 9439: train loss 0.9330775737762451 val loss 0.9358280301094055\n",
      "Epoch 9440: train loss 0.9358280301094055 val loss 0.9498896598815918\n",
      "Epoch 9441: train loss 0.9498896598815918 val loss 0.9353364706039429\n",
      "Epoch 9442: train loss 0.9353364706039429 val loss 0.9352467060089111\n",
      "Epoch 9443: train loss 0.9352467060089111 val loss 0.9331523180007935\n",
      "Epoch 9444: train loss 0.9331523180007935 val loss 0.9443918466567993\n",
      "Epoch 9445: train loss 0.9443918466567993 val loss 0.9927947521209717\n",
      "Epoch 9446: train loss 0.9927947521209717 val loss 0.930349588394165\n",
      "Epoch 9447: train loss 0.930349588394165 val loss 1.0324134826660156\n",
      "Epoch 9448: train loss 1.0324134826660156 val loss 0.95417320728302\n",
      "Epoch 9449: train loss 0.95417320728302 val loss 0.9747041463851929\n",
      "Epoch 9450: train loss 0.9747041463851929 val loss 1.0319170951843262\n",
      "Epoch 9451: train loss 1.0319170951843262 val loss 0.9399375915527344\n",
      "Epoch 9452: train loss 0.9399375915527344 val loss 1.104420781135559\n",
      "Epoch 9453: train loss 1.104420781135559 val loss 0.9384236335754395\n",
      "Epoch 9454: train loss 0.9384236335754395 val loss 1.1298415660858154\n",
      "Epoch 9455: train loss 1.1298415660858154 val loss 0.9858951568603516\n",
      "Epoch 9456: train loss 0.9858951568603516 val loss 1.2167861461639404\n",
      "Epoch 9457: train loss 1.2167861461639404 val loss 1.189670205116272\n",
      "Epoch 9458: train loss 1.189670205116272 val loss 1.003159523010254\n",
      "Epoch 9459: train loss 1.003159523010254 val loss 1.0319514274597168\n",
      "Epoch 9460: train loss 1.0319514274597168 val loss 1.0598390102386475\n",
      "Epoch 9461: train loss 1.0598390102386475 val loss 0.97881019115448\n",
      "Epoch 9462: train loss 0.97881019115448 val loss 1.115594744682312\n",
      "Epoch 9463: train loss 1.115594744682312 val loss 1.0206526517868042\n",
      "Epoch 9464: train loss 1.0206526517868042 val loss 1.1823937892913818\n",
      "Epoch 9465: train loss 1.1823937892913818 val loss 1.1681619882583618\n",
      "Epoch 9466: train loss 1.1681619882583618 val loss 1.009203553199768\n",
      "Epoch 9467: train loss 1.009203553199768 val loss 1.0360639095306396\n",
      "Epoch 9468: train loss 1.0360639095306396 val loss 1.0559062957763672\n",
      "Epoch 9469: train loss 1.0559062957763672 val loss 0.9805307984352112\n",
      "Epoch 9470: train loss 0.9805307984352112 val loss 1.0955559015274048\n",
      "Epoch 9471: train loss 1.0955559015274048 val loss 1.0107985734939575\n",
      "Epoch 9472: train loss 1.0107985734939575 val loss 1.166420817375183\n",
      "Epoch 9473: train loss 1.166420817375183 val loss 1.1230827569961548\n",
      "Epoch 9474: train loss 1.1230827569961548 val loss 1.059836745262146\n",
      "Epoch 9475: train loss 1.059836745262146 val loss 1.080832839012146\n",
      "Epoch 9476: train loss 1.080832839012146 val loss 1.0135526657104492\n",
      "Epoch 9477: train loss 1.0135526657104492 val loss 0.9788874387741089\n",
      "Epoch 9478: train loss 0.9788874387741089 val loss 1.011433720588684\n",
      "Epoch 9479: train loss 1.011433720588684 val loss 0.9379037618637085\n",
      "Epoch 9480: train loss 0.9379037618637085 val loss 0.9575917720794678\n",
      "Epoch 9481: train loss 0.9575917720794678 val loss 0.9358161687850952\n",
      "Epoch 9482: train loss 0.9358161687850952 val loss 1.0138940811157227\n",
      "Epoch 9483: train loss 1.0138940811157227 val loss 0.9813976883888245\n",
      "Epoch 9484: train loss 0.9813976883888245 val loss 0.966526985168457\n",
      "Epoch 9485: train loss 0.966526985168457 val loss 0.9975544214248657\n",
      "Epoch 9486: train loss 0.9975544214248657 val loss 0.9312951564788818\n",
      "Epoch 9487: train loss 0.9312951564788818 val loss 0.9526139497756958\n",
      "Epoch 9488: train loss 0.9526139497756958 val loss 0.9429272413253784\n",
      "Epoch 9489: train loss 0.9429272413253784 val loss 0.9373559355735779\n",
      "Epoch 9490: train loss 0.9373559355735779 val loss 0.9325251579284668\n",
      "Epoch 9491: train loss 0.9325251579284668 val loss 0.9482642412185669\n",
      "Epoch 9492: train loss 0.9482642412185669 val loss 0.9578018188476562\n",
      "Epoch 9493: train loss 0.9578018188476562 val loss 0.9316748380661011\n",
      "Epoch 9494: train loss 0.9316748380661011 val loss 0.9741476774215698\n",
      "Epoch 9495: train loss 0.9741476774215698 val loss 0.9825619459152222\n",
      "Epoch 9496: train loss 0.9825619459152222 val loss 0.9366534948348999\n",
      "Epoch 9497: train loss 0.9366534948348999 val loss 1.016219139099121\n",
      "Epoch 9498: train loss 1.016219139099121 val loss 0.941419243812561\n",
      "Epoch 9499: train loss 0.941419243812561 val loss 0.9851547479629517\n",
      "Epoch 9500: train loss 0.9851547479629517 val loss 0.9873439073562622\n",
      "Epoch 9501: train loss 0.9873439073562622 val loss 0.9304566979408264\n",
      "Epoch 9502: train loss 0.9304566979408264 val loss 0.9603074789047241\n",
      "Epoch 9503: train loss 0.9603074789047241 val loss 0.9548715353012085\n",
      "Epoch 9504: train loss 0.9548715353012085 val loss 0.9293335676193237\n",
      "Epoch 9505: train loss 0.9293335676193237 val loss 0.9517185091972351\n",
      "Epoch 9506: train loss 0.9517185091972351 val loss 0.9414259195327759\n",
      "Epoch 9507: train loss 0.9414259195327759 val loss 0.937370240688324\n",
      "Epoch 9508: train loss 0.937370240688324 val loss 0.9350792169570923\n",
      "Epoch 9509: train loss 0.9350792169570923 val loss 0.9317659139633179\n",
      "Epoch 9510: train loss 0.9317659139633179 val loss 0.9337033033370972\n",
      "Epoch 9511: train loss 0.9337033033370972 val loss 0.9334113597869873\n",
      "Epoch 9512: train loss 0.9334113597869873 val loss 0.9316321611404419\n",
      "Epoch 9513: train loss 0.9316321611404419 val loss 0.9344732761383057\n",
      "Epoch 9514: train loss 0.9344732761383057 val loss 0.9311861991882324\n",
      "Epoch 9515: train loss 0.9311861991882324 val loss 0.9515966176986694\n",
      "Epoch 9516: train loss 0.9515966176986694 val loss 0.9584186673164368\n",
      "Epoch 9517: train loss 0.9584186673164368 val loss 0.9340031743049622\n",
      "Epoch 9518: train loss 0.9340031743049622 val loss 0.932197093963623\n",
      "Epoch 9519: train loss 0.932197093963623 val loss 0.9545347690582275\n",
      "Epoch 9520: train loss 0.9545347690582275 val loss 0.9558768272399902\n",
      "Epoch 9521: train loss 0.9558768272399902 val loss 0.9332468509674072\n",
      "Epoch 9522: train loss 0.9332468509674072 val loss 0.9367985129356384\n",
      "Epoch 9523: train loss 0.9367985129356384 val loss 0.9321659207344055\n",
      "Epoch 9524: train loss 0.9321659207344055 val loss 0.9410852193832397\n",
      "Epoch 9525: train loss 0.9410852193832397 val loss 0.9439234137535095\n",
      "Epoch 9526: train loss 0.9439234137535095 val loss 0.9328852891921997\n",
      "Epoch 9527: train loss 0.9328852891921997 val loss 0.9288420677185059\n",
      "Epoch 9528: train loss 0.9288420677185059 val loss 0.9347566366195679\n",
      "Epoch 9529: train loss 0.9347566366195679 val loss 0.9429178237915039\n",
      "Epoch 9530: train loss 0.9429178237915039 val loss 0.9291573762893677\n",
      "Epoch 9531: train loss 0.9291573762893677 val loss 0.9392224550247192\n",
      "Epoch 9532: train loss 0.9392224550247192 val loss 0.9523733854293823\n",
      "Epoch 9533: train loss 0.9523733854293823 val loss 0.9299406409263611\n",
      "Epoch 9534: train loss 0.9299406409263611 val loss 0.9387344717979431\n",
      "Epoch 9535: train loss 0.9387344717979431 val loss 0.9647094011306763\n",
      "Epoch 9536: train loss 0.9647094011306763 val loss 0.9401024580001831\n",
      "Epoch 9537: train loss 0.9401024580001831 val loss 0.9327373504638672\n",
      "Epoch 9538: train loss 0.9327373504638672 val loss 0.9318547248840332\n",
      "Epoch 9539: train loss 0.9318547248840332 val loss 0.9339102506637573\n",
      "Epoch 9540: train loss 0.9339102506637573 val loss 0.9412633776664734\n",
      "Epoch 9541: train loss 0.9412633776664734 val loss 0.9315472841262817\n",
      "Epoch 9542: train loss 0.9315472841262817 val loss 0.9496744871139526\n",
      "Epoch 9543: train loss 0.9496744871139526 val loss 0.9529170989990234\n",
      "Epoch 9544: train loss 0.9529170989990234 val loss 0.9414700269699097\n",
      "Epoch 9545: train loss 0.9414700269699097 val loss 0.9341729283332825\n",
      "Epoch 9546: train loss 0.9341729283332825 val loss 0.932429850101471\n",
      "Epoch 9547: train loss 0.932429850101471 val loss 0.9322309494018555\n",
      "Epoch 9548: train loss 0.9322309494018555 val loss 0.935285210609436\n",
      "Epoch 9549: train loss 0.935285210609436 val loss 0.9314168095588684\n",
      "Epoch 9550: train loss 0.9314168095588684 val loss 0.9400269985198975\n",
      "Epoch 9551: train loss 0.9400269985198975 val loss 0.9467469453811646\n",
      "Epoch 9552: train loss 0.9467469453811646 val loss 0.9309062957763672\n",
      "Epoch 9553: train loss 0.9309062957763672 val loss 0.9320191144943237\n",
      "Epoch 9554: train loss 0.9320191144943237 val loss 0.9300905466079712\n",
      "Epoch 9555: train loss 0.9300905466079712 val loss 0.9372353553771973\n",
      "Epoch 9556: train loss 0.9372353553771973 val loss 0.9340629577636719\n",
      "Epoch 9557: train loss 0.9340629577636719 val loss 0.9300939440727234\n",
      "Epoch 9558: train loss 0.9300939440727234 val loss 0.9577518701553345\n",
      "Epoch 9559: train loss 0.9577518701553345 val loss 0.9430505037307739\n",
      "Epoch 9560: train loss 0.9430505037307739 val loss 0.9308007955551147\n",
      "Epoch 9561: train loss 0.9308007955551147 val loss 0.9559535980224609\n",
      "Epoch 9562: train loss 0.9559535980224609 val loss 0.9464865326881409\n",
      "Epoch 9563: train loss 0.9464865326881409 val loss 0.9464815258979797\n",
      "Epoch 9564: train loss 0.9464815258979797 val loss 0.9349895715713501\n",
      "Epoch 9565: train loss 0.9349895715713501 val loss 0.9338405132293701\n",
      "Epoch 9566: train loss 0.9338405132293701 val loss 0.9302003383636475\n",
      "Epoch 9567: train loss 0.9302003383636475 val loss 0.9331268668174744\n",
      "Epoch 9568: train loss 0.9331268668174744 val loss 0.945643961429596\n",
      "Epoch 9569: train loss 0.945643961429596 val loss 0.9426180124282837\n",
      "Epoch 9570: train loss 0.9426180124282837 val loss 0.9302482604980469\n",
      "Epoch 9571: train loss 0.9302482604980469 val loss 0.9289382696151733\n",
      "Epoch 9572: train loss 0.9289382696151733 val loss 0.9292728900909424\n",
      "Epoch 9573: train loss 0.9292728900909424 val loss 0.9321234226226807\n",
      "Epoch 9574: train loss 0.9321234226226807 val loss 0.9343410730361938\n",
      "Epoch 9575: train loss 0.9343410730361938 val loss 0.9327701330184937\n",
      "Epoch 9576: train loss 0.9327701330184937 val loss 0.9300062656402588\n",
      "Epoch 9577: train loss 0.9300062656402588 val loss 0.9324672222137451\n",
      "Epoch 9578: train loss 0.9324672222137451 val loss 0.9423308372497559\n",
      "Epoch 9579: train loss 0.9423308372497559 val loss 0.9329820275306702\n",
      "Epoch 9580: train loss 0.9329820275306702 val loss 0.9307308197021484\n",
      "Epoch 9581: train loss 0.9307308197021484 val loss 0.9308298230171204\n",
      "Epoch 9582: train loss 0.9308298230171204 val loss 0.9402708411216736\n",
      "Epoch 9583: train loss 0.9402708411216736 val loss 0.9646682739257812\n",
      "Epoch 9584: train loss 0.9646682739257812 val loss 0.9391688108444214\n",
      "Epoch 9585: train loss 0.9391688108444214 val loss 0.929629921913147\n",
      "Epoch 9586: train loss 0.929629921913147 val loss 0.9321287274360657\n",
      "Epoch 9587: train loss 0.9321287274360657 val loss 0.9284437894821167\n",
      "Epoch 9588: train loss 0.9284437894821167 val loss 0.9379355907440186\n",
      "Epoch 9589: train loss 0.9379355907440186 val loss 0.9326995611190796\n",
      "Epoch 9590: train loss 0.9326995611190796 val loss 0.929779052734375\n",
      "Epoch 9591: train loss 0.929779052734375 val loss 0.9288703203201294\n",
      "Epoch 9592: train loss 0.9288703203201294 val loss 0.9346103072166443\n",
      "Epoch 9593: train loss 0.9346103072166443 val loss 0.9339629411697388\n",
      "Epoch 9594: train loss 0.9339629411697388 val loss 0.927825927734375\n",
      "Epoch 9595: train loss 0.927825927734375 val loss 0.9435295462608337\n",
      "Epoch 9596: train loss 0.9435295462608337 val loss 0.9668881297111511\n",
      "Epoch 9597: train loss 0.9668881297111511 val loss 0.9384849071502686\n",
      "Epoch 9598: train loss 0.9384849071502686 val loss 0.9324057102203369\n",
      "Epoch 9599: train loss 0.9324057102203369 val loss 0.9297447204589844\n",
      "Epoch 9600: train loss 0.9297447204589844 val loss 0.9359092712402344\n",
      "Epoch 9601: train loss 0.9359092712402344 val loss 0.942267656326294\n",
      "Epoch 9602: train loss 0.942267656326294 val loss 0.9747124314308167\n",
      "Epoch 9603: train loss 0.9747124314308167 val loss 0.9464818239212036\n",
      "Epoch 9604: train loss 0.9464818239212036 val loss 0.9336645603179932\n",
      "Epoch 9605: train loss 0.9336645603179932 val loss 0.9286110401153564\n",
      "Epoch 9606: train loss 0.9286110401153564 val loss 0.9307555556297302\n",
      "Epoch 9607: train loss 0.9307555556297302 val loss 0.9296280145645142\n",
      "Epoch 9608: train loss 0.9296280145645142 val loss 0.9301252365112305\n",
      "Epoch 9609: train loss 0.9301252365112305 val loss 0.9400389194488525\n",
      "Epoch 9610: train loss 0.9400389194488525 val loss 0.9457032680511475\n",
      "Epoch 9611: train loss 0.9457032680511475 val loss 0.9369585514068604\n",
      "Epoch 9612: train loss 0.9369585514068604 val loss 0.9290449619293213\n",
      "Epoch 9613: train loss 0.9290449619293213 val loss 0.9472674131393433\n",
      "Epoch 9614: train loss 0.9472674131393433 val loss 0.9608188271522522\n",
      "Epoch 9615: train loss 0.9608188271522522 val loss 0.9382305145263672\n",
      "Epoch 9616: train loss 0.9382305145263672 val loss 0.928341805934906\n",
      "Epoch 9617: train loss 0.928341805934906 val loss 0.9321681261062622\n",
      "Epoch 9618: train loss 0.9321681261062622 val loss 0.9283678531646729\n",
      "Epoch 9619: train loss 0.9283678531646729 val loss 0.9381256103515625\n",
      "Epoch 9620: train loss 0.9381256103515625 val loss 0.9566094875335693\n",
      "Epoch 9621: train loss 0.9566094875335693 val loss 0.9474802017211914\n",
      "Epoch 9622: train loss 0.9474802017211914 val loss 0.9308961629867554\n",
      "Epoch 9623: train loss 0.9308961629867554 val loss 0.9452348947525024\n",
      "Epoch 9624: train loss 0.9452348947525024 val loss 0.9672505855560303\n",
      "Epoch 9625: train loss 0.9672505855560303 val loss 0.9562379121780396\n",
      "Epoch 9626: train loss 0.9562379121780396 val loss 0.9280797243118286\n",
      "Epoch 9627: train loss 0.9280797243118286 val loss 0.9498372077941895\n",
      "Epoch 9628: train loss 0.9498372077941895 val loss 0.9636005163192749\n",
      "Epoch 9629: train loss 0.9636005163192749 val loss 0.9341863393783569\n",
      "Epoch 9630: train loss 0.9341863393783569 val loss 0.9372894167900085\n",
      "Epoch 9631: train loss 0.9372894167900085 val loss 0.9752365350723267\n",
      "Epoch 9632: train loss 0.9752365350723267 val loss 0.9296239614486694\n",
      "Epoch 9633: train loss 0.9296239614486694 val loss 0.9825704097747803\n",
      "Epoch 9634: train loss 0.9825704097747803 val loss 0.9864353537559509\n",
      "Epoch 9635: train loss 0.9864353537559509 val loss 0.9367082118988037\n",
      "Epoch 9636: train loss 0.9367082118988037 val loss 0.9749054312705994\n",
      "Epoch 9637: train loss 0.9749054312705994 val loss 0.9602401852607727\n",
      "Epoch 9638: train loss 0.9602401852607727 val loss 0.9300354719161987\n",
      "Epoch 9639: train loss 0.9300354719161987 val loss 0.9818376302719116\n",
      "Epoch 9640: train loss 0.9818376302719116 val loss 0.9685962200164795\n",
      "Epoch 9641: train loss 0.9685962200164795 val loss 0.9324460029602051\n",
      "Epoch 9642: train loss 0.9324460029602051 val loss 1.0274442434310913\n",
      "Epoch 9643: train loss 1.0274442434310913 val loss 0.9320762157440186\n",
      "Epoch 9644: train loss 0.9320762157440186 val loss 0.9513184428215027\n",
      "Epoch 9645: train loss 0.9513184428215027 val loss 0.9476462602615356\n",
      "Epoch 9646: train loss 0.9476462602615356 val loss 0.9300678968429565\n",
      "Epoch 9647: train loss 0.9300678968429565 val loss 0.9392313957214355\n",
      "Epoch 9648: train loss 0.9392313957214355 val loss 0.9336278438568115\n",
      "Epoch 9649: train loss 0.9336278438568115 val loss 0.9292793273925781\n",
      "Epoch 9650: train loss 0.9292793273925781 val loss 0.9322675466537476\n",
      "Epoch 9651: train loss 0.9322675466537476 val loss 0.9318205118179321\n",
      "Epoch 9652: train loss 0.9318205118179321 val loss 0.9285242557525635\n",
      "Epoch 9653: train loss 0.9285242557525635 val loss 0.929526686668396\n",
      "Epoch 9654: train loss 0.929526686668396 val loss 0.9541623592376709\n",
      "Epoch 9655: train loss 0.9541623592376709 val loss 0.9385205507278442\n",
      "Epoch 9656: train loss 0.9385205507278442 val loss 0.9270576238632202\n",
      "Epoch 9657: train loss 0.9270576238632202 val loss 0.9368476271629333\n",
      "Epoch 9658: train loss 0.9368476271629333 val loss 0.9389345645904541\n",
      "Epoch 9659: train loss 0.9389345645904541 val loss 0.9520370960235596\n",
      "Epoch 9660: train loss 0.9520370960235596 val loss 0.9272226095199585\n",
      "Epoch 9661: train loss 0.9272226095199585 val loss 0.9426645040512085\n",
      "Epoch 9662: train loss 0.9426645040512085 val loss 0.9803224802017212\n",
      "Epoch 9663: train loss 0.9803224802017212 val loss 0.9298444986343384\n",
      "Epoch 9664: train loss 0.9298444986343384 val loss 0.9649860858917236\n",
      "Epoch 9665: train loss 0.9649860858917236 val loss 1.0294146537780762\n",
      "Epoch 9666: train loss 1.0294146537780762 val loss 0.9317945241928101\n",
      "Epoch 9667: train loss 0.9317945241928101 val loss 1.0720312595367432\n",
      "Epoch 9668: train loss 1.0720312595367432 val loss 0.9321394562721252\n",
      "Epoch 9669: train loss 0.9321394562721252 val loss 1.0183486938476562\n",
      "Epoch 9670: train loss 1.0183486938476562 val loss 0.9577629566192627\n",
      "Epoch 9671: train loss 0.9577629566192627 val loss 0.9281569719314575\n",
      "Epoch 9672: train loss 0.9281569719314575 val loss 0.9315956830978394\n",
      "Epoch 9673: train loss 0.9315956830978394 val loss 0.927600085735321\n",
      "Epoch 9674: train loss 0.927600085735321 val loss 0.9451773166656494\n",
      "Epoch 9675: train loss 0.9451773166656494 val loss 0.9637532830238342\n",
      "Epoch 9676: train loss 0.9637532830238342 val loss 0.9420346021652222\n",
      "Epoch 9677: train loss 0.9420346021652222 val loss 0.9283095598220825\n",
      "Epoch 9678: train loss 0.9283095598220825 val loss 0.9718711972236633\n",
      "Epoch 9679: train loss 0.9718711972236633 val loss 0.9571993350982666\n",
      "Epoch 9680: train loss 0.9571993350982666 val loss 0.9307107925415039\n",
      "Epoch 9681: train loss 0.9307107925415039 val loss 0.936383843421936\n",
      "Epoch 9682: train loss 0.936383843421936 val loss 0.9305135011672974\n",
      "Epoch 9683: train loss 0.9305135011672974 val loss 0.9605987071990967\n",
      "Epoch 9684: train loss 0.9605987071990967 val loss 1.0010532140731812\n",
      "Epoch 9685: train loss 1.0010532140731812 val loss 0.9374136924743652\n",
      "Epoch 9686: train loss 0.9374136924743652 val loss 1.02263343334198\n",
      "Epoch 9687: train loss 1.02263343334198 val loss 0.9298863410949707\n",
      "Epoch 9688: train loss 0.9298863410949707 val loss 0.9935709238052368\n",
      "Epoch 9689: train loss 0.9935709238052368 val loss 0.951262354850769\n",
      "Epoch 9690: train loss 0.951262354850769 val loss 0.93389493227005\n",
      "Epoch 9691: train loss 0.93389493227005 val loss 0.9666005373001099\n",
      "Epoch 9692: train loss 0.9666005373001099 val loss 0.9328114986419678\n",
      "Epoch 9693: train loss 0.9328114986419678 val loss 0.9395995140075684\n",
      "Epoch 9694: train loss 0.9395995140075684 val loss 0.9682000875473022\n",
      "Epoch 9695: train loss 0.9682000875473022 val loss 0.9329328536987305\n",
      "Epoch 9696: train loss 0.9329328536987305 val loss 0.9552235007286072\n",
      "Epoch 9697: train loss 0.9552235007286072 val loss 1.02376389503479\n",
      "Epoch 9698: train loss 1.02376389503479 val loss 0.9334862232208252\n",
      "Epoch 9699: train loss 0.9334862232208252 val loss 1.0742043256759644\n",
      "Epoch 9700: train loss 1.0742043256759644 val loss 0.9358997344970703\n",
      "Epoch 9701: train loss 0.9358997344970703 val loss 1.0363075733184814\n",
      "Epoch 9702: train loss 1.0363075733184814 val loss 0.980439305305481\n",
      "Epoch 9703: train loss 0.980439305305481 val loss 0.9395304918289185\n",
      "Epoch 9704: train loss 0.9395304918289185 val loss 1.008810043334961\n",
      "Epoch 9705: train loss 1.008810043334961 val loss 0.9380971193313599\n",
      "Epoch 9706: train loss 0.9380971193313599 val loss 0.9551918506622314\n",
      "Epoch 9707: train loss 0.9551918506622314 val loss 1.005448341369629\n",
      "Epoch 9708: train loss 1.005448341369629 val loss 0.9303029775619507\n",
      "Epoch 9709: train loss 0.9303029775619507 val loss 1.0358827114105225\n",
      "Epoch 9710: train loss 1.0358827114105225 val loss 0.9547476768493652\n",
      "Epoch 9711: train loss 0.9547476768493652 val loss 0.9684463739395142\n",
      "Epoch 9712: train loss 0.9684463739395142 val loss 1.0021007061004639\n",
      "Epoch 9713: train loss 1.0021007061004639 val loss 0.9366984367370605\n",
      "Epoch 9714: train loss 0.9366984367370605 val loss 0.9595716595649719\n",
      "Epoch 9715: train loss 0.9595716595649719 val loss 0.9325450658798218\n",
      "Epoch 9716: train loss 0.9325450658798218 val loss 0.9312214851379395\n",
      "Epoch 9717: train loss 0.9312214851379395 val loss 0.9307225346565247\n",
      "Epoch 9718: train loss 0.9307225346565247 val loss 0.9276551008224487\n",
      "Epoch 9719: train loss 0.9276551008224487 val loss 0.928840160369873\n",
      "Epoch 9720: train loss 0.928840160369873 val loss 0.9314501285552979\n",
      "Epoch 9721: train loss 0.9314501285552979 val loss 0.9318418502807617\n",
      "Epoch 9722: train loss 0.9318418502807617 val loss 0.9310950040817261\n",
      "Epoch 9723: train loss 0.9310950040817261 val loss 0.9357316493988037\n",
      "Epoch 9724: train loss 0.9357316493988037 val loss 0.9355762004852295\n",
      "Epoch 9725: train loss 0.9355762004852295 val loss 0.9333735704421997\n",
      "Epoch 9726: train loss 0.9333735704421997 val loss 0.9279764294624329\n",
      "Epoch 9727: train loss 0.9279764294624329 val loss 0.9312585592269897\n",
      "Epoch 9728: train loss 0.9312585592269897 val loss 0.9409557580947876\n",
      "Epoch 9729: train loss 0.9409557580947876 val loss 0.9319460391998291\n",
      "Epoch 9730: train loss 0.9319460391998291 val loss 0.9601421356201172\n",
      "Epoch 9731: train loss 0.9601421356201172 val loss 1.0056192874908447\n",
      "Epoch 9732: train loss 1.0056192874908447 val loss 0.9330153465270996\n",
      "Epoch 9733: train loss 0.9330153465270996 val loss 0.9807392358779907\n",
      "Epoch 9734: train loss 0.9807392358779907 val loss 0.9535636901855469\n",
      "Epoch 9735: train loss 0.9535636901855469 val loss 0.9382416009902954\n",
      "Epoch 9736: train loss 0.9382416009902954 val loss 0.9652508497238159\n",
      "Epoch 9737: train loss 0.9652508497238159 val loss 0.9449739456176758\n",
      "Epoch 9738: train loss 0.9449739456176758 val loss 0.9291555881500244\n",
      "Epoch 9739: train loss 0.9291555881500244 val loss 0.9427047967910767\n",
      "Epoch 9740: train loss 0.9427047967910767 val loss 0.9671190977096558\n",
      "Epoch 9741: train loss 0.9671190977096558 val loss 0.9387086629867554\n",
      "Epoch 9742: train loss 0.9387086629867554 val loss 0.9356262683868408\n",
      "Epoch 9743: train loss 0.9356262683868408 val loss 0.9293190836906433\n",
      "Epoch 9744: train loss 0.9293190836906433 val loss 0.939070463180542\n",
      "Epoch 9745: train loss 0.939070463180542 val loss 0.9406380653381348\n",
      "Epoch 9746: train loss 0.9406380653381348 val loss 0.9307851791381836\n",
      "Epoch 9747: train loss 0.9307851791381836 val loss 0.9488625526428223\n",
      "Epoch 9748: train loss 0.9488625526428223 val loss 0.9480249881744385\n",
      "Epoch 9749: train loss 0.9480249881744385 val loss 0.9267529249191284\n",
      "Epoch 9750: train loss 0.9267529249191284 val loss 0.9331763982772827\n",
      "Epoch 9751: train loss 0.9331763982772827 val loss 0.9382988214492798\n",
      "Epoch 9752: train loss 0.9382988214492798 val loss 0.9282788038253784\n",
      "Epoch 9753: train loss 0.9282788038253784 val loss 0.9412257671356201\n",
      "Epoch 9754: train loss 0.9412257671356201 val loss 0.9624530076980591\n",
      "Epoch 9755: train loss 0.9624530076980591 val loss 0.9298945665359497\n",
      "Epoch 9756: train loss 0.9298945665359497 val loss 0.9299776554107666\n",
      "Epoch 9757: train loss 0.9299776554107666 val loss 0.9482971429824829\n",
      "Epoch 9758: train loss 0.9482971429824829 val loss 0.957777738571167\n",
      "Epoch 9759: train loss 0.957777738571167 val loss 0.9373127222061157\n",
      "Epoch 9760: train loss 0.9373127222061157 val loss 0.9461420178413391\n",
      "Epoch 9761: train loss 0.9461420178413391 val loss 0.9650280475616455\n",
      "Epoch 9762: train loss 0.9650280475616455 val loss 0.9466477632522583\n",
      "Epoch 9763: train loss 0.9466477632522583 val loss 0.9307010173797607\n",
      "Epoch 9764: train loss 0.9307010173797607 val loss 0.9519345760345459\n",
      "Epoch 9765: train loss 0.9519345760345459 val loss 0.9381999969482422\n",
      "Epoch 9766: train loss 0.9381999969482422 val loss 0.9307792782783508\n",
      "Epoch 9767: train loss 0.9307792782783508 val loss 0.9322250485420227\n",
      "Epoch 9768: train loss 0.9322250485420227 val loss 0.927388072013855\n",
      "Epoch 9769: train loss 0.927388072013855 val loss 0.9275608062744141\n",
      "Epoch 9770: train loss 0.9275608062744141 val loss 0.9321911931037903\n",
      "Epoch 9771: train loss 0.9321911931037903 val loss 0.9296854734420776\n",
      "Epoch 9772: train loss 0.9296854734420776 val loss 0.9264198541641235\n",
      "Epoch 9773: train loss 0.9264198541641235 val loss 0.9255750179290771\n",
      "Epoch 9774: train loss 0.9255750179290771 val loss 0.9267863035202026\n",
      "Epoch 9775: train loss 0.9267863035202026 val loss 0.9253592491149902\n",
      "Epoch 9776: train loss 0.9253592491149902 val loss 0.9277329444885254\n",
      "Epoch 9777: train loss 0.9277329444885254 val loss 0.9314005374908447\n",
      "Epoch 9778: train loss 0.9314005374908447 val loss 0.9328572750091553\n",
      "Epoch 9779: train loss 0.9328572750091553 val loss 0.9268777370452881\n",
      "Epoch 9780: train loss 0.9268777370452881 val loss 0.9280858039855957\n",
      "Epoch 9781: train loss 0.9280858039855957 val loss 0.9262659549713135\n",
      "Epoch 9782: train loss 0.9262659549713135 val loss 0.9305194616317749\n",
      "Epoch 9783: train loss 0.9305194616317749 val loss 0.9434353113174438\n",
      "Epoch 9784: train loss 0.9434353113174438 val loss 0.9337716102600098\n",
      "Epoch 9785: train loss 0.9337716102600098 val loss 0.9250180721282959\n",
      "Epoch 9786: train loss 0.9250180721282959 val loss 0.94219970703125\n",
      "Epoch 9787: train loss 0.94219970703125 val loss 1.0026953220367432\n",
      "Epoch 9788: train loss 1.0026953220367432 val loss 0.934712290763855\n",
      "Epoch 9789: train loss 0.934712290763855 val loss 0.9397141933441162\n",
      "Epoch 9790: train loss 0.9397141933441162 val loss 1.0123229026794434\n",
      "Epoch 9791: train loss 1.0123229026794434 val loss 0.9312502145767212\n",
      "Epoch 9792: train loss 0.9312502145767212 val loss 1.005800485610962\n",
      "Epoch 9793: train loss 1.005800485610962 val loss 0.9588358402252197\n",
      "Epoch 9794: train loss 0.9588358402252197 val loss 0.9337222576141357\n",
      "Epoch 9795: train loss 0.9337222576141357 val loss 1.0138514041900635\n",
      "Epoch 9796: train loss 1.0138514041900635 val loss 0.9572968482971191\n",
      "Epoch 9797: train loss 0.9572968482971191 val loss 0.9358655214309692\n",
      "Epoch 9798: train loss 0.9358655214309692 val loss 1.0149624347686768\n",
      "Epoch 9799: train loss 1.0149624347686768 val loss 0.9421250820159912\n",
      "Epoch 9800: train loss 0.9421250820159912 val loss 0.9619234800338745\n",
      "Epoch 9801: train loss 0.9619234800338745 val loss 1.061581015586853\n",
      "Epoch 9802: train loss 1.061581015586853 val loss 0.9391228556632996\n",
      "Epoch 9803: train loss 0.9391228556632996 val loss 1.0248539447784424\n",
      "Epoch 9804: train loss 1.0248539447784424 val loss 0.9346011877059937\n",
      "Epoch 9805: train loss 0.9346011877059937 val loss 0.9913952350616455\n",
      "Epoch 9806: train loss 0.9913952350616455 val loss 0.9400251507759094\n",
      "Epoch 9807: train loss 0.9400251507759094 val loss 0.9550086259841919\n",
      "Epoch 9808: train loss 0.9550086259841919 val loss 1.0128898620605469\n",
      "Epoch 9809: train loss 1.0128898620605469 val loss 0.9270948171615601\n",
      "Epoch 9810: train loss 0.9270948171615601 val loss 0.9959672689437866\n",
      "Epoch 9811: train loss 0.9959672689437866 val loss 0.987859845161438\n",
      "Epoch 9812: train loss 0.987859845161438 val loss 0.9399781227111816\n",
      "Epoch 9813: train loss 0.9399781227111816 val loss 1.0334594249725342\n",
      "Epoch 9814: train loss 1.0334594249725342 val loss 0.9297642707824707\n",
      "Epoch 9815: train loss 0.9297642707824707 val loss 0.9878401160240173\n",
      "Epoch 9816: train loss 0.9878401160240173 val loss 0.9510607719421387\n",
      "Epoch 9817: train loss 0.9510607719421387 val loss 0.9272036552429199\n",
      "Epoch 9818: train loss 0.9272036552429199 val loss 0.9294931888580322\n",
      "Epoch 9819: train loss 0.9294931888580322 val loss 0.9275758266448975\n",
      "Epoch 9820: train loss 0.9275758266448975 val loss 0.9295287132263184\n",
      "Epoch 9821: train loss 0.9295287132263184 val loss 0.9360799789428711\n",
      "Epoch 9822: train loss 0.9360799789428711 val loss 0.9304968118667603\n",
      "Epoch 9823: train loss 0.9304968118667603 val loss 0.9256665706634521\n",
      "Epoch 9824: train loss 0.9256665706634521 val loss 0.9255148768424988\n",
      "Epoch 9825: train loss 0.9255148768424988 val loss 0.9289975166320801\n",
      "Epoch 9826: train loss 0.9289975166320801 val loss 0.9255316853523254\n",
      "Epoch 9827: train loss 0.9255316853523254 val loss 0.9266932010650635\n",
      "Epoch 9828: train loss 0.9266932010650635 val loss 0.9312365651130676\n",
      "Epoch 9829: train loss 0.9312365651130676 val loss 0.9299107193946838\n",
      "Epoch 9830: train loss 0.9299107193946838 val loss 0.9279749393463135\n",
      "Epoch 9831: train loss 0.9279749393463135 val loss 0.9277068972587585\n",
      "Epoch 9832: train loss 0.9277068972587585 val loss 0.9291071891784668\n",
      "Epoch 9833: train loss 0.9291071891784668 val loss 0.9362246990203857\n",
      "Epoch 9834: train loss 0.9362246990203857 val loss 0.9268172979354858\n",
      "Epoch 9835: train loss 0.9268172979354858 val loss 0.9271142482757568\n",
      "Epoch 9836: train loss 0.9271142482757568 val loss 0.9323792457580566\n",
      "Epoch 9837: train loss 0.9323792457580566 val loss 0.9325928688049316\n",
      "Epoch 9838: train loss 0.9325928688049316 val loss 0.9330847263336182\n",
      "Epoch 9839: train loss 0.9330847263336182 val loss 0.9261265993118286\n",
      "Epoch 9840: train loss 0.9261265993118286 val loss 0.9301865696907043\n",
      "Epoch 9841: train loss 0.9301865696907043 val loss 0.9392709732055664\n",
      "Epoch 9842: train loss 0.9392709732055664 val loss 0.9369610548019409\n",
      "Epoch 9843: train loss 0.9369610548019409 val loss 0.9288346171379089\n",
      "Epoch 9844: train loss 0.9288346171379089 val loss 0.9308387041091919\n",
      "Epoch 9845: train loss 0.9308387041091919 val loss 0.9281784296035767\n",
      "Epoch 9846: train loss 0.9281784296035767 val loss 0.9291157722473145\n",
      "Epoch 9847: train loss 0.9291157722473145 val loss 0.9315667152404785\n",
      "Epoch 9848: train loss 0.9315667152404785 val loss 0.9267958402633667\n",
      "Epoch 9849: train loss 0.9267958402633667 val loss 0.9328538179397583\n",
      "Epoch 9850: train loss 0.9328538179397583 val loss 0.935447096824646\n",
      "Epoch 9851: train loss 0.935447096824646 val loss 0.9539609551429749\n",
      "Epoch 9852: train loss 0.9539609551429749 val loss 0.9445158243179321\n",
      "Epoch 9853: train loss 0.9445158243179321 val loss 0.9284695386886597\n",
      "Epoch 9854: train loss 0.9284695386886597 val loss 0.9350334405899048\n",
      "Epoch 9855: train loss 0.9350334405899048 val loss 0.9345411658287048\n",
      "Epoch 9856: train loss 0.9345411658287048 val loss 0.9294324517250061\n",
      "Epoch 9857: train loss 0.9294324517250061 val loss 0.9317072629928589\n",
      "Epoch 9858: train loss 0.9317072629928589 val loss 0.9287896156311035\n",
      "Epoch 9859: train loss 0.9287896156311035 val loss 0.9268119931221008\n",
      "Epoch 9860: train loss 0.9268119931221008 val loss 0.9262977838516235\n",
      "Epoch 9861: train loss 0.9262977838516235 val loss 0.929007351398468\n",
      "Epoch 9862: train loss 0.929007351398468 val loss 0.9337754249572754\n",
      "Epoch 9863: train loss 0.9337754249572754 val loss 0.9297491908073425\n",
      "Epoch 9864: train loss 0.9297491908073425 val loss 0.926695704460144\n",
      "Epoch 9865: train loss 0.926695704460144 val loss 0.9266351461410522\n",
      "Epoch 9866: train loss 0.9266351461410522 val loss 0.924625039100647\n",
      "Epoch 9867: train loss 0.924625039100647 val loss 0.9247332811355591\n",
      "Epoch 9868: train loss 0.9247332811355591 val loss 0.9305177927017212\n",
      "Epoch 9869: train loss 0.9305177927017212 val loss 0.9311448335647583\n",
      "Epoch 9870: train loss 0.9311448335647583 val loss 0.9260541200637817\n",
      "Epoch 9871: train loss 0.9260541200637817 val loss 0.9266580939292908\n",
      "Epoch 9872: train loss 0.9266580939292908 val loss 0.9235568642616272\n",
      "Epoch 9873: train loss 0.9235568642616272 val loss 0.9245812892913818\n",
      "Epoch 9874: train loss 0.9245812892913818 val loss 0.9300509691238403\n",
      "Epoch 9875: train loss 0.9300509691238403 val loss 0.9473761320114136\n",
      "Epoch 9876: train loss 0.9473761320114136 val loss 0.935090184211731\n",
      "Epoch 9877: train loss 0.935090184211731 val loss 0.9251828193664551\n",
      "Epoch 9878: train loss 0.9251828193664551 val loss 0.9374142289161682\n",
      "Epoch 9879: train loss 0.9374142289161682 val loss 0.9719464778900146\n",
      "Epoch 9880: train loss 0.9719464778900146 val loss 0.9346953630447388\n",
      "Epoch 9881: train loss 0.9346953630447388 val loss 0.9368849992752075\n",
      "Epoch 9882: train loss 0.9368849992752075 val loss 0.9784172773361206\n",
      "Epoch 9883: train loss 0.9784172773361206 val loss 0.9278169870376587\n",
      "Epoch 9884: train loss 0.9278169870376587 val loss 0.9515112638473511\n",
      "Epoch 9885: train loss 0.9515112638473511 val loss 0.9946566820144653\n",
      "Epoch 9886: train loss 0.9946566820144653 val loss 0.9291591644287109\n",
      "Epoch 9887: train loss 0.9291591644287109 val loss 0.9869773387908936\n",
      "Epoch 9888: train loss 0.9869773387908936 val loss 0.9515355825424194\n",
      "Epoch 9889: train loss 0.9515355825424194 val loss 0.9364050626754761\n",
      "Epoch 9890: train loss 0.9364050626754761 val loss 0.9790248870849609\n",
      "Epoch 9891: train loss 0.9790248870849609 val loss 0.9436345100402832\n",
      "Epoch 9892: train loss 0.9436345100402832 val loss 0.9297260046005249\n",
      "Epoch 9893: train loss 0.9297260046005249 val loss 0.9499448537826538\n",
      "Epoch 9894: train loss 0.9499448537826538 val loss 0.9595227241516113\n",
      "Epoch 9895: train loss 0.9595227241516113 val loss 0.9273161888122559\n",
      "Epoch 9896: train loss 0.9273161888122559 val loss 0.9510768055915833\n",
      "Epoch 9897: train loss 0.9510768055915833 val loss 0.9596130847930908\n",
      "Epoch 9898: train loss 0.9596130847930908 val loss 0.9270095825195312\n",
      "Epoch 9899: train loss 0.9270095825195312 val loss 1.00840425491333\n",
      "Epoch 9900: train loss 1.00840425491333 val loss 0.9801268577575684\n",
      "Epoch 9901: train loss 0.9801268577575684 val loss 0.934807300567627\n",
      "Epoch 9902: train loss 0.934807300567627 val loss 1.0255162715911865\n",
      "Epoch 9903: train loss 1.0255162715911865 val loss 0.9295972585678101\n",
      "Epoch 9904: train loss 0.9295972585678101 val loss 0.966930627822876\n",
      "Epoch 9905: train loss 0.966930627822876 val loss 0.9614653587341309\n",
      "Epoch 9906: train loss 0.9614653587341309 val loss 0.9289313554763794\n",
      "Epoch 9907: train loss 0.9289313554763794 val loss 0.9287978410720825\n",
      "Epoch 9908: train loss 0.9287978410720825 val loss 0.9326087236404419\n",
      "Epoch 9909: train loss 0.9326087236404419 val loss 0.9253090620040894\n",
      "Epoch 9910: train loss 0.9253090620040894 val loss 0.9245045781135559\n",
      "Epoch 9911: train loss 0.9245045781135559 val loss 0.9352079629898071\n",
      "Epoch 9912: train loss 0.9352079629898071 val loss 0.9451415538787842\n",
      "Epoch 9913: train loss 0.9451415538787842 val loss 0.926474928855896\n",
      "Epoch 9914: train loss 0.926474928855896 val loss 0.9313942193984985\n",
      "Epoch 9915: train loss 0.9313942193984985 val loss 0.9591931104660034\n",
      "Epoch 9916: train loss 0.9591931104660034 val loss 0.9289056062698364\n",
      "Epoch 9917: train loss 0.9289056062698364 val loss 0.9933884143829346\n",
      "Epoch 9918: train loss 0.9933884143829346 val loss 0.9868189692497253\n",
      "Epoch 9919: train loss 0.9868189692497253 val loss 0.943792998790741\n",
      "Epoch 9920: train loss 0.943792998790741 val loss 1.0522079467773438\n",
      "Epoch 9921: train loss 1.0522079467773438 val loss 0.9281917810440063\n",
      "Epoch 9922: train loss 0.9281917810440063 val loss 0.963553786277771\n",
      "Epoch 9923: train loss 0.963553786277771 val loss 0.9448244571685791\n",
      "Epoch 9924: train loss 0.9448244571685791 val loss 0.9306395649909973\n",
      "Epoch 9925: train loss 0.9306395649909973 val loss 0.9479138851165771\n",
      "Epoch 9926: train loss 0.9479138851165771 val loss 0.9590795040130615\n",
      "Epoch 9927: train loss 0.9590795040130615 val loss 0.9405735731124878\n",
      "Epoch 9928: train loss 0.9405735731124878 val loss 0.9248467683792114\n",
      "Epoch 9929: train loss 0.9248467683792114 val loss 0.9561525583267212\n",
      "Epoch 9930: train loss 0.9561525583267212 val loss 0.9555481672286987\n",
      "Epoch 9931: train loss 0.9555481672286987 val loss 0.9267537593841553\n",
      "Epoch 9932: train loss 0.9267537593841553 val loss 0.9520498514175415\n",
      "Epoch 9933: train loss 0.9520498514175415 val loss 0.9330085515975952\n",
      "Epoch 9934: train loss 0.9330085515975952 val loss 0.9302141666412354\n",
      "Epoch 9935: train loss 0.9302141666412354 val loss 0.9336577653884888\n",
      "Epoch 9936: train loss 0.9336577653884888 val loss 0.9287266731262207\n",
      "Epoch 9937: train loss 0.9287266731262207 val loss 0.9406326413154602\n",
      "Epoch 9938: train loss 0.9406326413154602 val loss 0.9703441858291626\n",
      "Epoch 9939: train loss 0.9703441858291626 val loss 0.940906286239624\n",
      "Epoch 9940: train loss 0.940906286239624 val loss 0.9250285625457764\n",
      "Epoch 9941: train loss 0.9250285625457764 val loss 0.9603430032730103\n",
      "Epoch 9942: train loss 0.9603430032730103 val loss 0.9512437582015991\n",
      "Epoch 9943: train loss 0.9512437582015991 val loss 0.9290111660957336\n",
      "Epoch 9944: train loss 0.9290111660957336 val loss 0.9247905015945435\n",
      "Epoch 9945: train loss 0.9247905015945435 val loss 0.9374092817306519\n",
      "Epoch 9946: train loss 0.9374092817306519 val loss 0.9410803318023682\n",
      "Epoch 9947: train loss 0.9410803318023682 val loss 0.9294700622558594\n",
      "Epoch 9948: train loss 0.9294700622558594 val loss 0.9284309148788452\n",
      "Epoch 9949: train loss 0.9284309148788452 val loss 0.9256377220153809\n",
      "Epoch 9950: train loss 0.9256377220153809 val loss 0.9243885278701782\n",
      "Epoch 9951: train loss 0.9243885278701782 val loss 0.9254724383354187\n",
      "Epoch 9952: train loss 0.9254724383354187 val loss 0.9279928207397461\n",
      "Epoch 9953: train loss 0.9279928207397461 val loss 0.9307171106338501\n",
      "Epoch 9954: train loss 0.9307171106338501 val loss 0.9251481294631958\n",
      "Epoch 9955: train loss 0.9251481294631958 val loss 0.923438310623169\n",
      "Epoch 9956: train loss 0.923438310623169 val loss 0.9313517808914185\n",
      "Epoch 9957: train loss 0.9313517808914185 val loss 0.9309867024421692\n",
      "Epoch 9958: train loss 0.9309867024421692 val loss 0.9297076463699341\n",
      "Epoch 9959: train loss 0.9297076463699341 val loss 0.9251301288604736\n",
      "Epoch 9960: train loss 0.9251301288604736 val loss 0.9245991706848145\n",
      "Epoch 9961: train loss 0.9245991706848145 val loss 0.9267292022705078\n",
      "Epoch 9962: train loss 0.9267292022705078 val loss 0.9353647232055664\n",
      "Epoch 9963: train loss 0.9353647232055664 val loss 0.9253429174423218\n",
      "Epoch 9964: train loss 0.9253429174423218 val loss 0.9252095222473145\n",
      "Epoch 9965: train loss 0.9252095222473145 val loss 0.9328012466430664\n",
      "Epoch 9966: train loss 0.9328012466430664 val loss 0.9263050556182861\n",
      "Epoch 9967: train loss 0.9263050556182861 val loss 0.926303505897522\n",
      "Epoch 9968: train loss 0.926303505897522 val loss 0.9272615909576416\n",
      "Epoch 9969: train loss 0.9272615909576416 val loss 0.9260435104370117\n",
      "Epoch 9970: train loss 0.9260435104370117 val loss 0.924823522567749\n",
      "Epoch 9971: train loss 0.924823522567749 val loss 0.9249199628829956\n",
      "Epoch 9972: train loss 0.9249199628829956 val loss 0.931781530380249\n",
      "Epoch 9973: train loss 0.931781530380249 val loss 0.9263530969619751\n",
      "Epoch 9974: train loss 0.9263530969619751 val loss 0.923880934715271\n",
      "Epoch 9975: train loss 0.923880934715271 val loss 0.922416090965271\n",
      "Epoch 9976: train loss 0.922416090965271 val loss 0.9269887208938599\n",
      "Epoch 9977: train loss 0.9269887208938599 val loss 0.9381705522537231\n",
      "Epoch 9978: train loss 0.9381705522537231 val loss 0.9753532409667969\n",
      "Epoch 9979: train loss 0.9753532409667969 val loss 0.9385367631912231\n",
      "Epoch 9980: train loss 0.9385367631912231 val loss 0.9326961040496826\n",
      "Epoch 9981: train loss 0.9326961040496826 val loss 0.9284337759017944\n",
      "Epoch 9982: train loss 0.9284337759017944 val loss 0.9310731887817383\n",
      "Epoch 9983: train loss 0.9310731887817383 val loss 0.9342726469039917\n",
      "Epoch 9984: train loss 0.9342726469039917 val loss 0.9281691312789917\n",
      "Epoch 9985: train loss 0.9281691312789917 val loss 0.9252535700798035\n",
      "Epoch 9986: train loss 0.9252535700798035 val loss 0.928038477897644\n",
      "Epoch 9987: train loss 0.928038477897644 val loss 0.9317445158958435\n",
      "Epoch 9988: train loss 0.9317445158958435 val loss 0.9296296834945679\n",
      "Epoch 9989: train loss 0.9296296834945679 val loss 0.9249017238616943\n",
      "Epoch 9990: train loss 0.9249017238616943 val loss 0.9272942543029785\n",
      "Epoch 9991: train loss 0.9272942543029785 val loss 0.9316693544387817\n",
      "Epoch 9992: train loss 0.9316693544387817 val loss 0.9271218776702881\n",
      "Epoch 9993: train loss 0.9271218776702881 val loss 0.9268758296966553\n",
      "Epoch 9994: train loss 0.9268758296966553 val loss 0.9284778833389282\n",
      "Epoch 9995: train loss 0.9284778833389282 val loss 0.9260342121124268\n",
      "Epoch 9996: train loss 0.9260342121124268 val loss 0.9339526891708374\n",
      "Epoch 9997: train loss 0.9339526891708374 val loss 0.948004961013794\n",
      "Epoch 9998: train loss 0.948004961013794 val loss 0.9541533589363098\n",
      "Epoch 9999: train loss 0.9541533589363098 val loss 0.9224053025245667\n",
      "Epoch 10000: train loss 0.9224053025245667 val loss 0.9841156005859375\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "  model, \n",
    "  train_dataset=val, \n",
    "  val_dataset=val, \n",
    "  n_epochs=10000,\n",
    "  lr=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"models/lstmae_5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (rnn1): LSTM(1, 256, batch_first=True)\n",
       "    (rnn2): LSTM(256, 128, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn1): LSTM(128, 128, batch_first=True)\n",
       "    (rnn2): LSTM(128, 256, batch_first=True)\n",
       "    (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"models/lstmae_5000\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faad4a5dd10>]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model(dataset[0]).cpu().detach().numpy().squeeze())\n",
    "plt.plot(dataset[0].cpu().detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-b8C0A6mP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
