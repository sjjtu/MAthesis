\section{Theoretical background on Differential Privacy}\label{ch2}

In this chapter we briefly describe and derive the most important results from Cynthia Dwork's work on differential privacy that was first introduced in 2006 \parencite{dwork2006differential}. This summary heavily relies on her writings in \parencite{dwork2019differential} as well as lecture notes from \parencite{lecture_CSE711}.

\subsection{Defining differential privacy}
Differential privacy (DP) should be understood as an agreement between the data holder and the data subject: the latter should not be "affected, adversely or otherwise, by allowing [her] data to be used in any study or analysis, no matter what other studies, data sets or information sources are available". This addresses the paradox of learning something useful about a population while learning nothing about the individuals


\begin{ex}[Randomised response]
    In 1965 Warner \parencite{stan65rr} proposes the following random answering procedure: In a study where participants are asked to answer with ``Yes'' or ``No'' whether they have engaged in an illegal or embarrassing activity $A$, they should:
    \begin{enumerate}
        \item Flip a coin
        \item If the coin shows tails, then the participant should respond truthfully.
        \item If the coin shows head, then the participant should flip the coin a second time and answer "Yes" if the second coin shows head and "no" otherwise.
    \end{enumerate}
    This procedure ensures participants' privacy by ``plausible deniability''; each participant's answer has non-zero probability of being truthful or not. By understanding the probabilities of the noise generation process, the data analyst can estimate the true number of ``yes'' and ``no'' answers. To this end, let $p$ be the true percentage of ``yes'' answers, $N$ the total number of participants, $n_{true}$ the true number of ``yes'' responses and $\hat{n}_{obs}$ the observed number of ``yes'' responses. We assume a fair coin with equal probability of showing heads or tails. Then the expected number of ``yes'' answers after applying the described procedure is:
    \begin{align}
        \mathbb{E}(``Yes'') = \frac{1}{4} n_{true} + \frac{1}{4} (N-n_{true}) + \frac{1}{2} n_{true} = \frac{1}{4} N + \frac{n_{true}}{2}
    \end{align}
    We can estimate this using the $\hat{n}_{obs} \approx \mathbb{E}("Yes") = \frac{1}{4} N + \frac{n_{true}}{2}$ and finally solving for $n_{true}$ yields the estimate:
    \begin{align}
        \hat{n_{true}} = 2 \hat{n}_{obs} - \frac{1}{2} N
    \end{align}
\end{ex}


\begin{definition}[Probability Simplex]
    Given a discrete set $B$, the probability simplex over $B$ is defined as the set
    \begin{align}
        \Delta(B) = \left\{ x \in \mathbb{R}^{|B|}, x_i \ge 0 \text{ and } \sum_i x_i = 1  \right\}
    \end{align}
\end{definition}

\begin{definition}[Randomised Algorithm]
    A randomized algorithm $\mathcal{M}$ with domain $A$ and discrete range $B$ is associated with a mapping $M: A \rightarrow \Delta(B)$. On input $a\in A$ algorithm $\mathcal{M}$ outputs $\mathcal{M}(a)=b$ with probability $(M(a))_b$
\end{definition}

\begin{definition}[Histogram representation of a data base]
        Given a set $\mathcal{X}$, the universe of all possible records, the histrogram representation of a database $x$ is the vector
        \begin{align}
            x \in \mathbb{N}^{|\mathcal{X}|}
        \end{align}
       in which each entry $x_i$ represents the number of elements in database $x$ of type $i\in \mathcal{X}$.
\end{definition}

The previous definition of a database might sound cryptic at first, hence we will illustrate it with the following example:
\begin{ex}[Database of patients]
    Let $\mathcal{X}=\{P_1, ..., P_N\}$ be the set of $N$ distinct patients in a study. Then $x_1 = (1,0,...,0) \in \mathbb{N}^N$ would correspond to patient $P_1$, $x_2 = (0,1,0...,0) \in \mathbb{N}^N$ to patient $P_2$ and so on.
\end{ex}

Equipped with this definition of a database one can now naturally define a way to measure ``how much databases differ'', i. e. in how many entries they differ.

\begin{definition}[$l_1$-norm of a database in histogram representation]
    The $l_1$-norm of a database is a measure of the size of the database and defined as:
    \begin{align}
        ||x||_1 = \sum_{i=1}^{|\mathcal{X}|} |x_i|
    \end{align}
\end{definition}

This immediately gives rise to a notion of distance between two databases $x$ and $y$, namely:
\begin{align}
    ||x-y||_1   = \sum_{i=1}^{|\mathcal{X}|} |x_i-y_i|
\end{align}
which basically counts the number of different entries.

Now we are ready to give the general definition of differential privacy:
\begin{definition}[$(\epsilon, \delta)$-DP]
    A randomised algorithm $\mathcal{M}$ with domain $\mathbb{N}^{|\mathcal{X}|}$ is $(\epsilon, \delta)$- differentially private if for all outcomes $S \subset ran \mathcal{M}$ and for all databases $x,y \in \mathbb{N}^{|\mathcal{X}|}$, such that $||x-y||_1$ (i. e. they only differ in one element) we have
    \begin{align}
        \mathbb{P}(\mathcal{M}(x) \in S) \le e^\epsilon \cdot \mathbb{P}(\mathcal{M}(y) \in S) + \delta 
    \end{align}
    where the probability is taken over the randomness of $\mathcal{M}$. If $\delta=0$, we say $\mathcal{M}$ is $\epsilon$-differentially private.
\end{definition}

\colorbox{red}{why $e^\epsilon$}

\begin{ex}[Randomised response revisited]
    
\end{ex}

\subsection{Important results for Differential Privacy}

\begin{thm}[DP requires randomisation] \label{thm:dp_random}
    Any non-trivial DP-mechanism requires randomisation.
\end{thm}
\begin{proof}
    TBA
\end{proof}

\begin{thm}[Post-processing] \label{thm:postpro}
    Let $\mathcal{M}: \mathbb{N}^{|\mathcal{X}|} \rightarrow R$ be a randomised algorithm that is $(\epsilon, \delta)$- DP. Further let $f: R \rightarrow R'$ an arbitrary function. Then $f \circ \mathcal{M}$ is also $(\epsilon, \delta)$ -DP.
\end{thm}
\begin{proof}
    First fix data sets $x,y \in \mathbb{N}^{|\mathcal{X}|}$, s. t. $||x-y||_1\le 1$ and outcome $S' \subseteq R'$. Define a set $S=\left\{r\in R: f(r) \in S'\right\}$. Then we have:
    \begin{align}
        \mathbb{P}(f(\mathcal{M}(x))\in S') &= \mathbb{P}(\mathcal{M}(x)\in S) \nonumber \\
        &\le e^\epsilon \cdot \mathbb{P}(\mathcal{M}(y)\in S) + \delta \nonumber \\
        &= e^\epsilon \cdot \mathbb{P}(f(\mathcal{M}(y))\in S') + \delta
    \end{align}
    where the inequality follows from the $(\epsilon, \delta)-DP$ of $\mathcal{M}$.
\end{proof}

\begin{thm}[Group privacy]
    Let $\mathcal{M}: \mathbb{N}^{|\mathcal{X}|} \rightarrow R$ be a randomised algorithm that is $(\epsilon, \delta)$- DP, then $\mathcal{M}$ is $(k\epsilon, k e^{k\epsilon} \delta)$- DP for groups of size $k$, i. e. it holds for databases $x,y \in \mathbb{N}^{|\mathcal{X}|}$ such that $||x-y||_1\le k$ and for all $S \subseteq R$:
    \begin{align}
        \mathbb{P}(\mathcal{M}(x) \in S) \le e^{k\epsilon} \cdot \mathbb{P}(\mathcal{M}(y) \in S) + k\delta 
    \end{align}
\end{thm}
\begin{proof}
    First fix data sets $x,y \in \mathbb{N}^{|\mathcal{X}|}$, s. t. $||x-y||_1\le k$ and outcome $S \subseteq R$. Now there exists a series of databases $z_0,..., z_k$, such that $x=z_0$ and $y=z_k$ and $|| z_{i+1} - z_i||_1 \le 1$, i. e. we can find a series of databases that transforms $x$ into $y$ by removing or adding one record at a time. Then we have:
    \begin{align}
        \mathbb{P}(\mathcal{M}(x)\in S) &= \mathbb{P}(\mathcal{M}(z_0)\in S) \nonumber \\
        &\le e^\epsilon \cdot \mathbb{P}(\mathcal{M}(z_1)\in S) + \delta \nonumber \\
        &\le e^\epsilon \left( e^\epsilon \cdot \mathbb{P}(\mathcal{M}(z_2)\in S) + \delta \right) + \delta \nonumber \\
        &\le ... \nonumber \\
        &= k e^\epsilon \cdot \mathbb{P}(f(\mathcal{M}(y))\in S') + k e^{k \epsilon} \delta
    \end{align}
\end{proof}

\begin{thm}[Standard composition]
    Let $\mathcal{M}_1: \mathbb{N}^{|\mathcal{X}|} \rightarrow R_1$ and $\mathcal{M}_2: \mathbb{N}^{|\mathcal{X}|} \rightarrow R_2$ be two randomised algorithms that are $(\epsilon_1, \delta_1)$- and $(\epsilon_2, \delta_2)$ DP, then their composition defined by $\mathcal{M}_{12}: \mathbb{N}^{|\mathcal{X}|} \rightarrow R_1 \times R_2$, $\mathcal{M}_{12}(x)=(\mathcal{M}_{1}(x), \mathcal{M}_{2}(x))$ is $(\epsilon_1+\epsilon_2, \delta_1+\delta_2)$ DP.
\end{thm}
\begin{proof}
    TBA
\end{proof}

\subsection{Example of DP-mechanism: Gaussian Mechanism}

Let $f:\mathbb{N}^{|\mathcal{X}|} \longrightarrow \mathbb{R}^d$ an arbitrary function mapping to a $d$-dimensional real space. $f$ can represent numerous models, e. g. a neural network, an SVM-classifier etc. We have seen from theorem \ref*{thm:dp_random} that in order to ``privatise'' the output of $f$, we need to add randomness to its output. One way to achieve this is to add gaussian noise, which is calibrated to mask the influence of a specific input. Because differential privacy aims to hide the influence of the input to the output, a natural quantity to consider when calibrating the noise is to look at how much $f$ will change, when using different inputs. This leads the following definition:

\begin{definition}[$l_2$-sensitivity]
    Let $f:\mathbb{N}^{|\mathcal{X}|} \longrightarrow \mathbb{R}^d$ an arbitrary function, then its $l_2$-sensitivity is defined as:
    \begin{align}
        \Delta f = \max_{\substack{x,y \in \mathbb{N}^{|\mathcal{X}|} \\ ||x-y||_1\le 1}} ||f(x)-f(y)||_2 
    \end{align}
\end{definition}

Now we can calibrate the noise according to its sensitivity which we can prove to satisfy differential privacy:
\begin{definition}[Gaussian Mechanism]\label{def:gm} \label{def:gm}
    For a given function  $f:\mathbb{N}^{|\mathcal{X}|} \longrightarrow \mathbb{R}^d$, privacy parameters $\epsilon \in (0,1)$ and $\delta>0$ define the gaussian mechanism $F(x)$ as follows:
    \begin{align}
        F(x) = f(x) + \mathcal{N}(0, \sigma^2)
    \end{align}
    where the variance is calibrated by the sensitivity of $f$ and the given privacy level, s. t. $\sigma \ge \frac{2 \Delta f}{\epsilon}\ln(\frac{1.25}{\delta})$
\end{definition}

\begin{thm}[Gaussian Mechanism satisfies DP]
    The gaussian mechanism defined in definition \ref{def:gm} satisfies $(\epsilon, \delta)$-DP.
\end{thm}
The proof is rather lengthy and the curious reader is referred to read through \parencite[][Appendix A]{dwork2014algorithmic}