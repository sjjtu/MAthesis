\section{Discussion}

\subsection{(Privacy-Preserving) Data Generation}
From our experiments, we can clearly see that the AE-(dp)MERF generated samples achieved a much higher utility compared to the GAN-based approach. This confirms the result from \parencite{hu2023sok} stating DP-MERF is the ``best all purpose generator''. We found that AE-(dp)MERF also performs well on time series data, which has not been examined prior to this work to the best of our knowledge. Of course, further data and models need to be tested for verification. Adding privacy was straight forward to implement for AE-(dp)MERF and the training of the generator worked well in even lower $\epsilon$ regimes. This is not the case for the GAN-based approache: due to the inherent stability issues with training GANs, adding noise during the process imposes even more instabilities into the training process. Thus, training with even higher \(\epsilon\) values required a careful selection of hyperparameters.


\subsection{Utility-Privacy-Tradeoff}

Many studies suggest, that adding (differential) privacy to a model decreases its utility. Our result suggest, that this tradeoff is more nuanced and depends on the use case and the utility measure. The conducted experiments show, that for reasonable $\epsilon$ values and depending on the generator, utility of synthetic, privacy-preserving data and privacy can go hand in hand. We have observed only a slight decrease in utility when employing DP to our models. For AE-(dp)MERF using $\epsilon=$ values as low as 0.5 did not ``destroy'' utility. The GAN-based model behaved similarly, but with much higher \(\epsilon\) values ($\epsilon=25$). However, this only applies to anomaly detection and does not say something about the quality of the generated samples. On the contrary, we have seen that the generated samples appear much noisier when we add differential privacy to the generation process.

\subsection{Privacy and Robustness}
In the last experiment, we contaminated the private training set with anomalous samples. While the baseline modelled trained directly on this contaminated data set saw a steep decrease in performance, models trained with non-privacy-preserving, synthetic data generated by AE-MERF achieved much more stable results. This could be attributed to how the mean embeddings are computed (see \cref{eq:rff}): We hypothesise, that the inherent randomness from computing the random Fourier features has a regularising effect. This needs to be examined further.

Additionally, the DP generated samples based on contaminated training data not only exhibit the same robustness but even for low contamination levels even improved its utility. A similar result has been observed in \parencite{du2019robust}. A possible explanation lies in the nature of DP: by definition, DP bounds the influence one particular sample can have on the outcome. In particular, this can hide the presence of the small amount of anomalous heartbeat samples that were injected into the training data. This has to be verified rigorously.


\subsection{Future Works}

We have tested two different privacy-preserving algorithms for ECG time series data generation using th MITBIH data set. Although those algorithms fulfill differential privacy. This is a probabilistic framework for assessing privacy and needs to be tested empirically. One could follow the works of \colorbox{red}{ref}. 
Furthermore, as we have only used the models to generated ECG data for arrhythmia detection, our findings need to be confirmed on other common benchmarking data set, e.g. \colorbox{red}{tba.}.
Lastly, our experiments show an useful connection between (differential) privacy and robustness, which opens new lines of research for privacy-preserving and robust data generation algorithms.

