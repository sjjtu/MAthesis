\section{Introduction}

\subsection{Motivation}

Data-driven technology and especially machine learning have gained a lot of momentum the past years. Models like ChatGPT or BERT heavily depend on large datasets that are available publicly. At the same time machine learning models are now being considered in other data sensitive domains like health care \parencite[see][]{ai_and_med,aimed2,aimed3,aimed4}. One exciting field within health care is arrhythmia detection for heartbeats, where machine learning methods can aid physicians to detect irregular heartbeat conditions. Recently, several methods have been proposed, ranging from SVMs to neural networks \parencite[see review][]{arr_rev}.

\paragraph{}
When working with those sensitive data, privacy plays a major role in general acceptance of those models. Now governmental institutions like the European Union have established a right to privacy manifested in the General Data Protection Regulation laws \footnote[1]{see https://gdpr-info.eu/}. Previous simple anonymisation attempts that simply removed some identifying attributes (e. g. name, birthday etc.) have been proven to be ineffective. For example, user profiles from the anonymised dataset used in the infamous Netflix prize have been reconstructed with the help of publicly available data from IMDB \cite{4531148}. This is why technological advances in the area of privacy-preserving machine learning have increased in the past few years, with the development of various machine learning models that aim to preserve the privacy of individual data records. Protecting privacy becomes crucial for heartbeat data because it can be used to identify patients, thus heavily impacting the patient's privacy \parencite[see heartbeat biometrics][]{heartb_auth,hegde2011heartbeat}.

\paragraph{}
One promising solution \parencite[see][]{jordon2022synthetic} is to replace the original, possibly sensitive data set with a synthetic data set that resembles the original raw data in some statistical properties. Much research has been done to generate tabular or image data ???REF, whereas dedicated time series data generation is still a \textit{"burgeoning"} area of research according to a recent benchmark \parencite[][]{ang2023tsgbench}. Regardless of the data type, data generators with no formal privacy guarantees have been shown to still be susceptible to privacy leaks~\cite{stadler2022synthetic}. 

To improve privacy, this thesis aims to analyse the combination of synthetic data with tools from so-called differential privacy. Differential privacy has been developed by Dwork et al \cite{10.1145/1866739.1866758} and is widely considered as the mathematical framework to rigorously provide privacy guarantees to privacy-preserving algorithms, relying on applied probability theory and statistics. This thesis will study existing architectures based on private generative AI models, as well as explore the possibility of new solution. Experiments were conducted to assess the performance of these models using the MITBIH dataset on heartbeat arrhythmia \parencite[][]{moody2001impact}. Unfortunately, there is no free lunch and privacy always comes with a decrease in utility \cite{stadler2022synthetic}. A careful balance between privacy and utility needs to be established.

\subsection{Problem Definition}

This thesis aims to examine how to generate private time series data for heartbeat arrhythmia detection. Let $\mathcal{S}=\{s_i\}_{i=1}^N$ denote a set of heartbeat samples, where $s_i=(s_i^0,..., s_i^L)$ is a sequence of one-dimensional ECG measurements of fixed length $L$ corresponding to one heartbeat. Each heartbeat sequence is associated with a corresponding label denoting whether it is a normal or anomalous heartbeat according to ???. Therefore we separate the set into normal heartbeats $\mathcal{N}$ and $\mathcal{A}$ (i. e. $\mathcal{S} = \mathcal{N} \sqcup \mathcal{A}$)

\paragraph{}

Firstly, we want to design a time series generator (TSG) that can model the true probability distribution $p(\mathcal{N})$ of the normal heartbeats. Here, we only consider normal heartbeats since for the subsequent task of arrhythmia detection we will follow an anomaly detection approach explained next. The aim of the TSG is to generate a synthetic data set $\widehat{\mathcal{N}}$ with distribution $p(\widehat{\mathcal{N}})$ that is "close" to the original data $p(\mathcal{N})$.

Secondly, the utility of the generated data is assessed in the downstream task of detecting anomalous heartbeats (heartbeat arrhythmia detection). We treat this task as an anomaly detection task based on reconstruction error ??REF, i. e. we want to train a model only on normal heartbeats that can reconstruct those samples with low error, but give high reconstruction error when inputting an anomalous sample. Alternatively, one could treat this as a binary classification task, that classifies a given heartbeat sample as either normal or anomalous. Since the ratio of those two classes are heavily imbalanced due to the nature of arrhythmias, we will favor the first approach ??REF.

Lastly, we will embed the generation procedure in a differential privacy setting. This will provide a theoretical framework to assess privacy.

\subsection{Related Works and State of the Art}
TBA
\begin{itemize}
    \item sota privacy in ml 
    \item why DP
    \item private data generation
    \item heartbeat data generation
\end{itemize}
\subsubsection*{Privacy in machine learning}
Several notions of privacy have been proposed in the last decade, among which Differential Privacy (DP) has emerged as the \textit{``de-facto standard in datra privacy''} \parencite{kim2021survey}. Reasons for its popularity according to a recent survey \parencite{surv_dp2021} are among others:

\begin{enumerate}
    \item DP is future-proof and requires no extra knowledge about the adversary.
    \item DP provides rigorous privacy guarantees.
    \item DP provides a notion of privacy budget, which can be adapted to the specific use case to balance privacy and utility.
\end{enumerate}

We will revisit the definition and most important results in Chapter \ref{ch2} of this thesis. The basic idea is to add calibrated, random noise either the data or during model training.

Recently, a lot of popular neural network architectures have been ``privatised'' by adding DP noise, most notably there is a differential private version of stochastic gradient descent (SGD) called DP-SGD developed by Abadi et al \parencite{Abadi_2016} in 2016.

