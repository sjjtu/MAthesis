\section{Introduction}

\subsection{Motivation}

Data-driven technology and especially machine learning have gained a lot of momentum the past years. Models like ChatGPT or BERT heavily depend on large datasets that are available publicly. At the same time machine learning models are now being considered in other data sensitive domains like health care \parencite[see][]{ai_and_med,aimed2,aimed3,aimed4}. One exciting field within health care is arrhythmia detection for heartbeats, where machine learning methods can aid physicians to detect irregular heartbeat conditions. Recently, several methods have been proposed, ranging from SVMs to neural networks \parencite[see review][]{arr_rev}.

\paragraph{}
When working with those sensitive data, privacy plays a major role in general acceptance of those models. In some circumstances neural networks can memorise specific data samples, which constitutes a heavy privacy breach \Parencite[see][]{feldman2021does}. For example in \Parencite{carlini19}, Carlini et al.\ recovered credit card numbers from text completion models used by Google. Now governmental institutions like the European Union have established a right to privacy manifested in the General Data Protection Regulation laws\footnote[1]{see https://gdpr-info.eu/}. Previous simple anonymisation attempts that simply removed some identifying attributes (e.\ g.\ name, birthday etc.) have been proven to be ineffective. For example, user profiles from the anonymised dataset used in the infamous Netflix prize have been reconstructed with the help of publicly available data from IMDB~\cite{4531148}. This is why technological advances in the area of privacy-preserving machine learning have increased in the past few years, with the development of various machine learning models that aim to preserve the privacy of individual data records. Protecting privacy becomes crucial for heartbeat data because it can be used to identify patients, thus heavily impacting the patient's privacy \parencite[see heartbeat biometrics][]{heartb_auth,hegde2011heartbeat}.

\paragraph{}
One promising solution \parencite[see][]{jordon2022synthetic} is to replace the original, possibly sensitive data set with a synthetic data set that resembles the original raw data in some statistical properties. Much research has been done to generate tabular or image data, whereas dedicated time series data generation is still a \textit{``burgeoning''} area of research according to a recent benchmark \parencite[][]{ang2023tsgbench}. Regardless of the data type, data generators with no formal privacy guarantees have been shown to still be susceptible to privacy leaks~\cite{stadler2022synthetic}. 

To improve privacy, this thesis aims to analyse the combination of synthetic data with tools from so-called differential privacy. Differential privacy has been developed by Dwork et al. \parencite{dwork2006differential} and is widely considered as the mathematical framework to rigorously provide privacy guarantees to privacy-preserving algorithms, relying on applied probability theory and statistics. This thesis will study existing architectures based on private generative AI models, as well as explore the possibility of new solutions. Experiments were conducted to assess the performance of these models using the MITBIH dataset on heartbeat arrhythmia \parencite[][]{moody2001impact}. Unfortunately, there is no free lunch and privacy always comes with a decrease in utility~\cite{stadler2022synthetic}. A careful balance between privacy and utility needs to be established. However, we will challenge this trade-off and show that privacy and utility in the use case of anomaly detection can go hand in hand, because it can add some robustness to the model. This was first explored in \Parencite{du2019robust} for detecting anomalies.


\subsection{Problem Definition}

This thesis aims to examine how to generate privacy-preserving time series data for heartbeat arrhythmia detection. Let $\mathcal{S}={\{s_i\}}_{i=1}^N$ denote a set of heartbeat samples, where $s_i=(s_i^0,..., s_i^L)$ is a sequence of one-dimensional ECG measurements of fixed length $L$ corresponding to one heartbeat. Each heartbeat sequence is associated with a corresponding label denoting whether it is a normal or anomalous heartbeat according. Therefore we separate the set into normal heartbeats $\mathcal{N}$ and $\mathcal{A}$ (i.\ e.\ $\mathcal{S} = \mathcal{N} \sqcup \mathcal{A}$)

\paragraph{}

Firstly, we want to design a time series generator (TSG) that can model the true probability distribution $p(\mathcal{N})$ of the normal heartbeats. Here, we only consider normal heartbeats since for the subsequent task of arrhythmia detection we will follow an anomaly detection approach explained next. The aim of the TSG is to generate a synthetic data set $\widehat{\mathcal{N}}$ with distribution $p(\widehat{\mathcal{N}})$ that is ``close'' to the original data $p(\mathcal{N})$.

Secondly, the utility of the generated data is assessed in the downstream task of detecting anomalous heartbeats (heartbeat arrhythmia detection). We treat this task as an anomaly detection task based on reconstruction error \parencite[Section 3.2]{anom_rev}, i.\ e.\ we want to train a model only on normal heartbeats that can reconstruct those samples with low error, but give high reconstruction error when inputting an anomalous sample. Alternatively, one could treat this as a binary classification task, that classifies a given heartbeat sample as either normal or anomalous. Since the ratio of those two classes are heavily imbalanced due to the nature of arrhythmias, we will favour the first approach.

Lastly, we will embed the generation procedure in a differential privacy setting. This will provide a theoretical framework to assess privacy.

\subsection{Related Works and State of the Art}

\subsubsection*{Privacy in machine learning}
A lot of past efforts have been put into improving the performance of machine learning methods, where the privacy aspect has been neglected. Due to the increased awareness about private individual data and policies like EU's GDPR laws, big tech companies like Apple, Google and even the US Census have been implementing privacy measurements in their data collection \parencite[see][]{dwork2019differential,abowd2019census}. One of the first groundbreaking works on actually quantifying the privacy leakage in machine learning models has been studied in \parencite{shokri2017membership}, where Shokri et.\ al.\ have designed a framework to perform membership inference attacks (MIA) on basic classification tasks. MIA on machine learning models try to infer whether a certain record has been used when training the respective model. This becomes a privacy issue when e.\ g.\ an adversary can infer whether a certain patient's data was used to train a model associated with a disease. Then the adversary can conclude that this particular patient likely has this disease \parencite[cf.][p. 5]{shokri2017membership}. Hence, their results indicate a strong vulnerability in terms of privacy for data-based models.


Several notions of privacy have been proposed in the last decade, among which Differential Privacy (DP) has emerged as the \textit{``de-facto standard in data privacy''} \parencite{kim2021survey}. Reasons for its popularity according to a recent survey \parencite{surv_dp2021} are among others:

\begin{enumerate}
    \item DP is future-proof and requires no extra knowledge about the adversary.
    \item DP provides rigorous privacy guarantees.
    \item DP provides a notion of privacy budget, which can be adapted to the specific use case to balance privacy and utility.
\end{enumerate}

We will revisit the definition and most important results in \cref{ch2} of this thesis. The basic idea is to add calibrated, random noise either to the data, during model training or to the output. Broadly speaking, differential private noise can be injected in three different stages of the modelling pipeline: input, hidden or output layer \parencite[cf.][]{zhao2019differential}. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Different stages to add DP noise according to \parencite{wang2023differential}}
    \label{fig:enter-label}
\end{figure} 

Applying some DP mechanism at the input stage can be seen as a preprocessing step to either hide sensitive attributes in the data or generating new synthetic dataset. Some earlier works include random perturbation methods described for instance in \parencite{input_levelDP,erm_dp_input}. According to \parencite{wang2023differential} this approach is not utilised frequently because extra prior knowledge about the subsequent task is required to calibrate the right amount of noise. More recent methods focus exclusively on generating data samples by deep learning methods, that in turn employ a DP mechanism at gradient or output level. 

Adding privacy in the hidden layer is sometimes referred to as gradient-level DP. Due to the iterative nature of most training algorithms, extra care needs to be taken to track the privacy loss caused by each iteration. Most notably there is a differential private version of stochastic gradient descent (SGD) called DP-SGD developed by Abadi et al \parencite{Abadi_2016} where the authors have designed a mechanism to track the privacy loss incurred while training. Differential privacy is achieved by clipping the gradient and then adding gaussian noise to the gradient. The clipping step is necessary to ensure that the gradient is bounded. Based on a more relaxed definition of DP, the authors in \parencite{bu2020deep} propose an improved version of DP-SGD called NoisySGD. 

At the output level there are several ways to implement DP. One highly cited approach called the ``Functional Mechanism'' followed by the authors in \parencite{zhang2012functional} perturbs the objective function, so it is independent of the number of training steps. A further refinement of this approach was researched in \parencite{adlm2017}, where Phan et al. developed an algorithm that puts adaptive noise on the features based on its contribution to the output.

Other interesting approaches to incorporating differential privacy into deep learning include the PATE learning method by Papernot et al. \parencite{papernot2017semisupervised}. The idea behind this model is to train ``teacher models'' that will not be published, which in turn are used in a random manner to then train privacy-preserving ``student'' models. 

In a distributed setting, approaches like federated learning \parencite{konečný2015federated,Mo2019EfficientAP} have been proposed. Their privacy further analysed in \parencite{mcmahan2018learning} for learning language models.

\vspace*{1em}
For a more in-depth review see e. g. \parencite{surrve_ppml,surv_ppml_2,wang2023differential}


\subsubsection*{Data generation and Privacy}

As we have mentioned earlier, ensuring privacy in machine learning applications is crucial when working with sensitive data. One might naively assume, that synthetic data without any formal privacy guarantees provides enough privacy already by design, but this unfortunately is not the case. Especially when generating with GAN-based networks, recent works have shown that although under some circumstances GANs can satisfy a weak notion of DP, but with a very high $\epsilon$ which corresponds to a very weak privacy guarantee \parencite{lin2021privacy,stadler2022synthetic,jordon2022synthetic}. Combining generative algorithms with DP however is a promising solution to mitigate the privacy issue \parencite{bellovin2019privacy} which will be the focus of this thesis.

While we have outlined several techniques from the state of the art to ensure privacy, most of the methods are tailored for a specific model architecture or use case. On the other hand, synthetic data that has been generated with privacy guarantees can be used in any downstream task without privacy breach. To this end, several deep learning based architectures have been proposed. Following \parencite{hu2023sok}, one can broadly categorise them as follows:
\begin{itemize}
    \item GAN-based
    \item Feature-based
    \item Autoencoder based {\tiny \Parencite[see e. g.][for a generator based on a variational autoencoder that is trained with DP-SGD]{vae}}
    \item Optimal transport based {\tiny \Parencite[see e. g.][for generator based on the so-called Sinkhorn divergence]{cao2021dont}}
    \item Stochastic simulation based {\tiny \Parencite[see e. g.][for a differentially-private diffusion model]{dpgen}}
\end{itemize}

We will present the first two approaches in more detail in \cref{chapter3}.

\subsubsection*{Heartbeat arrhythmia detection}
For the experiments conducted in this thesis we will use the common benchmark data set for heartbeat arrhythmia MIT-BIH \parencite{moody2001impact}. It contains one-dimensional ECG measurements of 47 patients each lasting about 30 minutes. This kind of data is commonly referred to as time series data which due to its time dependency needs to be treated differently than tabular data. Each heartbeat is labelled as one of 16 heartbeat classes by experts. We will follow the Association for the Advancement of Medical Instrumentation's (AAMI) standard to divide those classes into normal and anomalous heartbeats \Parencite{aami}. Finding anomalous heartbeats, i. e. arrhythmia detection, can be linked to several common tasks found in machine learning. For example, one can view this problem as a binary classification problem, where one wishes to train a classifier, that given a heartbeat will classify this as either normal or anomalous. Since this requires a balanced dataset, we follow a different approach from anomaly detection. In particular, we will train a baseline model for arrhythmia detection based on so-called the reconstruction error \parencite[see][for an in-depth survey on anomaly detection with times series]{schmidl2022anomaly}. This is a semi-supervised approach where a model is only trained on normal data. The model learns to encode and reconstruct normal data from a (lower-dimensional) latent space with low reconstruction error, whereas it will reconstruct anomalous data with a higher reconstruction error. This method will also be more useful in real-life applications since 1) heartbeat arrhythmias are rather scarce and 2) there is a lot of heartbeat data being collected but not labelled.

Some efforts have been taken to generate ECG data. Most of the recent approaches deploy a GAN based model to generate heartbeat data \Parencite[see e. g.][]{zhu2019electrocardiogram,Delaney2019SynthesisOR,wang2020accurate} getting favourable results. We will also follow a GAN based approach to generate synthetic ECG data. A very recent paper achieved even better result using a transformer architecture \Parencite[see][]{Kaleli2023GenerationOS}.  

\subsection{Note on terminology}
To avoid confusion about the different kinds of data sets used in this thesis, we establish some convention that will be followed throughout this work:

\paragraph{}
We refer to data set as being private, if this data set's privacy should be protected and therefore never be shared with the public. In contrast to that, a we refer to a data set as being public, if we either do not aim to protect its privacy or this data set was generated with some rigorous privacy-preserving mechanisms.

\paragraph{}
Data that is coming from a real-life patients will be called the original data set. Data that is generated by some data-generating procedure is called synthetic data.

\paragraph{}
In the course of this thesis, we will look at different heartbeat samples that represent different heartbeat conditions. The heartbeat samples are classified according to those conditions. Following a guideline set by the AAMI we will simply label heartbeats as regular if they do not indicate any heartbeat disease or a non-symptomatic disease. Otherwise, those heartbeats will by labelled as anomalous heartbeats.