\section{Introduction}

\subsection{Motivation}
Over the past years, data-driven technologies and especially machine learning have rapidly gained a lot of momentum and recognition. Prominent examples include language models like ChatGPT\footnote{https://openai.com/chatgpt} or classification of handwritten digits\footnote{http://yann.lecun.com/exdb/mnist/}. These models are heavily dependent on training data, since they are trained on data to recognise certain patterns. Using the example of recognising handwritten digits, one needs to have a large amount of handwritten digits and their corrsponding labels denoting the desired result. A machine learning model is then trained on this data to recognise certain patterns and features about the digits, in order to classify new, unseen examples.


At the same time machine learning models are now being considered in other privacy-sensitive domains like health care \parencite[see][]{ai_and_med,aimed2,aimed3,aimed4}. One important field within health care is arrhythmia detection for heartbeats, where machine learning methods can aid physicians to detect irregular heartbeat conditions. Recently, several methods have been proposed, ranging from support vector machines to neural networks \parencite[see review][]{arr_rev}.

\paragraph{}
When working with sensitive data, privacy plays a major role in general acceptance of those models. In some circumstances neural networks can memorise specific data samples, which constitutes a heavy privacy breach \Parencite[see][]{feldman2021does}. For example in \Parencite{carlini19}, Carlini et al.\ recovered credit card numbers from text completion models used by Google. Now governmental institutions like the European Union have established a right to privacy manifested in the General Data Protection Regulation laws\footnote[1]{see https://gdpr-info.eu/}. Previous simple anonymisation attempts that simply removed some identifying attributes (e.\ g.\ name, birthday etc.) have been proven to be ineffective. For example, user profiles from the anonymised dataset used in the infamous Netflix prize have been reconstructed with the help of publicly available data from IMDB~\cite{4531148}. This is why technological advances in the area of privacy-preserving machine learning have increased in the past few years, with the development of various machine learning models that aim to preserve the privacy of individual data records. Protecting privacy becomes crucial for e.g. heartbeat data because it can be used to identify patients, thus heavily impacting the patient's privacy \parencite[see heartbeat biometrics][]{heartb_auth,hegde2011heartbeat}.

\paragraph{}
One promising solution \parencite[see][]{jordon2022synthetic} is to replace the original, possibly sensitive data set with a synthetic data set that resembles the original raw data in some statistical properties. Much research has been done to generate tabular or image data, whereas dedicated time series data generation is still a \textit{``burgeoning''} area of research according to a recent benchmark \parencite[][]{ang2023tsgbench}. Regardless of the data type, data generators with no formal privacy guarantees have been shown to still be susceptible to privacy leaks~\cite{stadler2022synthetic}. 

To improve privacy, this thesis aims to analyse the combination of synthetic data generation with tools from so-called differential privacy. Differential privacy has been developed by Dwork et al. \parencite{dwork2006differential} and is widely considered as the mathematical framework to rigorously provide privacy guarantees to privacy-preserving algorithms, relying on applied probability theory and statistics. This thesis will study existing architectures based on private generative AI models, as well as explore the possibility of new solutions. Experiments were conducted to assess the performance of these models using the MITBIH dataset on heartbeat arrhythmia \parencite[][]{moody2001impact}. Unfortunately, there is no free lunch and privacy always comes with a decrease in utility~\cite{stadler2022synthetic}. A careful balance between privacy and utility needs to be established. However, we will challenge this trade-off and show that privacy and utility in the use case of anomaly detection can go hand in hand, because it can add some robustness to the model. This was first explored in \Parencite{du2019robust} for detecting anomalies.


\subsection{Problem Definition}

This thesis aims to examine how to generate privacy-preserving time series data for heartbeat arrhythmia detection. Let $\mathcal{S}={\{s_i\}}_{i=1}^N$ denote a set of heartbeat samples, where $s_i=(s_i^0,..., s_i^L)$ is a sequence of one-dimensional ECG measurements of fixed length $L$ corresponding to one heartbeat. Each heartbeat sequence is associated with a corresponding label denoting whether it is a normal or anomalous heartbeat respectively. The utility of a data set in this case for arrhythmia detection is defined to be the performance of an arrhythmia detection model trained on this data set. Therefore we will use several metrics to measure this performance.

Firstly, we train a baseline model \(\mathcal{K}^\mathcal{S}\) on the private data to detect arrhythmias. This model's performance will be used as the baseline to which we compare all other models. As we have seen in the motivation, this model can potentially reveal information about the underlying private training data $\mathcal{S}$.

This is why in the second step, we test different time series generators $\mathcal{G}$, that are trained on the private data $\mathcal{S}$ to generate a synthetic data set $\widehat{\mathcal{S}}$ that is ``close'' to the original data $\mathcal{S}$. Then we assess the utility of the generated data in the downstream task of detecting anomalous heartbeats (heartbeat arrhythmia detection). 

Lastly, as we have seen from previous works that synthetic data itself cannot sufficiently preserve privacy, we will embed the generation procedure in a differential privacy setting. This will provide a theoretical framework to assess privacy.

\subsection{Related Works and State of the Art}

\subsubsection*{Privacy in machine learning}
Most work on machine learning focusses on improving the performance of machine learning methods and has neglected the privacy aspect. Due to the increased awareness about private individual data and policies like EU's GDPR laws, big tech companies like Apple, Google and even the US Census have been implementing privacy measurements in their data collection \parencite[see][]{dwork2019differential,abowd2019census}. One of the first groundbreaking works on actually quantifying the privacy leakage in machine learning models has been studied in \parencite{shokri2017membership}, where Shokri et.\ al.\ have designed a framework to perform membership inference attacks (MIA) on basic classification tasks. MIA on machine learning models try to infer whether a certain record has been used when training the respective model. This becomes a privacy issue when e.g.\ an adversary can infer whether a certain patient's data was used to train a model associated with a disease. Then the adversary can conclude that this particular patient likely has this disease \parencite[cf.][p. 5]{shokri2017membership}. Hence, their results indicate a strong vulnerability in terms of privacy for data based models.


Several notions of privacy have been proposed in the last decade, among which Differential Privacy (DP) has emerged as the \textit{``de-facto standard in data privacy''} \parencite{kim2021survey}. Reasons for its popularity according to a recent survey \parencite{surv_dp2021} are among others:

\begin{enumerate}
    \item DP is future-proof and requires no extra knowledge about the adversary.
    \item DP provides rigorous privacy guarantees.
    \item DP provides a notion of privacy budget, which can be adapted to the specific use case to balance privacy and utility.
\end{enumerate}

We will visit the definition and most important results in \cref{ch2} of this thesis. The basic idea is to add calibrated, random noise either to the data, during model training or to the output. Broadly speaking, differential private noise can be injected in three different stages of the modelling pipeline: input, hidden or output layer \parencite[cf.][]{zhao2019differential}. 

Applying some DP mechanism at the input stage can be seen as a preprocessing step to either hide sensitive attributes in the data or generating new synthetic datasets. Some earlier works include random perturbation methods described for instance in \parencite{input_levelDP,erm_dp_input}. According to \parencite{wang2023differential} this approach is not utilised frequently because extra prior knowledge about the subsequent task is required to calibrate the right amount of noise. More recent methods focus exclusively on generating data samples by deep learning methods, that in turn employ a DP mechanism at gradient or output level. 

Adding privacy in the hidden layer is sometimes referred to as gradient-level DP. Due to the iterative nature of most training algorithms, extra care needs to be taken to track the privacy loss caused by each iteration. Most notably there is a differential private version of stochastic gradient descent (SGD) called DP-SGD developed by Abadi et al \parencite{Abadi_2016} where the authors have designed a mechanism to track the privacy loss incurred while training. Differential privacy is achieved by clipping the gradient and then adding gaussian noise to the gradient. The clipping step is necessary to ensure that the gradient is bounded. Based on a more relaxed definition of DP, the authors in \parencite{bu2020deep} propose an improved version of DP-SGD called NoisySGD. 

At the output level there are several ways to implement DP. One highly cited approach called the ``Functional Mechanism'' followed by the authors in \parencite{zhang2012functional} perturbs the objective function, so it is independent of the number of training steps. A further refinement of this approach was researched in \parencite{adlm2017}, where Phan et al. developed an algorithm that puts adaptive noise on the features based on its contribution to the output.

Other interesting approaches to incorporating differential privacy into deep learning include the PATE learning method by Papernot et al. \parencite{papernot2017semisupervised}. The idea behind this model is to train ``teacher models'' that will not be published, which in turn are used in a random manner to then train privacy-preserving ``student'' models. 

In a distributed setting, approaches like federated learning \parencite{konečný2015federated,Mo2019EfficientAP} have been proposed. Their privacy further analysed in \parencite{mcmahan2018learning} for learning language models.

\vspace*{1em}
For a more in-depth review see e.g.\parencite{surrve_ppml,surv_ppml_2,wang2023differential}


\subsubsection*{Data Generation and Privacy}

As we have mentioned earlier, ensuring privacy in machine learning applications is crucial when working with sensitive data. One might naively assume, that synthetic data without any formal privacy guarantees provides enough privacy already by design, but this unfortunately is not the case. Especially when generating with GAN-based networks, recent works have shown that although under some circumstances GANs can satisfy a weak notion of DP, but with a very high $\epsilon$ which corresponds to a very weak privacy guarantee \parencite{lin2021privacy,stadler2022synthetic,jordon2022synthetic}. Combining generative algorithms with DP however is a promising solution to mitigate the privacy issue \parencite{bellovin2019privacy} which will be the focus of this thesis.

While we have outlined several techniques from the state of the art to ensure privacy, most of the methods are tailored for a specific model architecture or use case. On the other hand, synthetic data that has been generated with privacy guarantees can be used in any downstream task without privacy breach. To this end, several deep learning based architectures have been proposed. Following \parencite{hu2023sok}, one can broadly categorise them as follows:
\begin{itemize}
    \item GAN-based
    \item Feature-based
    \item Autoencoder based {\tiny \Parencite[see e.g.][for a generator based on a variational autoencoder that is trained with DP-SGD]{vae}}
    \item Optimal transport based {\tiny \Parencite[see e.g.][for generator based on the so-called Sinkhorn divergence]{cao2021dont}}
    \item Stochastic simulation based {\tiny \Parencite[see e.g.][for a differentially-private diffusion model]{dpgen}}
\end{itemize}

We will present the first two approaches in more detail in \cref{chapter3}.

\phantom{asd}

Furthermore, some efforts have been taken to generate ECG data. Most of the recent approaches deploy a GAN based model to generate heartbeat data \Parencite[see e. g.][]{zhu2019electrocardiogram,Delaney2019SynthesisOR,wang2020accurate} obtaining favourable results. We will also follow a GAN based approach to generate synthetic ECG data. A very recent paper achieved even better results using a transformer architecture \Parencite[see][]{Kaleli2023GenerationOS}.  

\subsubsection*{Heartbeat Arrhythmia Detection}
Heartbeat arrhythmia is a medical condition of the heart that results in inconsistent heart beat patterns, too fast or too slow heartbeats. This disease can be diagnosed by experts using ECG measurements that measure the heart's activity. Several different machine learning models have been proposed to detect heartbeat arrhythmias. One can broadly classify them into classification based and anomaly detection based approaches. Classifiaction models include e.g. support vector machine \parencite{svm_ecg}, linear discriminat analysis \parencite{Chazal20041196}, neural networks \parencite{ann_ecg} etc. Those models give good performance (accuracy above 90\%) but rely on heavy preprocessing and feature engineering that might require some expert knowledge about ECG data. Anomaly detection based models usually are trained to approximate the probability distribution of regular heartbeats and are thus able to detect if a given heartbeat is sampled from the distribution or not. Examples for those models include Long-Short-Term-Memory networks \parencite{lstm_ecg}, Variational Autoencoders \parencite{matias2021robust}, Autoencoders \parencite{anobeat}. The advantage of the latter approach is twofold: (1) They work in a semi-supervised approach, i.e. they are only trained on regular data samples and (2) there is no need for complicated preprocessing or feature engineering steps.



\subsection{Note on Terminology}
To avoid confusion about the different kinds of data sets used in this thesis, we establish some convention that will be followed throughout this work:

\paragraph{}
We refer to data set as being private, if this data set's privacy should be protected and therefore never be shared with the public. In contrast to that, a we refer to a data set as being public, if we either do not aim to protect its privacy or this data set was generated with some rigorous privacy-preserving mechanisms.

\paragraph{}
Data that is coming from a real-life patients will be called the original data set. Data that is generated by some data-generating procedure is called synthetic data.

\paragraph{}
In the course of this thesis, we will look at different heartbeat samples that represent different heartbeat conditions. The heartbeat samples are classified according to those conditions. Following a guideline set by the AAMI we will simply label heartbeats as regular if they do not indicate any heartbeat disease or a non-symptomatic disease. Otherwise, those heartbeats will by labelled as anomalous heartbeats.