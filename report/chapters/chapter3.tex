\section[(Time Series) Data generation]{(Time Series) Data generation} \label{chapter3}
\subsection{Overview}
    \begin{itemize}
        \item Data generation in general
        \item what is special about time series
        \item what about Privacy
        \item choice of models
    \end{itemize}
\subsection{DP-MERF}

DP-MERF \parencite{dpmerf} is an efficient all purpose data generation algorithm that is based on minimising the so-called Maximum Mean Discrepancy between the real and the synthetic data distributions. The authors mainly verfied their results using tabular data like ????, but also image data, notably the MNIST ???CITE data set. It has not been used for time series data, but we will consider this data generation for generating time series data in this thesis, because accorrding to a recent survey \parencite{hu2023sok}, DP-MERF delivers the best all purpose data generation performance.

\subsubsection{Maximum Mean Discrepancy}
There are different ways to measure the "distance" between two distributions $P$ and $Q$. On popular metric is the Maximum Mean Discrepancy (MMD) between $P$ and $Q$, where the random variables are projected into another feature space and the expected values are compared to each other in this space.

\begin{definition}[MMD]
    Let $\phi: \mathcal{X} \rightarrow \mathcal{H}$, where $\mathcal{H}$ is a reproducing kernel hilbert space (RKHS) and $P$ and $Q$ some distributions over $\mathcal{X}$ and random variables $X \sim P$, $Y \sim Q$ given. Then the Maximum mean Discrepancy is defines as:
    \begin{align}
        MMD(P,Q)=|| \mathbb{E}[\phi(X)] - \mathbb{E}[\phi(Y)] ||_\mathcal{H}
    \end{align}
\end{definition}

Some "easy" features maps $\phi$ are for example:
\begin{ex}
    Let $P$ and $Q$ some distributions over $\mathcal{X}$ and random variables $X \sim P$, $Y \sim Q$ given.
    \begin{itemize}
        \item \textbf{Identity kernel}: $\mathcal{X}=\mathcal{H}=\mathbb{R}^d$ and $\phi(x)=x$, then we have:
        \begin{align}
            MMD(P,Q) &= || \mathbb{E}[\phi(X)] - \mathbb{E}[\phi(Y)] ||_\mathcal{H} \nonumber \\
            &= || \mathbb{E}[X] - \mathbb{E}[Y] ||_{\mathbb{R}^d}
        \end{align}
        So we only compare the two distributions in terms of their means. 

        \item \textbf{Quadratic kernel}: $\mathcal{X}=\mathbb{R}$ $\mathcal{H}=\mathbb{R}^2$ and $\phi(x)=(x, x^2)$, then we have:
        \begin{align}
            MMD(P,Q) &= || \mathbb{E}[\phi(X)] - \mathbb{E}[\phi(Y)] ||_\mathcal{H} \nonumber \\
            &= || \mathbb{E}[(X, X^2)] - \mathbb{E}[(Y, Y^2)] ||_\mathcal{H} \nonumber \\
            &= || \begin{pmatrix}
                \mathbb{E}[X] \\ \mathbb{E}[X^2]
            \end{pmatrix} - \begin{pmatrix}
                \mathbb{E}[Y] \\ \mathbb{E}[Y^2]
            \end{pmatrix} ||_{\mathbb{R}^2} \nonumber \\
            &= \sqrt{(\mathbb{E}[X] - \mathbb{E}[Y])^2 - (\mathbb{E}[X^2] - \mathbb{E}[Y^2])^2}
        \end{align}
        So here we compare the two distributions in terms of their means and their variance (or first and second moments respectively).
        \item \textbf{Gaussian kernel} ????
    \end{itemize}
\end{ex}

Now instead of computing a possibly high or even infinite dimensional transformation $\phi$ one can use the well-known kernel trick ????REF. Let $k(x,y)=<\phi(x), \phi(y)>_{\mathcal{H}}$ be a kernel with corresponding reproducing kernel hilbert space $\mathcal{H}$, then the computation of the MMD simplifies to:

\begin{align}
    MMD^2(P,Q) &= || \mathbb{E}[\phi(X)] - \mathbb{E}[\phi(Y)] ||^2_\mathcal{H} \nonumber \\
    &= <\mathbb{E}[\phi(X)], \mathbb{E}[\phi(X')]> - <\mathbb{E}[\phi(X)], \mathbb{E}[\phi(Y)]> - <\mathbb{E}[\phi(Y)], \mathbb{E}[\phi(X)]> \nonumber \\ &\phantom{mmmmmmmmmmmmmmmmmmmm}+ <\mathbb{E}[\phi(Y)], \mathbb{E}[\phi(Y')]> \nonumber \\
    &= \mathbb{E}[<\phi(X), \phi(X')>] - 2 \mathbb{E}[<\phi(X), \phi(Y)>] + \mathbb{E}[<\phi(Y), \phi(Y')>] \nonumber \\
    &= \mathbb{E}[k(X,X')] - 2 \mathbb{E}[k(X,Y)] + \mathbb{E}[k(Y,Y')]
\end{align}

Where we introduced independent random variables $X,X' \sim P$, $Y,Y' \sim Q$.

Now given a training data set $X_m = \{x_i\}_{i=1}^m \sim P$ and a synthetic data set $X'_m = \{x_i\}_{i=1}^m \sim Q$ we can estimate their $MMD^2$ by estimating the expected value with a mean estimate:

\begin{align}
    \widehat{MMD}^2(X_m, X'_m) = \frac{1}{m^2} \sum_{i,j=1}^m k(x_i,x_j) + \frac{1}{m^2} \sum_{i,j=1}^m k(x'_i,x'_j) - \frac{2}{m^2} \sum_{i,j=1}^m k(x_i,x'_j)
\end{align}
Unfortunately, this will require $\mathcal{O}(m^2)$ computations which grows quadratically in the number of samples. This will be too big for a large training data set. As a remedy, the authors of \parencite{dpmerf} propose to use Random Fourier Features based on a paper from 2007 \parencite[see][]{rff}, to approximate the kernel $k$ using its fourier transform and Monte-Carlo-Simulation. Thus,

\begin{align}
    k(x,y) \approx \hat{\Phi}(x)^T \hat{\Phi}(x')
\end{align}
where $\hat{\Phi}(x) \in \mathbb{R}^D$ and $\hat{\Phi}_j(x) = \sqrt{\frac{2}{D}} cos (\omega_j^T x)$.

\subsection{GAN based} 