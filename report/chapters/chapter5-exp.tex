\section{Experiment}
In this chapter we describe the experiments conducted on heartbeat data taken from the MIT-BIH Arrhythmia data set. Afterwards we present the results and draw insights. The aim is to assess the utility of synthetic data generated by the two algorithms mentioned in \cref{chapter4}. The utility is measured in the downstream task of arrhythmia detection. We are testing differentially private and non-private versions of the models and compare them to a baseline model that is trained on the original data.

\subsection{Data Set and Experiment setup}
The MIT-BIH is a commonly used benchmark data set for arrhythmia detection. It contains two channel ECG data collected in the 1980s from some 47 patients by the Arrhythmia Laboratory of Boston's Beth Israel Hospital (BIH; now the Beth Israel Deaconess Medical Center). 23 patients were chosen at random from a pool of over 4000 thousand patients, whereas the rest was chosen to include examples of clinically important but statistically uncommon arrhythmias. Hence, the proportion of arrhythmias in the whole data set is much larger than in reality. Studies suggest a percentage between 1.5\% and 5\% of patients with arrhythmias depending on the reference group \colorbox{red}{REF???}. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{../images/placeholder.png}
    \caption{Excerpt from one ECG data sample, where the R peaks are highlighted}
\end{figure}

\colorbox{red}{description of heartbeat complex}

\colorbox{red}{what is aheartbeat arrhythmia how relate to anomaly detection}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{../images/placeholder.png}
    \caption{heart beat samples}
\end{figure}

Before we proceed with the training, we need to preprocess the data. Firstly, since the information contained in both channels are identical, one channel is discarded. We are roughly following the steps from \colorbox{red}{???REF}. This involves normalising the data and extracting single heartbeats from the sequence. To this end, we employ the commonly used QRS detector \colorbox{red}{REF} that can detect the R peaks. This is already implemented in the \texttt{wfdb} package for \texttt{python}. Then, from every peak we consider 90 time steps before and after that peak. This gives one heartbeat with sequence length 180. The associated beat type can then be extracted from the labelled data set. In total this gives \colorbox{red}{???} single heartbeats each of length 180. We will refer to those samples as private data samples, of which we aim to protect the privacy. In contrast, the generated synthetic data samples will be called public data samples, that we can publish for public use with some associated privacy measure (the privacy is measured by the privacy budget given by the \(\epsilon\)) parameter from \cref{def:dp}.

To ensure consistency of results and subsequent comparison of the different model, we split the original data into:
\begin{itemize}
    \item a private training set that consists only of regular samples
    \item a private validation set that consists of roughly equal parts regular and anomalous samples
    \item a private test set that consists of roughly equal parts regular and anomalous samples
\end{itemize}

Therefore, we use the following proportions:

\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        & \\ 
        &
    \end{tabular}
    \caption{Train-test-validation split of the preprocessed and extracted heartbeats}
    \label{tab:train-test-val}
\end{table}

All preprocessing tasks and subsequent models are implemented in \texttt{python 3.10} on a machine running Arch Linux with kernel version 6.7.4 with 16 GB of RAM, an Nvidia RTX 3070 8GB, and AMD 7700 processor with 8 cores / 16 threads.

\subsection{Baseline Model}
We will establish a baseline model for arrhythmia detection based on an anomaly detection approach from machine learning. One could treat this also a binary classification problem, but due to the class imbalancy, this would decrease the performance. Hence we are following another idea: one popular solution is to train an autoencoder model on only the regular heartbeats. This way, the model is able to compress regular heartbeat sequence into a latent space and recover them with low reconstruction error. When seeing an anomalous sample, the model (hopefully) fails at properly reconstruction the sample, resulting in a high reconstruction error. We will adjust this error threshold using the validation set, that consists of roughly equal numbers of regular and anomalous samples.

The baseline model is an LSTM-autoencoder with two components:
\begin{itemize}
    \item \underline{Encoder component:} \colorbox{red}{TBA}
    \item \underline{Decoder component:} \colorbox{red}{TBA}
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Architecture of baseline model}
    \label{fig:arch_baseline}
\end{figure}

\subsubsection*{Training the Baseline Model}
We train the model on the training set which consists of \colorbox{red}{TBA} regular heartbeat samples. As loss function we take the mean-squared-error (MSE) between original and reconstructed sample. Training is done using the Adam optimiser with \colorbox{red}{TBA} parameters. As we can see from \cref{fig:loss_baseline}, the loss function converges well indicating a (locally) optimal solution with an average reconstruction error of \colorbox{red}{TBA}. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Loss over epoch for baseline model}
    \label{fig:loss_baseline}
\end{figure}

When inputting a regular test sample, the model seems to capture all the visually important features and even removes some of the underlying noise.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Regular test sample vs. reconstructed sample}
    \label{fig:regog_vs_recon}
\end{figure}

Now we do the same but with an anomalous sample for demonstration purposes:

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Anomalous test sample vs. reconstructed sample}
    \label{fig:anomog_vs_recon}
\end{figure}

\subsubsection*{Performance of baseline model}
So visually, we can already verify that the baseline model seems to be able to distinguish between regular and anomalous samples. We now find an optimal threshold for the reconstruction error and measure the performance. Using the determined theshold value, we do the classification as follows: we input a sample to the anomaly detection model, which outputs the reconstructed sample with some error. If the error is below the threshold, it will be classified as a regular heartbeat, otherwise it will be classified as an anomalous heartbeat. Firstly, let us plot the error distribution the held out validation set:

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Distribution of reconstruction error for regular and anomalous samples in the validation set.}
    \label{fig:distr_err_baseline}
\end{figure}
Clearly, the average error is much lower for regular heartbeats. We can find the ``good'' threshold by computing the percentage of correctly classified regular samples and correctly classified anomalous samples based on different threshold values. From \cref{fig:thres_baseline} we can see that the percentage of correctly classified regular samples slowly increases, while the percentage of correctly classfied anomalous samples decreases. This is to be expected, since a threshold value of 0 would simply classify all samples as anomalous, giving perfect classification for anomalous samples. Conversely, a very high threshold value (e. g. 20) would tend to classify each samples as regular, yielding perfect classification for regular samples. We need to strike a balance between those two extremes by examining \cref{fig:thres_baseline}.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{placeholder.png}
    \caption{Percentages of correctly classified samples based on different threshold values}
    \label{fig:thres_baseline}
\end{figure}

There are different ways to find a well-suited threshold depending on whether one want to have better performance on anomalous or regular samples. We choose to not favour any classification over the other, hence the neutral decision strategy for the threshold value is the one where both lines cross. Choosing the threshold here to be \colorbox{red}{TBA}, we can compute different metrics that measure the performance of the arrhythmia detection:

\colorbox{red}{TBA}


\subsubsection*{Summary of performance evaluation}
Since we will reuse the previous performance evaluation procedure for measuring the utility of the generated data, we quickly summarise the process:
\begin{enumerate}
    \item Train an autoencoder on the synthetic training data set consisting of only regular samples. This synthetic training data is generated by our two different models.
    \item Based on the distribution of the reconstruction errors on the private validation set for regular and anomalous samples, we determine a threshold value for classification.
    \item We evaluate the utility of the generated data based on metrics for anomaly detection.
\end{enumerate}

This approach commonly referred to as the ``Train on synthetic, test on real'' (TSTR) paradigm \colorbox{red}{ref} that is being used to measure utility of synthetic data.

\subsection{Data generation}

\subsection{Privacy-preserving Data Generation}

\subsection{Polluted Data Set}

\subsection{Results}