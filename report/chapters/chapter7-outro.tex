\section{Conclusion}

In this project, we have tested two different models --- AE-(dp)MERF and AE-(dp)WGAN --- to generate privacy-preserving, synthetic ECG times series data. We employed differential privacy that comes with rigorous albeit probabilistic privacy guarantees. The utility of the generated data is measured by training a downstream anomaly detection model and assessing its performance. As a baseline, we trained the same model with the original, private data.

Our experiments with the MITBIH data set suggest that the privacy-utility tradeoff is more nuanced: with the AE-dpMERF model we generated privacy-preserving, synthetic data with little loss in utility for anomaly detection. Even more surprisingly, tests with contaminated training data suggest that an anomaly detection model trained using privacy-preserving, synthetic training data outperforms the same model trained directly on the original private data. This robustness could be a game changer for anomaly detection problems that involve private data.

This opens up one interesting further research direction: The aforementioned relation between (differential) privacy and robustness needs to be verified rigorously. Additionally, we need to examine how this translates to other machine learning task, e.g. classification and regression tasks. 
