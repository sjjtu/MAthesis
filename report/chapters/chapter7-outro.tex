\section{Conclusion}

In this project, we have tested two different models --- AE-(dp)MERF and AE-(dp)WGAN --- to generate privacy-preserving, synthetic ECG times series data. We employed differential privacy that comes with rigorous albeit probabilistic privacy guarantees. The utility of the generated data is measured by training a downstream anomaly detection model on this synthetic data and assessing its performance. As a baseline, we trained the same model with the original, private data. To test for robustness, we contaminated the training data with anomalous samples.

Our experiments with the MITBIH data set suggest that the privacy-utility tradeoff is more nuanced: while there is a gap when replacing the original, private data with (non-privacy-preserving) synthetic data, there was only little loss in utility for anomaly detection with the AE-dpMERF model that generated privacy-preserving, synthetic data. Even more surprisingly, tests with contaminated training data suggest that an anomaly detection model trained using privacy-preserving, synthetic training data outperforms the same model trained directly on the original private data. This robustness could be a game changer for anomaly detection problems that involve private data.

This opens up one interesting further research direction: The aforementioned relation between (differential) privacy and robustness needs to be verified rigorously. Additionally, we need to examine how this translates to other machine learning task, e.g. classification and regression tasks. Furthermore, the probabilistic privacy guarantees provided by differential privacy need to be tested empirically, e.g. by performing membership inference attacks.
