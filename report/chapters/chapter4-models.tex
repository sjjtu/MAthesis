\section{Models}\label{chapter4}
In this chapter we will introduce the models that are being used for time series ECG data generation. In particular, we will modify the DP-MERF algorithm to work with time series data. Our modified version will be called AE-MERF for the non-privacy-preserving algorithm and AE-dpMERF for the DP version. To ensure comparability and privacy, we do some small modifications to the RTSGAN architecture as well and call the result AE-WGAN and AE-dpWGAN respectively.

\subsection{AE-(dp)MERF}
Out of the box, DP-MERF does not work well to generate time series data. We hypothesize, that the algorithm is not able to capture the temporal dependencies and inherent ordering in sequential data. The authors have verified their algorithm mainly on tabular data as well as image data, where it delivers competitive results. Hence, we want to leverage the capabilities and translate it into the sequential setting. Therefore, we use an autoencoder architecture, that maps the time series to a compact latent representation of fixed dimension. Data points in the latent space can then be treated as tabular data with no temporal dependencies. This approach is akin to \parencite{gan_softtext} where Haidar et al. used an autoencoder with a GAN network to generate sequences of text. AE-(dp)MERF consists of two components.
\begin{itemize}
    \item \underline{Autoencoder component:} \colorbox{red}{TBA}
    \item \underline{DP-MERF component:} \colorbox{red}{TBA}
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{../images/placeholder.png}
    \caption[]{AE-(dp)MERF architecture}
\end{figure}

We will proceed to train the two components separately, i. e. we will first train the autoencoder and then the generator component. The autoencoder will learn a latent representation of the original ECG time series. The DP-MERF generator is trained to generate data in the latent space which will then be transformed back to a ECG time series sample using the decoder. For DP version of this model just employs the same DP techniques from the original DP-MERF algorithm. The post-processing theorem for DP \ref{thm:postpro} ensures (differential) privacy for the generated ECG time series samples after being passed to the decoder.

\subsection{AE-(dp)WGAN}
We are making some small modification to the original architecture of RTSGAN. Firstly, we swap the GRU-based autoencoder with the same LSTM-based one from AE-(dp)MERF. This is to make sure, that both models are compared in the same latent space.


To implement a (differential) private GAN there are different ways:
\begin{itemize}
    \item \underline{Input perturbation:} One could add differential private noise to the input data set before training. (Differential) Privacy is again preserved under the post-processing theorem \ref{thm:postpro}. \colorbox{red}{TBA more info}
    \item \underline{Output perturbation:} Similarly, one could add noise to the output of the model, once it is done training. 
    \item \underline{Gradient perturbation:} During training, adding a small amount of calibrated noise can also ensure (differential) privacy. This is implemented in the DP-SGD algorithm. The privacy level depends on the number iterations. 
\end{itemize}
Although the latter approach deteriorates privacy at each training iteration, it is still the preferred way to ``privatise'' a GAN according to this paper by researcher from Facebook \parencite{vandermaaten2020tradeoffs}. Hence, we will implement this in out GAN-based generator. Unfortunately, at the time of writing adding a gradient penalty is not supported \footnote{see github issue: \href{https://github.com/pytorch/opacus/issues/31}{https://github.com/pytorch/opacus/issues/31}} with the DP implementation of stochastic gradient descent in the \texttt{opacus} \footnote{see \href{https://opacus.ai/}{https://opacus.ai/}} package for \texttt{python}. Hence, instead of imposing the Lipschitz condition via the penalty term, we impose a strict constraint on the norm of the weights directly to enforce the Lipschitz condition. This approach was suggested initially by the authors of the original WGAN paper \parencite{arjovsky2017wasserstein}. We employ this in both the non-privacy preserving and DP version of this model to again ensure comparability.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{../images/placeholder.png}
    \caption[short]{Architecture of AE-(dp)WGAN}
\end{figure}
